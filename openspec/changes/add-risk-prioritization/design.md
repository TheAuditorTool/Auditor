## Overview
Implement coverage-aware risk prioritization that surfaces the highest danger findings first while keeping all persistence database-first. The change introduces three new storage surfaces (coverage summary, uncovered line gaps, and finding risk metadata), a deterministic scoring module, and updated pipeline/reporting steps that publish AI-sized summaries.

## Data Model & Persistence

| Table | Purpose | Key Columns |
|-------|---------|-------------|
| `test_coverage_summary` | Stores per-file coverage facts generated by `MetadataCollector` | `file_path TEXT PRIMARY KEY`, `coverage_ratio REAL NOT NULL`, `lines_executed INTEGER`, `lines_missing INTEGER`, `last_updated TEXT` |
| `test_coverage_gaps` | Tracks uncovered line numbers for coverage lookups | `file_path TEXT NOT NULL`, `line_number INTEGER NOT NULL`, composite primary key (`file_path`, `line_number`) |
| `finding_risk_scores` | Links `findings_consolidated.id` to normalized severity weight, coverage ratio, and computed `risk_score` | `finding_id INTEGER PRIMARY KEY`, `severity_weight REAL NOT NULL`, `coverage_ratio REAL NOT NULL`, `risk_score REAL NOT NULL`, `is_line_uncovered BOOLEAN NOT NULL DEFAULT 0`, `metadata TEXT` |

Design notes:
1. Schema defined centrally in `theauditor/indexer/schema.py` and created via `DatabaseManager.create_schema` migration branch (ALTER + CREATE statements with idempotent guards).  
2. `clear_tables` and any rebuild helpers truncate the new tables so `aud index --rebuild` maintains consistency.  
3. Coverage collectors write via two helper methods in `DatabaseManager` (`write_coverage_summary_batch`, `write_coverage_gap_batch`) to keep batching and commit strategy aligned with existing dual-write flows.  
4. Risk scoring inserts/updates leverage `DatabaseManager.write_risk_scores_batch` with `INSERT ... ON CONFLICT(finding_id) DO UPDATE` semantics so reruns refresh data atomically.

## Risk Scoring Engine

Module: `theauditor/risk_prioritizer.py`

Algorithm:
```
severity_weight = {
    "critical": 1.0,
    "high": 0.75,
    "medium": 0.5,
    "low": 0.25,
    "warning": 0.15,
    "info": 0.15,
    "style": 0.05,
}
coverage_ratio = clamp(coverage_pct / 100, 0.0, 1.0)
risk_score = severity_weight * (1.0 - coverage_ratio)
+ uncovered-line boost (adds 0.1 if finding line is explicitly uncovered, capped <=1.0)
```

Execution flow:
1. Query `findings_consolidated` for findings + join to `test_coverage_summary` on normalized file path using `build_query` helpers.  
2. Fetch uncovered line data on demand from `test_coverage_gaps` to apply the uncovered-line boost.  
3. Normalize severities using `theauditor.utils.finding_priority.normalize_severity` to align with existing priority order.  
4. Persist results into `finding_risk_scores` (UPSERT on `finding_id`) and emit `.pf/raw/prioritized_findings.json` sorted by `risk_score DESC, severity_weight DESC, coverage_ratio ASC`.  
5. Expose a CLI command `aud prioritize` (Click command under `theauditor/commands/prioritize.py`) that wires these steps, writes CLI telemetry (top 5 findings, highest risk), and respects `--db`/`--root` arguments for sandbox portability.

## Pipeline & FCE Integration

Pipeline order changes (Stage 3):
1. Replace `aud metadata churn` with `aud metadata analyze --days 90` so both churn and coverage facts are available.  
2. Insert `aud prioritize` immediately after taint/pattern/graph phases so risk scores reflect final deduplicated findings (positioned before `aud fce`).  
3. Track outputs in `.pf/raw/prioritized_findings.json`, create `.pf/risk_scores.json` mirrors for compatibility, and ensure `.pf/raw/coverage_analysis.json` is still produced for human debugging.  
4. Update `command_order` and the stage classifier so the new command participates in Stage 3B and inherits timeout telemetry.

FCE updates:
1. Join `finding_risk_scores` when loading findings (`scan_all_findings`) to attach `risk_score`, `coverage_ratio`, `severity_weight`, and uncovered-line flags.  
2. Sort `results["all_findings"]` primarily by `risk_score DESC`, then severity priority (existing `finding_priority` fallback), preserving deterministic tie-breaking.  
3. Emit risk metadata in correlation outputs and the meta-finding aggregations so `/readthis` chunks remain fact-complete without re-querying SQLite.  
4. Update the FCE log summary to include the top 5 risk scores and highlight uncovered high-risk findings for quick triage.

## Summary & Report Outputs

1. `aud summary` builds `summary_prioritized_combined.json` (≤100 KB) and `summary_prioritized_overflow.json` if needed. Each file contains:
   - Overall stats (top risk counts, number of uncovered criticals).  
   - Top findings (id, file, line, rule, severity, risk_score, coverage_percent, uncovered flag, source command).  
   - Cross-links pointing to `.pf/raw/prioritized_findings.json` and database table names with SQL snippets (read-only).  
2. Per-track capsules (`summary_lint_top_risk.json`, `summary_graph_top_risk.json`, `summary_import_top_risk.json`, `summary_taint_top_risk.json`, etc.) list the top N (configurable default 10) risk-ranked findings for that analyzer and include offsets to the combined summary for deduplication.  
3. `theauditor/extraction.py` publishes the new summaries into `.pf/readthis/` (under 100 KB per file) and updates the manifest so downstream agents can discover the prioritized documents first.  
4. CLI doc generation (`theauditor/docgen.py`) documents the `aud prioritize` command, new summary artifacts, and database tables to keep help text current.

## Verification & Testing Strategy

Automated checks:
1. Schema contract unit tests verifying new tables exist and migrate correctly (`tests/test_schema_contract.py`).  
2. New integration test `tests/test_risk_prioritizer.py` covers severity normalization, coverage join logic, uncovered-line boost, and UPSERT behavior.  
3. Pipeline smoke test extension ensuring `aud prioritize` runs during `aud full` and risk summaries are generated (assert presence in `.pf/pipeline.log`).  
4. Summary size test verifying combined prioritized files remain ≤100 KB (use fixture with synthetic data to assert truncation rules) and that per-track capsules cap at configured item counts.

Manual verification checkpoints:
1. Run `aud metadata analyze`, confirm tables populated via `sqlite3 .pf/repo_index.db "SELECT COUNT(*) FROM test_coverage_summary"`.  
2. Execute `aud prioritize`, confirm CLI output and inspect `.pf/raw/prioritized_findings.json`.  
3. Run `aud fce` and ensure `[FCE] Sorted ...` log references risk ordering.  
4. Inspect `.pf/readthis/summary_prioritized_part1.json` and per-track capsules for structure and size.  
5. Validate overall change with `openspec validate add-risk-prioritization --strict` and project QA suite (`pytest`, `ruff`, `mypy`).
