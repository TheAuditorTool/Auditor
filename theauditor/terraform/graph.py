"""Terraform Provisioning Flow Graph Builder.

Constructs data flow graphs from Terraform resources, showing how variables,
resources, and outputs connect through dependencies and interpolations.

Architecture:
- Database-first: Reads from repo_index.db terraform_* tables
- Outputs to graphs.db via XGraphStore.save_custom_graph()
- Zero fallbacks: Missing data = empty graph (exposes indexer bugs)
- Same format as DFGBuilder (dataclass â†’ asdict)

Usage:
    builder = TerraformGraphBuilder(db_path=".pf/repo_index.db")
    graph = builder.build_provisioning_flow_graph(root=".")
    # Returns: {'nodes': [...], 'edges': [...], 'metadata': {...}}
"""

import sqlite3
import json
import re
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Set, Optional

from ..graph.store import XGraphStore
from ..utils.logger import setup_logger

logger = setup_logger(__name__)


@dataclass
class ProvisioningNode:
    """Represents a node in the Terraform provisioning graph."""

    id: str  # Format: "resource_id", "variable_id", or "output_id"
    file: str
    node_type: str  # "resource", "variable", "output", "data"
    terraform_type: str  # e.g., "aws_db_instance", "string", etc.
    name: str
    is_sensitive: bool = False
    has_public_exposure: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ProvisioningEdge:
    """Represents a data flow edge in the provisioning graph."""

    source: str  # Source node ID
    target: str  # Target node ID
    file: str
    edge_type: str  # "variable_reference", "resource_dependency", "output_reference"
    expression: str = ""  # The interpolation expression
    metadata: Dict[str, Any] = field(default_factory=dict)


class TerraformGraphBuilder:
    """Build Terraform provisioning flow graphs from repo_index.db.

    Follows DFGBuilder pattern exactly - database-first, zero fallbacks.
    """

    def __init__(self, db_path: str):
        """Initialize builder with database path.

        Args:
            db_path: Path to repo_index.db
        """
        self.db_path = Path(db_path)
        if not self.db_path.exists():
            raise FileNotFoundError(f"Database not found: {db_path}")

        # Initialize XGraphStore for writing to graphs.db
        graphs_db_path = self.db_path.parent / "graphs.db"
        self.store = XGraphStore(db_path=str(graphs_db_path))

    def build_provisioning_flow_graph(self, root: str = ".") -> Dict[str, Any]:
        """Build provisioning flow graph from Terraform data.

        Queries terraform_* tables to construct a graph showing how
        variables flow through resources to outputs.

        Args:
            root: Project root (for metadata only)

        Returns:
            Dict with nodes, edges, and metadata (same format as DFGBuilder)
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        nodes: Dict[str, ProvisioningNode] = {}
        edges: List[ProvisioningEdge] = []

        stats = {
            'total_resources': 0,
            'total_variables': 0,
            'total_outputs': 0,
            'edges_created': 0,
            'files_processed': 0,
        }

        # Step 1: Load all variables as source nodes
        cursor.execute("""
            SELECT variable_id, file_path, variable_name, variable_type, is_sensitive
            FROM terraform_variables
        """)

        for row in cursor.fetchall():
            stats['total_variables'] += 1
            nodes[row['variable_id']] = ProvisioningNode(
                id=row['variable_id'],
                file=row['file_path'],
                node_type='variable',
                terraform_type=row['variable_type'] or 'unknown',
                name=row['variable_name'],
                is_sensitive=bool(row['is_sensitive']),
                metadata={'source': 'terraform_variables'}
            )

        # Step 2: Load all resources as processing nodes
        cursor.execute("""
            SELECT resource_id, file_path, resource_type, resource_name,
                   properties_json, depends_on_json, sensitive_flags_json,
                   has_public_exposure
            FROM terraform_resources
        """)

        for row in cursor.fetchall():
            stats['total_resources'] += 1

            # Create resource node
            resource_id = row['resource_id']
            nodes[resource_id] = ProvisioningNode(
                id=resource_id,
                file=row['file_path'],
                node_type='resource',
                terraform_type=row['resource_type'],
                name=row['resource_name'],
                has_public_exposure=bool(row['has_public_exposure']),
                metadata={
                    'properties': json.loads(row['properties_json']) if row['properties_json'] else {},
                    'sensitive_properties': json.loads(row['sensitive_flags_json']) if row['sensitive_flags_json'] else [],
                }
            )

            # Parse properties to find variable references
            properties = json.loads(row['properties_json']) if row['properties_json'] else {}
            var_refs = self._extract_variable_references(properties)

            for var_name in var_refs:
                # Find variable node by name (may be in different file)
                var_id = self._find_variable_id(cursor, var_name, row['file_path'])
                if var_id and var_id in nodes:
                    edges.append(ProvisioningEdge(
                        source=var_id,
                        target=resource_id,
                        file=row['file_path'],
                        edge_type='variable_reference',
                        expression=f"var.{var_name}",
                        metadata={'property_path': self._find_property_path(properties, var_name)}
                    ))
                    stats['edges_created'] += 1

            # Parse depends_on to create explicit dependency edges
            depends_on = json.loads(row['depends_on_json']) if row['depends_on_json'] else []
            for dep_ref in depends_on:
                # dep_ref format: "aws_security_group.web" or full reference
                dep_id = self._resolve_resource_reference(cursor, dep_ref, row['file_path'])
                if dep_id and dep_id in nodes:
                    edges.append(ProvisioningEdge(
                        source=dep_id,
                        target=resource_id,
                        file=row['file_path'],
                        edge_type='resource_dependency',
                        expression=dep_ref,
                        metadata={'explicit_depends_on': True}
                    ))
                    stats['edges_created'] += 1

        # Step 3: Load outputs as sink nodes
        cursor.execute("""
            SELECT output_id, file_path, output_name, value_json, is_sensitive
            FROM terraform_outputs
        """)

        for row in cursor.fetchall():
            stats['total_outputs'] += 1
            output_id = row['output_id']

            nodes[output_id] = ProvisioningNode(
                id=output_id,
                file=row['file_path'],
                node_type='output',
                terraform_type='output',
                name=row['output_name'],
                is_sensitive=bool(row['is_sensitive']),
                metadata={'value_expr': row['value_json']}
            )

            # Parse output value to find resource/variable references
            value_json = row['value_json']
            if value_json:
                refs = self._extract_references_from_expression(value_json)
                for ref in refs:
                    source_id = self._resolve_reference(cursor, ref, row['file_path'])
                    if source_id and source_id in nodes:
                        edges.append(ProvisioningEdge(
                            source=source_id,
                            target=output_id,
                            file=row['file_path'],
                            edge_type='output_reference',
                            expression=ref,
                        ))
                        stats['edges_created'] += 1

        conn.close()

        # Count unique files
        files = set(node.file for node in nodes.values())
        stats['files_processed'] = len(files)

        # Convert to dict format (same as DFGBuilder)
        result = {
            'nodes': [asdict(node) for node in nodes.values()],
            'edges': [asdict(edge) for edge in edges],
            'metadata': {
                'root': str(Path(root).resolve()),
                'graph_type': 'terraform_provisioning',
                'stats': stats,
            }
        }

        # Write to graphs.db
        self._write_to_graphs_db(result)

        logger.info(
            f"Built Terraform provisioning graph: {stats['total_resources']} resources, "
            f"{stats['total_variables']} variables, {stats['total_outputs']} outputs, "
            f"{stats['edges_created']} edges"
        )

        return result

    def _extract_variable_references(self, properties: Dict) -> Set[str]:
        """Extract variable names from property values.

        Searches for Terraform variable interpolations:
        - Modern: var.NAME
        - Legacy: ${var.NAME}
        """
        var_names = set()

        def scan_value(val):
            if isinstance(val, str):
                # Look for ${var.NAME} or var.NAME patterns
                matches = re.findall(r'\$\{var\.(\w+)\}|var\.(\w+)', val)
                for match in matches:
                    var_names.add(match[0] or match[1])
            elif isinstance(val, dict):
                for v in val.values():
                    scan_value(v)
            elif isinstance(val, list):
                for item in val:
                    scan_value(item)

        scan_value(properties)
        return var_names

    def _find_variable_id(self, cursor, var_name: str, current_file: str) -> Optional[str]:
        """Find variable ID by name (may be in different file).

        Tries same file first, then any file (for module variables).
        """
        # Try same file first
        cursor.execute("""
            SELECT variable_id FROM terraform_variables
            WHERE variable_name = ? AND file_path = ?
        """, (var_name, current_file))
        row = cursor.fetchone()
        if row:
            return row['variable_id']

        # Try any file (module variables)
        cursor.execute("""
            SELECT variable_id FROM terraform_variables
            WHERE variable_name = ?
            LIMIT 1
        """, (var_name,))
        row = cursor.fetchone()
        return row['variable_id'] if row else None

    def _resolve_resource_reference(self, cursor, ref: str, current_file: str) -> Optional[str]:
        """Resolve resource reference like 'aws_security_group.web' to resource_id.

        Args:
            cursor: Database cursor
            ref: Resource reference (e.g., "aws_security_group.web")
            current_file: Current file path

        Returns:
            resource_id or None if not found
        """
        # ref format: "resource_type.resource_name"
        parts = ref.split('.', 1)
        if len(parts) != 2:
            return None

        resource_type, resource_name = parts

        cursor.execute("""
            SELECT resource_id FROM terraform_resources
            WHERE resource_type = ? AND resource_name = ?
        """, (resource_type, resource_name))
        row = cursor.fetchone()
        return row['resource_id'] if row else None

    def _extract_references_from_expression(self, expr_json: str) -> Set[str]:
        """Extract all Terraform references from an expression.

        Matches patterns:
        - aws_*.name (resource references)
        - var.name (variable references)
        - data.*.name (data source references)
        """
        refs = set()
        if not expr_json:
            return refs

        try:
            expr = json.loads(expr_json) if isinstance(expr_json, str) else expr_json
        except:
            expr = str(expr_json)

        # Match patterns: aws_*.name, var.name, data.*.name
        matches = re.findall(r'((?:aws_|azurerm_|google_)\w+\.\w+|var\.\w+|data\.\w+\.\w+)', str(expr))
        refs.update(matches)
        return refs

    def _resolve_reference(self, cursor, ref: str, current_file: str) -> Optional[str]:
        """Resolve any Terraform reference to node ID.

        Args:
            cursor: Database cursor
            ref: Terraform reference (var.X, aws_Y.Z, etc.)
            current_file: Current file path

        Returns:
            Node ID or None if not found
        """
        if ref.startswith('var.'):
            var_name = ref.split('.', 1)[1]
            return self._find_variable_id(cursor, var_name, current_file)
        else:
            return self._resolve_resource_reference(cursor, ref, current_file)

    def _find_property_path(self, properties: Dict, var_name: str) -> Optional[str]:
        """Find which property path contains the variable reference.

        Args:
            properties: Resource properties dict
            var_name: Variable name to search for

        Returns:
            Property key or None if not found
        """
        # Simplified - just return first match
        for key, val in properties.items():
            if isinstance(val, str) and var_name in val:
                return key
        return None

    def _write_to_graphs_db(self, graph: Dict[str, Any]):
        """Write graph to graphs.db using XGraphStore.save_custom_graph().

        Converts ProvisioningNode/Edge format to XGraphStore format.
        """
        # XGraphStore expects nodes/edges with specific fields
        store_nodes = []
        for node in graph['nodes']:
            store_nodes.append({
                'id': node['id'],
                'file': node['file'],
                'type': node['node_type'],
                'lang': 'terraform',
                'metadata': {
                    'terraform_type': node['terraform_type'],
                    'name': node['name'],
                    'is_sensitive': node['is_sensitive'],
                    'has_public_exposure': node['has_public_exposure'],
                    **node['metadata']
                }
            })

        store_edges = []
        for edge in graph['edges']:
            store_edges.append({
                'source': edge['source'],
                'target': edge['target'],
                'type': edge['edge_type'],
                'file': edge['file'],
                'metadata': {
                    'expression': edge['expression'],
                    **edge['metadata']
                }
            })

        # Write via XGraphStore.save_custom_graph()
        self.store.save_custom_graph({
            'nodes': store_nodes,
            'edges': store_edges,
            'metadata': graph['metadata']
        }, graph_type='terraform_provisioning')

        logger.debug(f"Wrote Terraform provisioning graph to graphs.db")
