"""Native vulnerability scanners wrapper for npm audit, pip-audit, and OSV-Scanner.

This module runs native security tools, cross-references findings for validation,
and writes to both database and JSON.

Architecture:
- Reads packages from package_configs table (populated by indexer)
- Runs 3 detection sources in parallel:
  * npm audit (sandboxed node runtime)
  * pip-audit (bundled in .theauditor_tools)
  * osv-scanner (Google's official OSV.dev scanner)
- Cross-references findings for confidence scoring
- Writes to findings_consolidated table (for FCE correlation)
- Writes to JSON (for AI readability)

Cross-Reference Strategy:
- Group findings by vulnerability ID (CVE/GHSA)
- Confidence = # of sources that found it
- Severity = highest when sources disagree
- Flag discrepancies for review

OSV-Scanner Facts (DO NOT HALLUCINATE):
- Binary location: .auditor_venv/.theauditor_tools/osv-scanner/osv-scanner.exe (Windows)
- Offline database: .auditor_venv/.theauditor_tools/osv-scanner/db/{ecosystem}/all.zip
- Usage: osv-scanner scan -L package-lock.json --format json --offline-vulnerabilities
- Database download: --download-offline-databases flag
- No rate limits (offline database)
"""

import json
import sqlite3
import subprocess
import shutil
import platform
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, UTC

from theauditor.utils.logger import setup_logger

# Windows compatibility
IS_WINDOWS = platform.system() == "Windows"

logger = setup_logger(__name__)


class VulnerabilityScanner:
    """Main vulnerability scanner orchestrator."""

    def __init__(self, db_path: str, offline: bool = False):
        """Initialize scanner.

        Args:
            db_path: Path to repo_index.db
            offline: If True, use offline databases only (no network)
        """
        self.db_path = db_path
        self.offline = offline
        self._package_file_map = {}  # Cache package->file path mapping

        # Tool status tracking for observability
        self.tool_status = {
            "npm-audit": {"status": "not_run", "error": None, "findings_count": 0},
            "pip-audit": {"status": "not_run", "error": None, "findings_count": 0},
            "osv-scanner": {"status": "not_run", "error": None, "findings_count": 0}
        }

        try:
            self.conn = sqlite3.connect(db_path)
            self.cursor = self.conn.cursor()
        except sqlite3.Error as e:
            logger.error(f"Failed to connect to database: {e}")
            raise

    def scan(self) -> List[Dict[str, Any]]:
        """Main entry point - run all detection sources and cross-reference.

        Returns:
            List of validated findings
        """
        logger.info("Starting vulnerability scan...")

        # Load packages from database
        packages = self._load_packages_from_db()
        logger.info(f"Loaded {len(packages)} packages from database")

        if not packages:
            logger.warning("No packages found in database - npm-audit will be skipped")
            # Continue anyway - pip-audit reads files directly, doesn't need DB

        # Run all 4 sources (COMBINE, not fallback)
        logger.info("Running npm audit...")
        npm_findings = self._run_npm_audit()
        logger.info(f"npm audit found {len(npm_findings)} vulnerabilities")

        logger.info("Running pip-audit...")
        pip_findings = self._run_pip_audit()
        logger.info(f"pip-audit found {len(pip_findings)} vulnerabilities")

        logger.info("Running OSV-Scanner...")
        osv_findings = self._run_osv_scanner()
        logger.info(f"OSV-Scanner found {len(osv_findings)} vulnerabilities")

        # Cross-reference for validation
        logger.info("Cross-referencing findings...")
        validated = self._cross_reference(npm_findings, pip_findings, osv_findings)
        logger.info(f"Validated {len(validated)} unique vulnerabilities")

        # Dual write (database + JSON)
        logger.info("Writing findings to database...")
        self._write_to_db(validated)

        logger.info("Writing findings to JSON...")
        self._write_to_json(validated)

        logger.info("Vulnerability scan completed")
        return validated

    def _load_packages_from_db(self) -> List[Dict[str, str]]:
        """Load packages from package_configs table.

        Returns:
            List of package dicts with name, version, manager, file
        """
        packages = []

        # Query package_configs table
        self.cursor.execute("""
            SELECT package_name, version, file_path
            FROM package_configs
        """)

        for pkg_name, version, file_path in self.cursor.fetchall():
            # Infer manager from file path
            if 'package.json' in file_path:
                manager = 'npm'
            elif 'requirements.txt' in file_path or 'pyproject.toml' in file_path:
                manager = 'py'
            else:
                manager = 'unknown'

            packages.append({
                'name': pkg_name,
                'version': version or 'unknown',
                'manager': manager,
                'file': file_path
            })

            # Build package->file mapping for later use
            cache_key = f"{manager}:{pkg_name}"
            self._package_file_map[cache_key] = file_path

        return packages

    def _run_npm_audit(self) -> List[Dict[str, Any]]:
        """Run npm audit using sandboxed node runtime.

        Returns:
            List of vulnerability findings from npm audit
        """
        self.tool_status["npm-audit"]["status"] = "running"
        vulnerabilities = []

        try:
            # Check if package.json exists
            project_root = Path.cwd()
            package_json = project_root / "package.json"
            if not package_json.exists():
                self.tool_status["npm-audit"]["status"] = "success"
                self.tool_status["npm-audit"]["findings_count"] = 0
                return vulnerabilities

            # Check if node_modules exists (npm audit needs it)
            node_modules = project_root / "node_modules"
            if not node_modules.exists():
                self.tool_status["npm-audit"]["status"] = "success"
                self.tool_status["npm-audit"]["findings_count"] = 0
                return vulnerabilities

            # Find sandboxed npm
            sandbox_base = project_root / ".auditor_venv" / ".theauditor_tools"
            node_runtime = sandbox_base / "node-runtime"

            if IS_WINDOWS:
                node_exe = node_runtime / "node.exe"
                npm_cli = node_runtime / "node_modules" / "npm" / "bin" / "npm-cli.js"
                if npm_cli.exists():
                    npm_cmd = [str(node_exe), str(npm_cli), "audit", "--json"]
                else:
                    npm_cmd_path = node_runtime / "npm.cmd"
                    if npm_cmd_path.exists():
                        npm_cmd = [str(npm_cmd_path), "audit", "--json"]
                    else:
                        self.tool_status["npm-audit"]["status"] = "success"
                        self.tool_status["npm-audit"]["findings_count"] = 0
                        return vulnerabilities
            else:
                node_exe = node_runtime / "bin" / "node"
                npm_exe = node_runtime / "bin" / "npm"
                if npm_exe.exists():
                    npm_cmd = [str(npm_exe), "audit", "--json"]
                else:
                    self.tool_status["npm-audit"]["status"] = "success"
                    self.tool_status["npm-audit"]["findings_count"] = 0
                    return vulnerabilities

            if not node_exe.exists():
                self.tool_status["npm-audit"]["status"] = "success"
                self.tool_status["npm-audit"]["findings_count"] = 0
                return vulnerabilities

            result = subprocess.run(
                npm_cmd,
                cwd=str(project_root),
                capture_output=True,
                text=True,
                timeout=60,
                shell=IS_WINDOWS
            )

            if result.stdout:
                audit_data = json.loads(result.stdout)

                if "vulnerabilities" in audit_data:
                    for pkg_name, pkg_data in audit_data["vulnerabilities"].items():
                        if not pkg_data.get("via"):
                            continue

                        for via_item in pkg_data.get("via", []):
                            if isinstance(via_item, str):
                                continue

                            if isinstance(via_item, dict):
                                severity = via_item.get("severity", "")

                                vuln_id = via_item.get("cve")
                                if not vuln_id:
                                    vuln_id = via_item.get("ghsa")
                                if not vuln_id:
                                    vuln_id = via_item.get("source", f"npm-audit-{pkg_name}")

                                aliases = []
                                if via_item.get("cve"):
                                    aliases.append(via_item["cve"])
                                if via_item.get("ghsa"):
                                    aliases.append(via_item["ghsa"])

                                fixed_version = None
                                if pkg_data.get("fixAvailable"):
                                    fix_info = pkg_data["fixAvailable"]
                                    if isinstance(fix_info, dict) and "version" in fix_info:
                                        fixed_version = fix_info["version"]

                                affected_range = pkg_data.get("range", "")
                                current_version = affected_range.split(" ")[0].lstrip("<>=") if affected_range else ""

                                vulnerability = {
                                    "package": pkg_name,
                                    "version": current_version,
                                    "manager": "npm",
                                    "vulnerability_id": vuln_id,
                                    "severity": severity,
                                    "summary": via_item.get("title", "No summary available"),
                                    "details": via_item.get("overview", ""),
                                    "aliases": aliases,
                                    "published": via_item.get("created", ""),
                                    "modified": via_item.get("updated", ""),
                                    "references": [{
                                        "type": "ADVISORY",
                                        "url": via_item.get("url", "")
                                    }] if via_item.get("url") else [],
                                    "affected_ranges": [pkg_data.get("range", "")] if pkg_data.get("range") else [],
                                    "fixed_version": fixed_version,
                                    "source": "npm audit"
                                }

                                vulnerabilities.append(vulnerability)

            self.tool_status["npm-audit"]["status"] = "success"
            self.tool_status["npm-audit"]["findings_count"] = len(vulnerabilities)
            return vulnerabilities

        except Exception as e:
            self.tool_status["npm-audit"]["status"] = "error"
            self.tool_status["npm-audit"]["error"] = str(e)
            logger.error(f"npm-audit failed: {e}")
            return []

    def _find_pip_audit(self) -> str:
        """Find bundled pip-audit or system fallback.

        Returns:
            Path to pip-audit executable

        Raises:
            FileNotFoundError: If pip-audit is not found in either location
        """
        # Try bundled version first (PREFERRED)
        tools_dir = Path(".auditor_venv/.theauditor_tools/python-tools")
        if IS_WINDOWS:
            bundled = tools_dir / "pip-audit.exe"
        else:
            bundled = tools_dir / "pip-audit"

        if bundled.exists():
            return str(bundled)

        # Fallback to system pip-audit (if user installed it)
        system_pip_audit = shutil.which("pip-audit")
        if system_pip_audit:
            return system_pip_audit

        raise FileNotFoundError(
            f"pip-audit not found at {bundled} or in system PATH. "
            f"Run 'aud setup-ai --target .' to install vulnerability scanners."
        )

    def _run_pip_audit(self) -> List[Dict[str, Any]]:
        """Run pip-audit using bundled or system version.

        Returns:
            List of vulnerability findings from pip-audit
        """
        self.tool_status["pip-audit"]["status"] = "running"
        vulnerabilities = []

        try:
            # Check if we have Python dependencies to audit
            project_root = Path.cwd()
            has_requirements = (project_root / "requirements.txt").exists()
            has_pyproject = (project_root / "pyproject.toml").exists()

            if not has_requirements and not has_pyproject:
                self.tool_status["pip-audit"]["status"] = "success"
                self.tool_status["pip-audit"]["findings_count"] = 0
                return vulnerabilities

            # Find pip-audit executable (will raise FileNotFoundError if missing)
            pip_audit_path = self._find_pip_audit()

            cmd = [pip_audit_path, "--format", "json"]

            if has_requirements:
                cmd.extend(["-r", "requirements.txt"])

            result = subprocess.run(
                cmd,
                cwd=str(project_root),
                capture_output=True,
                text=True,
                timeout=60,
                shell=IS_WINDOWS
            )

            if result.stdout:
                audit_data = json.loads(result.stdout)

                # pip-audit structure: {"dependencies": [...], "fixes": [...]}
                for dep in audit_data.get("dependencies", []):
                    pkg_name = dep.get("name", "")
                    pkg_version = dep.get("version", "")

                    # Each dependency has "vulns" array
                    for vuln in dep.get("vulns", []):
                        vuln_id = vuln.get("id", f"pip-audit-{pkg_name}")

                        aliases = []
                        if vuln.get("aliases"):
                            aliases.extend(vuln["aliases"])

                        vulnerability = {
                            "package": pkg_name,
                            "version": pkg_version,
                            "manager": "py",
                            "vulnerability_id": vuln_id,
                            "severity": "",  # pip-audit doesn't provide severity
                            "summary": vuln.get("description", "No summary available"),
                            "details": vuln.get("description", ""),
                            "aliases": aliases,
                            "published": "",
                            "modified": "",
                            "references": [],
                            "affected_ranges": [],
                            "fixed_version": vuln.get("fix_versions", [""])[0] if vuln.get("fix_versions") else None,
                            "source": "pip-audit"
                        }

                        vulnerabilities.append(vulnerability)

            self.tool_status["pip-audit"]["status"] = "success"
            self.tool_status["pip-audit"]["findings_count"] = len(vulnerabilities)
            return vulnerabilities

        except Exception as e:
            self.tool_status["pip-audit"]["status"] = "error"
            self.tool_status["pip-audit"]["error"] = str(e)
            logger.error(f"pip-audit failed: {e}")
            return []

    def _find_osv_scanner(self) -> str:
        """Find bundled osv-scanner binary.

        Returns:
            Path to osv-scanner executable

        Raises:
            FileNotFoundError: If osv-scanner is not found in either location
        """
        tools_dir = Path(".auditor_venv/.theauditor_tools/osv-scanner")

        if IS_WINDOWS:
            binary = tools_dir / "osv-scanner.exe"
        else:
            binary = tools_dir / "osv-scanner"

        if binary.exists():
            return str(binary)

        # Fallback to system osv-scanner (if user installed it)
        system_osv = shutil.which("osv-scanner")
        if system_osv:
            return system_osv

        raise FileNotFoundError(
            f"osv-scanner not found at {binary} or in system PATH. "
            f"Run 'aud setup-ai --target .' to install vulnerability scanners."
        )

    def _run_osv_scanner(self) -> List[Dict[str, Any]]:
        """Run OSV-Scanner using bundled binary.

        FACTS (from usage.md):
        - Scan lockfiles: osv-scanner scan -L package-lock.json -L requirements.txt
        - Output format: --format json
        - Offline mode: --offline-vulnerabilities
        - Database location: env var OSV_SCANNER_LOCAL_DB_CACHE_DIRECTORY

        Returns:
            List of vulnerability findings from OSV-Scanner
        """
        self.tool_status["osv-scanner"]["status"] = "running"
        vulnerabilities = []

        try:
            # Find lockfiles to scan
            project_root = Path.cwd()
            lockfiles = []

            # npm lockfiles
            if (project_root / "package-lock.json").exists():
                lockfiles.extend(["-L", str(project_root / "package-lock.json")])
            elif (project_root / "yarn.lock").exists():
                lockfiles.extend(["-L", str(project_root / "yarn.lock")])

            # Python lockfiles
            if (project_root / "requirements.txt").exists():
                lockfiles.extend(["-L", str(project_root / "requirements.txt")])
            elif (project_root / "Pipfile.lock").exists():
                lockfiles.extend(["-L", str(project_root / "Pipfile.lock")])

            if not lockfiles:
                self.tool_status["osv-scanner"]["status"] = "success"
                self.tool_status["osv-scanner"]["findings_count"] = 0
                return vulnerabilities

            # Find osv-scanner binary (will raise FileNotFoundError if missing)
            osv_scanner_path = self._find_osv_scanner()

            # Set database location to our sandbox
            # IMPORTANT: Merge with system environment to preserve PATH, etc.
            db_dir = Path(".auditor_venv/.theauditor_tools/osv-scanner/db")
            env = {**os.environ, "OSV_SCANNER_LOCAL_DB_CACHE_DIRECTORY": str(db_dir)}

            # Build command
            cmd = [osv_scanner_path, "scan"] + lockfiles + ["--format", "json"]

            # ALWAYS use offline database (never hit API)
            cmd.append("--offline-vulnerabilities")

            # Run osv-scanner
            result = subprocess.run(
                cmd,
                cwd=str(project_root),
                capture_output=True,
                text=True,
                timeout=120,  # OSV-Scanner can be slower than npm audit
                env=env
            )

            # OSV-Scanner returns non-zero exit code if vulnerabilities found
            # So check stdout regardless of return code
            if result.stdout:
                try:
                    scan_data = json.loads(result.stdout)

                    # Parse OSV-Scanner JSON output
                    # Structure: {"results": [{"packages": [...], "source": {...}}]}
                    for result_item in scan_data.get("results", []):
                        for package_vuln in result_item.get("packages", []):
                            pkg_info = package_vuln.get("package", {})
                            pkg_name = pkg_info.get("name", "")
                            pkg_version = pkg_info.get("version", "")
                            pkg_ecosystem = pkg_info.get("ecosystem", "")

                            # Map ecosystem to manager
                            manager = "npm" if pkg_ecosystem in ["npm", "NPM"] else "py" if pkg_ecosystem in ["PyPI", "Python"] else "unknown"

                            for vuln in package_vuln.get("vulnerabilities", []):
                                vuln_id = vuln.get("id", f"osv-{pkg_name}")

                                # Extract severity (if available)
                                severity = ""
                                if vuln.get("database_specific", {}).get("severity"):
                                    severity = vuln["database_specific"]["severity"]

                                # Extract CWE from OSV database_specific field
                                cwe = ""
                                db_specific = vuln.get("database_specific", {})
                                if db_specific and "cwe_ids" in db_specific:
                                    cwe_ids = db_specific["cwe_ids"]
                                    if isinstance(cwe_ids, list) and len(cwe_ids) > 0:
                                        cwe = cwe_ids[0]  # Take first CWE

                                vulnerability = {
                                    "package": pkg_name,
                                    "version": pkg_version,
                                    "manager": manager,
                                    "vulnerability_id": vuln_id,
                                    "severity": severity.lower() if severity else "",
                                    "summary": vuln.get("summary", "No summary available"),
                                    "details": vuln.get("details", ""),
                                    "aliases": vuln.get("aliases", []),
                                    "published": vuln.get("published", ""),
                                    "modified": vuln.get("modified", ""),
                                    "references": vuln.get("references", []),
                                    "affected_ranges": [],
                                    "fixed_version": None,  # OSV-Scanner doesn't always provide this
                                    "cwe": cwe,  # Add CWE field for FCE consumption
                                    "source": "OSV-Scanner"
                                }

                                vulnerabilities.append(vulnerability)

                except json.JSONDecodeError:
                    # OSV-Scanner output wasn't valid JSON
                    pass

            self.tool_status["osv-scanner"]["status"] = "success"
            self.tool_status["osv-scanner"]["findings_count"] = len(vulnerabilities)
            return vulnerabilities

        except Exception as e:
            self.tool_status["osv-scanner"]["status"] = "error"
            self.tool_status["osv-scanner"]["error"] = str(e)
            logger.error(f"osv-scanner failed: {e}")
            return []

    def _cross_reference(
        self,
        npm_findings: List[Dict],
        pip_findings: List[Dict],
        osv_findings: List[Dict]
    ) -> List[Dict[str, Any]]:
        """Cross-reference findings from all sources for validation.

        Strategy:
        1. Group by vulnerability ID (CVE/GHSA)
        2. Count sources that found it
        3. Check severity agreement
        4. Assign confidence based on validation

        Confidence Scoring:
        - 3 sources agree: confidence = 1.0 (HIGHEST)
        - 2 sources agree: confidence = 0.9 (HIGH)
        - 1 source only:   confidence = 0.7 (MEDIUM)

        Severity Resolution (when sources disagree):
        - Use HIGHEST severity (conservative approach)
        - Document discrepancy in finding note

        Args:
            npm_findings: Findings from npm audit
            pip_findings: Findings from pip-audit
            osv_findings: Findings from OSV-Scanner

        Returns:
            List of validated and cross-referenced findings
        """
        # Severity ranking (for conflict resolution)
        SEVERITY_RANK = {
            'critical': 4,
            'high': 3,
            'medium': 2,
            'low': 1,
            '': 0,
            'unknown': 0
        }

        # Group all findings by vulnerability ID
        # Key: vuln_id, Value: list of (finding_dict, source_name)
        vuln_groups = {}

        for finding in npm_findings:
            vuln_id = finding.get('vulnerability_id', '')
            if vuln_id:
                if vuln_id not in vuln_groups:
                    vuln_groups[vuln_id] = []
                vuln_groups[vuln_id].append((finding, 'npm-audit'))

        for finding in pip_findings:
            vuln_id = finding.get('vulnerability_id', '')
            # Also check aliases for matching
            aliases = finding.get('aliases', [])

            # Find if this vuln already exists under a different ID
            matched = False
            for existing_id in vuln_groups.keys():
                if vuln_id == existing_id or vuln_id in aliases or existing_id in aliases:
                    vuln_groups[existing_id].append((finding, 'pip-audit'))
                    matched = True
                    break

            if not matched and vuln_id:
                vuln_groups[vuln_id] = [(finding, 'pip-audit')]

        for finding in osv_findings:
            vuln_id = finding.get('vulnerability_id', '')
            aliases = finding.get('aliases', [])

            # Find if this vuln already exists
            matched = False
            for existing_id in vuln_groups.keys():
                if vuln_id == existing_id or vuln_id in aliases or existing_id in aliases:
                    vuln_groups[existing_id].append((finding, 'osv-scanner'))
                    matched = True
                    break

            if not matched and vuln_id:
                vuln_groups[vuln_id] = [(finding, 'osv-scanner')]

        # Cross-reference and validate
        validated_findings = []

        for vuln_id, findings_list in vuln_groups.items():
            # Count unique sources
            sources = list(set(source for _, source in findings_list))
            source_count = len(sources)

            # Assign confidence based on source count
            if source_count >= 3:
                confidence = 1.0
            elif source_count == 2:
                confidence = 0.9
            else:
                confidence = 0.7

            # Merge findings from all sources
            # Prefer OSV-Scanner as primary source (most comprehensive data)
            # Then npm-audit, then pip-audit
            source_priority = {'osv-scanner': 0, 'npm-audit': 1, 'pip-audit': 2}
            findings_sorted = sorted(findings_list, key=lambda x: source_priority.get(x[1], 9))
            base_finding = findings_sorted[0][0].copy()

            # Resolve severity (use highest)
            severities = [f.get('severity', '').lower() for f, _ in findings_list]
            highest_severity = max(severities, key=lambda s: SEVERITY_RANK.get(s, 0))

            # Collect all unique aliases
            all_aliases = set()
            for finding, _ in findings_list:
                all_aliases.update(finding.get('aliases', []))

            # Collect all references
            all_references = []
            for finding, _ in findings_list:
                all_references.extend(finding.get('references', []))

            # Build validated finding (preserve file path for database writes)
            validated = {
                'package': base_finding.get('package', ''),
                'version': base_finding.get('version', ''),
                'manager': base_finding.get('manager', ''),
                'file': base_finding.get('file', 'package.json'),  # Preserve file path
                'vulnerability_id': vuln_id,
                'severity': highest_severity,
                'title': base_finding.get('summary', 'No title'),
                'summary': base_finding.get('summary', ''),
                'details': base_finding.get('details', ''),
                'aliases': list(all_aliases),
                'published': base_finding.get('published', ''),
                'modified': base_finding.get('modified', ''),
                'references': all_references[:5],  # Limit to 5 references
                'affected_ranges': base_finding.get('affected_ranges', []),
                'fixed_version': base_finding.get('fixed_version'),
                'cwe': base_finding.get('cwe', ''),  # Preserve CWE if available
                'confidence': confidence,
                'sources': sources,
                'source_count': source_count
            }

            validated_findings.append(validated)

        return validated_findings

    def _write_to_db(self, findings: List[Dict[str, Any]]):
        """Write findings to findings_consolidated table.

        Schema (from atomic_vuln_impl.md spec and indexer/database.py:706-722):
            file TEXT NOT NULL          - Package file path (package.json or requirements.txt)
            line INTEGER NOT NULL       - Always 0 for dependencies
            column INTEGER              - Always None for dependencies
            rule TEXT NOT NULL          - CVE/GHSA ID (e.g., CVE-2021-23337)
            tool TEXT NOT NULL          - 'vulnerability_scanner'
            message TEXT                - Vulnerability summary
            severity TEXT NOT NULL      - critical/high/medium/low
            category TEXT               - 'dependency'
            confidence REAL             - 0.7-1.0 based on source count
            code_snippet TEXT           - Simple string: "package@version"
            cwe TEXT                    - CWE ID if available
            timestamp TEXT NOT NULL     - ISO 8601 timestamp

        Args:
            findings: List of validated vulnerability findings
        """
        if not findings:
            return

        timestamp = datetime.now(UTC).isoformat()

        for finding in findings:
            # Lookup file path from package_configs using cached mapping
            cache_key = f"{finding.get('manager', 'unknown')}:{finding.get('package', 'unknown')}"
            file_path = self._package_file_map.get(cache_key)

            # Fallback to defaults if not found
            if not file_path:
                if finding.get('manager') == 'py':
                    file_path = 'requirements.txt'
                else:
                    file_path = 'package.json'

            # INSERT using spec-compliant schema
            # Column order: file, line, column, rule, tool, message, severity, category,
            #               confidence, code_snippet, cwe, timestamp
            self.cursor.execute("""
                INSERT INTO findings_consolidated
                (file, line, column, rule, tool, message, severity, category,
                 confidence, code_snippet, cwe, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                file_path,                                   # file (package.json or requirements.txt)
                0,                                           # line (not applicable for dependencies)
                None,                                        # column (not applicable)
                finding.get('vulnerability_id', 'UNKNOWN'),  # rule (CVE-2021-23337, GHSA-xxxx, etc.)
                'vulnerability_scanner',                     # tool (spec-compliant name)
                finding.get('summary', finding.get('title', 'No summary')),  # message
                finding.get('severity', 'medium'),           # severity
                'dependency',                                # category
                finding.get('confidence', 0.7),              # confidence
                f"{finding.get('package', 'unknown')}@{finding.get('version', 'unknown')}",  # code_snippet (simple string)
                finding.get('cwe', ''),                      # cwe (from vulnerability data, not constant)
                timestamp                                    # timestamp
            ))

        self.conn.commit()
        logger.info(f"Wrote {len(findings)} vulnerabilities to findings_consolidated table")

    def _write_to_json(self, findings: List[Dict[str, Any]], output_path: str = "./.pf/raw/vulnerabilities.json"):
        """Write findings to JSON file for AI readability.

        Args:
            findings: List of validated vulnerability findings
            output_path: Path to output JSON file (default: ./.pf/raw/vulnerabilities.json)
        """
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        report_data = {
            "timestamp": datetime.now(UTC).isoformat(),
            "total_vulnerabilities": len(findings),
            "vulnerabilities": findings,
            "sources_used": ["npm-audit", "pip-audit", "osv-scanner"],
            "cross_referenced": True,
            "tool_status": self.tool_status  # Add tool status for observability
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2)

        logger.info(f"Wrote {len(findings)} vulnerabilities to {output_path}")


# ============================================================================
# PUBLIC API - Wrapper Functions for CLI Integration
# ============================================================================

def scan_dependencies(deps_list: Optional[List[Dict]] = None, offline: bool = False) -> List[Dict[str, Any]]:
    """Scan dependencies for vulnerabilities (database-first architecture).

    This function reads dependency information from the database (populated by indexer)
    rather than using the deps_list parameter, which is deprecated.

    Args:
        deps_list: DEPRECATED - No longer used. Kept for backward compatibility only.
                   Scanner reads from package_configs table instead.
        offline: If True, skip network operations (use offline OSV databases)

    Returns:
        List of vulnerability findings

    Note:
        Migration to database-first architecture (v1.1):
        - Old: Pass deps_list from parse_dependencies()
        - New: Scanner reads from repo_index.db (populated by indexer)
        - Requires: Run 'aud index' before vulnerability scanning
    """
    if deps_list is not None:
        logger.warning("deps_list parameter is deprecated and ignored")
        logger.warning("Scanner now reads from database (package_configs table)")

    try:
        # Database is ALWAYS at .pf/repo_index.db
        db_path = Path("./.pf/repo_index.db")

        if not db_path.exists():
            logger.warning("Database not found at .pf/repo_index.db")
            logger.warning("Run 'aud index' first to populate dependency information")
            return []

        # Create scanner and run
        scanner = VulnerabilityScanner(str(db_path), offline=offline)
        findings = scanner.scan()

        return findings

    except FileNotFoundError as e:
        # Tool binaries missing - recoverable
        logger.error(f"Scanner tool not found: {e}")
        logger.error("Run 'aud setup-ai --target .' to install vulnerability scanners")
        return []

    except Exception as e:
        # Unexpected errors - must propagate
        logger.error(f"Vulnerability scan failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise  # Re-raise unexpected errors


def write_vulnerabilities_json(vulnerabilities: List[Dict], output_path: str = "./.pf/raw/vulnerabilities.json"):
    """Write vulnerability findings to JSON file.

    Args:
        vulnerabilities: List of vulnerability findings
        output_path: Path to output JSON file
    """
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    report_data = {
        "timestamp": datetime.now(UTC).isoformat(),
        "total_vulnerabilities": len(vulnerabilities),
        "vulnerabilities": vulnerabilities,
        "sources_used": ["npm-audit", "pip-audit", "osv-scanner"],
        "cross_referenced": True,
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2)

    logger.info(f"Wrote {len(vulnerabilities)} vulnerabilities to {output_path}")


def format_vulnerability_report(vulnerabilities: List[Dict]) -> str:
    """Format vulnerabilities as human-readable report.

    Args:
        vulnerabilities: List of vulnerability findings

    Returns:
        Formatted report string
    """
    if not vulnerabilities:
        return "[OK] No vulnerabilities found"

    # Count by severity
    severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
    for vuln in vulnerabilities:
        severity = vuln.get("severity", "unknown").lower()
        if severity in severity_counts:
            severity_counts[severity] += 1

    # Build report
    report_lines = []
    report_lines.append("=" * 60)
    report_lines.append("VULNERABILITY SCAN RESULTS")
    report_lines.append("=" * 60)
    report_lines.append("")
    report_lines.append(f"Total Vulnerabilities: {len(vulnerabilities)}")
    report_lines.append("")
    report_lines.append("Severity Breakdown:")
    report_lines.append(f"  CRITICAL: {severity_counts['critical']}")
    report_lines.append(f"  HIGH:     {severity_counts['high']}")
    report_lines.append(f"  MEDIUM:   {severity_counts['medium']}")
    report_lines.append(f"  LOW:      {severity_counts['low']}")
    report_lines.append("")
    report_lines.append("=" * 60)
    report_lines.append("FINDINGS:")
    report_lines.append("=" * 60)

    # Group by package
    by_package = {}
    for vuln in vulnerabilities:
        package = vuln.get("package", "unknown")
        if package not in by_package:
            by_package[package] = []
        by_package[package].append(vuln)

    for package, findings in sorted(by_package.items()):
        report_lines.append("")
        report_lines.append(f"Package: {package}")
        report_lines.append("-" * 60)

        for finding in findings:
            vuln_id = finding.get("vulnerability_id", "UNKNOWN")
            severity = finding.get("severity", "unknown").upper()
            title = finding.get("title", "No title")
            confidence = finding.get("confidence", 0)
            sources = finding.get("sources", [])

            report_lines.append(f"  [{severity}] {vuln_id}: {title}")
            report_lines.append(f"    Confidence: {confidence:.1f} (found by: {', '.join(sources)})")

            if finding.get("fixed_version"):
                report_lines.append(f"    Fix: Upgrade to {finding['fixed_version']}")

            if finding.get("references"):
                report_lines.append(f"    References: {finding['references'][0]}")

    report_lines.append("")
    report_lines.append("=" * 60)

    return "\n".join(report_lines)
