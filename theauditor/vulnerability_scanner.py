"""Native vulnerability scanners wrapper for npm audit, pip-audit, and OSV-Scanner.

This module runs native security tools, cross-references findings for validation,
and writes to both database and JSON.

Architecture:
- Reads packages from package_configs table (populated by indexer)
- Runs 3 detection sources in parallel:
  * npm audit (sandboxed node runtime)
  * pip-audit (bundled in .theauditor_tools)
  * osv-scanner (Google's official OSV.dev scanner)
- Cross-references findings for confidence scoring
- Writes to findings_consolidated table (for FCE correlation)
- Writes to JSON (for AI readability)

Cross-Reference Strategy:
- Group findings by vulnerability ID (CVE/GHSA)
- Confidence = # of sources that found it
- Severity = highest when sources disagree
- Flag discrepancies for review

OSV-Scanner Facts (DO NOT HALLUCINATE):
- Binary location: .auditor_venv/.theauditor_tools/osv-scanner/osv-scanner.exe (Windows)
- Offline database: .auditor_venv/.theauditor_tools/osv-scanner/db/{ecosystem}/all.zip
- Usage: osv-scanner scan -L package-lock.json --format json --offline-vulnerabilities
- Database download: --download-offline-databases flag
- No rate limits (offline database)
"""

import json
import sqlite3
import subprocess
import shutil
import platform
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, UTC

from theauditor.utils.logger import setup_logger

# Windows compatibility
IS_WINDOWS = platform.system() == "Windows"

logger = setup_logger(__name__)


class VulnerabilityScanner:
    """Main vulnerability scanner orchestrator."""

    def __init__(self, db_path: str, offline: bool = False):
        """Initialize scanner.

        Args:
            db_path: Path to repo_index.db
            offline: If True, use offline databases only (no network)
        """
        self.db_path = db_path
        self.offline = offline

        try:
            self.conn = sqlite3.connect(db_path)
            self.cursor = self.conn.cursor()
        except sqlite3.Error as e:
            logger.error(f"Failed to connect to database: {e}")
            raise

    def scan(self) -> List[Dict[str, Any]]:
        """Main entry point - run all detection sources and cross-reference.

        Returns:
            List of validated findings
        """
        logger.info("Starting vulnerability scan...")

        # Load packages from database
        packages = self._load_packages_from_db()
        logger.info(f"Loaded {len(packages)} packages from database")

        if not packages:
            logger.warning("No packages found in database")
            return []

        # Run all 3 sources (COMBINE, not fallback)
        logger.info("Running npm audit...")
        npm_findings = self._run_npm_audit()
        logger.info(f"npm audit found {len(npm_findings)} vulnerabilities")

        logger.info("Running pip-audit...")
        pip_findings = self._run_pip_audit()
        logger.info(f"pip-audit found {len(pip_findings)} vulnerabilities")

        logger.info("Running OSV-Scanner...")
        osv_findings = self._run_osv_scanner()
        logger.info(f"OSV-Scanner found {len(osv_findings)} vulnerabilities")

        # Cross-reference for validation
        logger.info("Cross-referencing findings...")
        validated = self._cross_reference(npm_findings, pip_findings, osv_findings)
        logger.info(f"Validated {len(validated)} unique vulnerabilities")

        # Dual write (database + JSON)
        logger.info("Writing findings to database...")
        self._write_to_db(validated)

        logger.info("Writing findings to JSON...")
        self._write_to_json(validated)

        logger.info("Vulnerability scan completed")
        return validated

    def _load_packages_from_db(self) -> List[Dict[str, str]]:
        """Load packages from package_configs table.

        Returns:
            List of package dicts with name, version, manager
        """
        packages = []

        # Query package_configs table
        self.cursor.execute("""
            SELECT package_name, version, file_path
            FROM package_configs
        """)

        for pkg_name, version, file_path in self.cursor.fetchall():
            # Infer manager from file path
            if 'package.json' in file_path:
                manager = 'npm'
            elif 'requirements.txt' in file_path or 'pyproject.toml' in file_path:
                manager = 'py'
            else:
                manager = 'unknown'

            packages.append({
                'name': pkg_name,
                'version': version or 'unknown',
                'manager': manager,
                'file': file_path
            })

        return packages

    def _run_npm_audit(self) -> List[Dict[str, Any]]:
        """Run npm audit using sandboxed node runtime.

        Returns:
            List of vulnerability findings from npm audit
        """
        vulnerabilities = []

        # Check if package.json exists
        project_root = Path.cwd()
        package_json = project_root / "package.json"
        if not package_json.exists():
            return vulnerabilities

        # Check if node_modules exists (npm audit needs it)
        node_modules = project_root / "node_modules"
        if not node_modules.exists():
            return vulnerabilities

        # Find sandboxed npm
        sandbox_base = project_root / ".auditor_venv" / ".theauditor_tools"
        node_runtime = sandbox_base / "node-runtime"

        if IS_WINDOWS:
            node_exe = node_runtime / "node.exe"
            npm_cli = node_runtime / "node_modules" / "npm" / "bin" / "npm-cli.js"
            if npm_cli.exists():
                npm_cmd = [str(node_exe), str(npm_cli), "audit", "--json"]
            else:
                npm_cmd_path = node_runtime / "npm.cmd"
                if npm_cmd_path.exists():
                    npm_cmd = [str(npm_cmd_path), "audit", "--json"]
                else:
                    return vulnerabilities
        else:
            node_exe = node_runtime / "bin" / "node"
            npm_exe = node_runtime / "bin" / "npm"
            if npm_exe.exists():
                npm_cmd = [str(npm_exe), "audit", "--json"]
            else:
                return vulnerabilities

        if not node_exe.exists():
            return vulnerabilities

        try:
            result = subprocess.run(
                npm_cmd,
                cwd=str(project_root),
                capture_output=True,
                text=True,
                timeout=60,
                shell=IS_WINDOWS
            )

            if result.stdout:
                audit_data = json.loads(result.stdout)

                if "vulnerabilities" in audit_data:
                    for pkg_name, pkg_data in audit_data["vulnerabilities"].items():
                        if not pkg_data.get("via"):
                            continue

                        for via_item in pkg_data.get("via", []):
                            if isinstance(via_item, str):
                                continue

                            if isinstance(via_item, dict):
                                severity = via_item.get("severity", "")

                                vuln_id = via_item.get("cve")
                                if not vuln_id:
                                    vuln_id = via_item.get("ghsa")
                                if not vuln_id:
                                    vuln_id = via_item.get("source", f"npm-audit-{pkg_name}")

                                aliases = []
                                if via_item.get("cve"):
                                    aliases.append(via_item["cve"])
                                if via_item.get("ghsa"):
                                    aliases.append(via_item["ghsa"])

                                fixed_version = None
                                if pkg_data.get("fixAvailable"):
                                    fix_info = pkg_data["fixAvailable"]
                                    if isinstance(fix_info, dict) and "version" in fix_info:
                                        fixed_version = fix_info["version"]

                                affected_range = pkg_data.get("range", "")
                                current_version = affected_range.split(" ")[0].lstrip("<>=") if affected_range else ""

                                vulnerability = {
                                    "package": pkg_name,
                                    "version": current_version,
                                    "manager": "npm",
                                    "vulnerability_id": vuln_id,
                                    "severity": severity,
                                    "summary": via_item.get("title", "No summary available"),
                                    "details": via_item.get("overview", ""),
                                    "aliases": aliases,
                                    "published": via_item.get("created", ""),
                                    "modified": via_item.get("updated", ""),
                                    "references": [{
                                        "type": "ADVISORY",
                                        "url": via_item.get("url", "")
                                    }] if via_item.get("url") else [],
                                    "affected_ranges": [pkg_data.get("range", "")] if pkg_data.get("range") else [],
                                    "fixed_version": fixed_version,
                                    "source": "npm audit"
                                }

                                vulnerabilities.append(vulnerability)

        except subprocess.TimeoutExpired:
            pass
        except (subprocess.SubprocessError, json.JSONDecodeError):
            pass

        return vulnerabilities

    def _find_pip_audit(self) -> Optional[str]:
        """Find bundled pip-audit or system fallback.

        Returns:
            Path to pip-audit executable or None
        """
        # Try bundled version first (PREFERRED)
        tools_dir = Path(".auditor_venv/.theauditor_tools/python-tools")
        if IS_WINDOWS:
            bundled = tools_dir / "pip-audit.exe"
        else:
            bundled = tools_dir / "pip-audit"

        if bundled.exists():
            return str(bundled)

        # Fallback to system pip-audit (if user installed it)
        system_pip_audit = shutil.which("pip-audit")
        if system_pip_audit:
            return system_pip_audit

        return None

    def _run_pip_audit(self) -> List[Dict[str, Any]]:
        """Run pip-audit using bundled or system version.

        Returns:
            List of vulnerability findings from pip-audit
        """
        vulnerabilities = []

        # Find pip-audit executable
        pip_audit_path = self._find_pip_audit()
        if not pip_audit_path:
            return vulnerabilities

        # Check if we have Python dependencies to audit
        project_root = Path.cwd()
        has_requirements = (project_root / "requirements.txt").exists()
        has_pyproject = (project_root / "pyproject.toml").exists()

        if not has_requirements and not has_pyproject:
            return vulnerabilities

        try:
            cmd = [pip_audit_path, "--format", "json"]

            if has_requirements:
                cmd.extend(["-r", "requirements.txt"])

            result = subprocess.run(
                cmd,
                cwd=str(project_root),
                capture_output=True,
                text=True,
                timeout=60,
                shell=IS_WINDOWS
            )

            if result.stdout:
                audit_data = json.loads(result.stdout)

                for vuln in audit_data:
                    pkg_name = vuln.get("name", "")
                    pkg_version = vuln.get("version", "")
                    vuln_id = vuln.get("id", f"pip-audit-{pkg_name}")

                    aliases = []
                    if vuln.get("aliases"):
                        aliases.extend(vuln["aliases"])

                    vulnerability = {
                        "package": pkg_name,
                        "version": pkg_version,
                        "manager": "py",
                        "vulnerability_id": vuln_id,
                        "severity": "",  # pip-audit doesn't provide severity
                        "summary": vuln.get("description", "No summary available"),
                        "details": vuln.get("description", ""),
                        "aliases": aliases,
                        "published": "",
                        "modified": "",
                        "references": [],
                        "affected_ranges": [],
                        "fixed_version": vuln.get("fix_versions", [""])[0] if vuln.get("fix_versions") else None,
                        "source": "pip-audit"
                    }

                    vulnerabilities.append(vulnerability)

        except subprocess.TimeoutExpired:
            pass
        except (subprocess.SubprocessError, json.JSONDecodeError):
            pass

        return vulnerabilities

    def _find_osv_scanner(self) -> Optional[str]:
        """Find bundled osv-scanner binary.

        Returns:
            Path to osv-scanner executable or None
        """
        tools_dir = Path(".auditor_venv/.theauditor_tools/osv-scanner")

        if IS_WINDOWS:
            binary = tools_dir / "osv-scanner.exe"
        else:
            binary = tools_dir / "osv-scanner"

        if binary.exists():
            return str(binary)

        # Fallback to system osv-scanner (if user installed it)
        return shutil.which("osv-scanner")

    def _run_osv_scanner(self) -> List[Dict[str, Any]]:
        """Run OSV-Scanner using bundled binary.

        FACTS (from usage.md):
        - Scan lockfiles: osv-scanner scan -L package-lock.json -L requirements.txt
        - Output format: --format json
        - Offline mode: --offline-vulnerabilities
        - Database location: env var OSV_SCANNER_LOCAL_DB_CACHE_DIRECTORY

        Returns:
            List of vulnerability findings from OSV-Scanner
        """
        vulnerabilities = []

        # Find osv-scanner binary
        osv_scanner_path = self._find_osv_scanner()
        if not osv_scanner_path:
            return vulnerabilities

        # Find lockfiles to scan
        project_root = Path.cwd()
        lockfiles = []

        # npm lockfiles
        if (project_root / "package-lock.json").exists():
            lockfiles.extend(["-L", str(project_root / "package-lock.json")])
        elif (project_root / "yarn.lock").exists():
            lockfiles.extend(["-L", str(project_root / "yarn.lock")])

        # Python lockfiles
        if (project_root / "requirements.txt").exists():
            lockfiles.extend(["-L", str(project_root / "requirements.txt")])
        elif (project_root / "Pipfile.lock").exists():
            lockfiles.extend(["-L", str(project_root / "Pipfile.lock")])

        if not lockfiles:
            return vulnerabilities

        # Set database location to our sandbox
        env = os.environ.copy()
        db_dir = Path(".auditor_venv/.theauditor_tools/osv-scanner/db")
        env["OSV_SCANNER_LOCAL_DB_CACHE_DIRECTORY"] = str(db_dir)

        try:
            # Build command
            cmd = [osv_scanner_path, "scan"] + lockfiles + ["--format", "json"]

            # Add offline flag if offline mode
            if self.offline:
                cmd.append("--offline-vulnerabilities")

            # Run osv-scanner
            result = subprocess.run(
                cmd,
                cwd=str(project_root),
                capture_output=True,
                text=True,
                timeout=120,  # OSV-Scanner can be slower than npm audit
                env=env
            )

            # OSV-Scanner returns non-zero exit code if vulnerabilities found
            # So check stdout regardless of return code
            if result.stdout:
                try:
                    scan_data = json.loads(result.stdout)

                    # Parse OSV-Scanner JSON output
                    # Structure: {"results": [{"packages": [...], "source": {...}}]}
                    for result_item in scan_data.get("results", []):
                        for package_vuln in result_item.get("packages", []):
                            pkg_info = package_vuln.get("package", {})
                            pkg_name = pkg_info.get("name", "")
                            pkg_version = pkg_info.get("version", "")
                            pkg_ecosystem = pkg_info.get("ecosystem", "")

                            # Map ecosystem to manager
                            manager = "npm" if pkg_ecosystem in ["npm", "NPM"] else "py" if pkg_ecosystem in ["PyPI", "Python"] else "unknown"

                            for vuln in package_vuln.get("vulnerabilities", []):
                                vuln_id = vuln.get("id", f"osv-{pkg_name}")

                                # Extract severity (if available)
                                severity = ""
                                if vuln.get("database_specific", {}).get("severity"):
                                    severity = vuln["database_specific"]["severity"]

                                vulnerability = {
                                    "package": pkg_name,
                                    "version": pkg_version,
                                    "manager": manager,
                                    "vulnerability_id": vuln_id,
                                    "severity": severity.lower() if severity else "",
                                    "summary": vuln.get("summary", "No summary available"),
                                    "details": vuln.get("details", ""),
                                    "aliases": vuln.get("aliases", []),
                                    "published": vuln.get("published", ""),
                                    "modified": vuln.get("modified", ""),
                                    "references": vuln.get("references", []),
                                    "affected_ranges": [],
                                    "fixed_version": None,  # OSV-Scanner doesn't always provide this
                                    "source": "OSV-Scanner"
                                }

                                vulnerabilities.append(vulnerability)

                except json.JSONDecodeError:
                    # OSV-Scanner output wasn't valid JSON
                    pass

        except subprocess.TimeoutExpired:
            pass
        except subprocess.SubprocessError:
            pass

        return vulnerabilities

    def _cross_reference(
        self,
        npm_findings: List[Dict],
        pip_findings: List[Dict],
        osv_findings: List[Dict]
    ) -> List[Dict[str, Any]]:
        """Cross-reference findings from all sources for validation.

        Strategy:
        1. Group by vulnerability ID (CVE/GHSA)
        2. Count sources that found it
        3. Check severity agreement
        4. Assign confidence based on validation

        Confidence Scoring:
        - 3 sources agree: confidence = 1.0 (HIGHEST)
        - 2 sources agree: confidence = 0.9 (HIGH)
        - 1 source only:   confidence = 0.7 (MEDIUM)

        Severity Resolution (when sources disagree):
        - Use HIGHEST severity (conservative approach)
        - Document discrepancy in finding note

        Args:
            npm_findings: Findings from npm audit
            pip_findings: Findings from pip-audit
            osv_findings: Findings from OSV-Scanner

        Returns:
            List of validated and cross-referenced findings
        """
        # Severity ranking (for conflict resolution)
        SEVERITY_RANK = {
            'critical': 4,
            'high': 3,
            'medium': 2,
            'low': 1,
            '': 0,
            'unknown': 0
        }

        # Group all findings by vulnerability ID
        # Key: vuln_id, Value: list of (finding_dict, source_name)
        vuln_groups = {}

        for finding in npm_findings:
            vuln_id = finding.get('vulnerability_id', '')
            if vuln_id:
                if vuln_id not in vuln_groups:
                    vuln_groups[vuln_id] = []
                vuln_groups[vuln_id].append((finding, 'npm-audit'))

        for finding in pip_findings:
            vuln_id = finding.get('vulnerability_id', '')
            # Also check aliases for matching
            aliases = finding.get('aliases', [])

            # Find if this vuln already exists under a different ID
            matched = False
            for existing_id in vuln_groups.keys():
                if vuln_id == existing_id or vuln_id in aliases or existing_id in aliases:
                    vuln_groups[existing_id].append((finding, 'pip-audit'))
                    matched = True
                    break

            if not matched and vuln_id:
                vuln_groups[vuln_id] = [(finding, 'pip-audit')]

        for finding in osv_findings:
            vuln_id = finding.get('vulnerability_id', '')
            aliases = finding.get('aliases', [])

            # Find if this vuln already exists
            matched = False
            for existing_id in vuln_groups.keys():
                if vuln_id == existing_id or vuln_id in aliases or existing_id in aliases:
                    vuln_groups[existing_id].append((finding, 'osv-scanner'))
                    matched = True
                    break

            if not matched and vuln_id:
                vuln_groups[vuln_id] = [(finding, 'osv-scanner')]

        # Cross-reference and validate
        validated_findings = []

        for vuln_id, findings_list in vuln_groups.items():
            # Count unique sources
            sources = list(set(source for _, source in findings_list))
            source_count = len(sources)

            # Assign confidence based on source count
            if source_count >= 3:
                confidence = 1.0
            elif source_count == 2:
                confidence = 0.9
            else:
                confidence = 0.7

            # Merge findings from all sources
            # Use the most complete finding as base
            base_finding = findings_list[0][0].copy()

            # Resolve severity (use highest)
            severities = [f.get('severity', '').lower() for f, _ in findings_list]
            highest_severity = max(severities, key=lambda s: SEVERITY_RANK.get(s, 0))

            # Collect all unique aliases
            all_aliases = set()
            for finding, _ in findings_list:
                all_aliases.update(finding.get('aliases', []))

            # Collect all references
            all_references = []
            for finding, _ in findings_list:
                all_references.extend(finding.get('references', []))

            # Build validated finding
            validated = {
                'package': base_finding.get('package', ''),
                'version': base_finding.get('version', ''),
                'manager': base_finding.get('manager', ''),
                'vulnerability_id': vuln_id,
                'severity': highest_severity,
                'title': base_finding.get('summary', 'No title'),
                'summary': base_finding.get('summary', ''),
                'details': base_finding.get('details', ''),
                'aliases': list(all_aliases),
                'published': base_finding.get('published', ''),
                'modified': base_finding.get('modified', ''),
                'references': all_references[:5],  # Limit to 5 references
                'affected_ranges': base_finding.get('affected_ranges', []),
                'fixed_version': base_finding.get('fixed_version'),
                'confidence': confidence,
                'sources': sources,
                'source_count': source_count
            }

            validated_findings.append(validated)

        return validated_findings

    def _write_to_db(self, findings: List[Dict[str, Any]]):
        """Write findings to findings_consolidated table.

        IMPORTANT: This table is owned by the indexer (created in indexer/database.py).
        We MUST use the exact schema defined there, not create our own.

        Schema (from indexer/database.py:706-722):
            file TEXT NOT NULL
            line INTEGER NOT NULL
            column INTEGER
            rule TEXT NOT NULL
            tool TEXT NOT NULL
            message TEXT
            severity TEXT NOT NULL
            category TEXT
            confidence REAL
            code_snippet TEXT
            cwe TEXT
            timestamp TEXT NOT NULL

        Args:
            findings: List of validated vulnerability findings
        """
        # DO NOT create table - indexer owns schema (runs before this in pipeline)
        # Table is created in indexer/database.py:706-722

        timestamp = datetime.now(UTC).isoformat()

        for finding in findings:
            # Format message
            message = f"{finding.get('vulnerability_id', 'UNKNOWN')}: {finding.get('title', 'No title')}"

            # Pack vulnerability metadata into code_snippet as JSON
            # (vulnerability-specific fields don't have dedicated columns)
            metadata_json = json.dumps({
                'package': finding.get('package', ''),
                'version': finding.get('version', ''),
                'manager': finding.get('manager', ''),
                'vulnerability_id': finding.get('vulnerability_id', ''),
                'aliases': finding.get('aliases', []),
                'published': finding.get('published', ''),
                'modified': finding.get('modified', ''),
                'references': finding.get('references', []),
                'affected_ranges': finding.get('affected_ranges', []),
                'fixed_version': finding.get('fixed_version'),
                'source_count': finding.get('source_count', 1),
                'sources': finding.get('sources', []),
                'details': finding.get('details', '')[:500]  # Truncate details
            })

            # INSERT using indexer's schema (lines 1338-1342 of indexer/database.py)
            # Column order: file, line, column, rule, tool, message, severity, category,
            #               confidence, code_snippet, cwe, timestamp
            self.cursor.execute("""
                INSERT INTO findings_consolidated
                (file, line, column, rule, tool, message, severity, category,
                 confidence, code_snippet, cwe, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                finding.get('package', 'unknown-package'),  # file (package name as identifier)
                0,                                           # line (not applicable for dependencies)
                None,                                        # column (not applicable)
                'vulnerability-scan',                       # rule
                'multi-source-validator',                   # tool (3 sources: npm/pip/osv)
                message,                                     # message
                finding.get('severity', 'unknown'),         # severity
                'dependency',                                # category
                finding.get('confidence', 0.7),             # confidence
                metadata_json,                               # code_snippet (JSON metadata)
                'CWE-1104',                                  # cwe (Use of Unmaintained Components)
                timestamp                                    # timestamp
            ))

        self.conn.commit()
        logger.info(f"Wrote {len(findings)} vulnerabilities to findings_consolidated table")

    def _write_to_json(self, findings: List[Dict[str, Any]], output_path: str = "./.pf/raw/vulnerabilities.json"):
        """Write findings to JSON file for AI readability.

        Args:
            findings: List of validated vulnerability findings
            output_path: Path to output JSON file (default: ./.pf/raw/vulnerabilities.json)
        """
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        report_data = {
            "timestamp": datetime.now(UTC).isoformat(),
            "total_vulnerabilities": len(findings),
            "vulnerabilities": findings,
            "sources_used": ["npm-audit", "pip-audit", "osv-scanner"],
            "cross_referenced": True,
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2)

        logger.info(f"Wrote {len(findings)} vulnerabilities to {output_path}")


# ============================================================================
# PUBLIC API - Wrapper Functions for CLI Integration
# ============================================================================

def scan_dependencies(deps_list: List[Dict], offline: bool = False) -> List[Dict[str, Any]]:
    """Scan dependencies for vulnerabilities (wrapper for CLI compatibility).

    This function provides backward compatibility with the deps.py command.
    It reads from the database (populated by indexer) rather than using deps_list.

    Args:
        deps_list: Dependency list (IGNORED - reads from database instead)
        offline: If True, skip network operations

    Returns:
        List of vulnerability findings

    Note:
        The deps_list parameter is kept for backward compatibility but not used.
        The scanner reads from package_configs table populated by the indexer.
    """
    try:
        # Database is ALWAYS at .pf/repo_index.db
        db_path = Path("./.pf/repo_index.db")

        if not db_path.exists():
            logger.warning("Database not found at .pf/repo_index.db")
            logger.warning("Run 'aud index' first to populate dependency information")
            return []

        # Create scanner and run
        scanner = VulnerabilityScanner(str(db_path), offline=offline)
        findings = scanner.scan()

        return findings

    except Exception as e:
        logger.error(f"Vulnerability scan failed: {e}")
        return []


def write_vulnerabilities_json(vulnerabilities: List[Dict], output_path: str = "./.pf/raw/vulnerabilities.json"):
    """Write vulnerability findings to JSON file.

    Args:
        vulnerabilities: List of vulnerability findings
        output_path: Path to output JSON file
    """
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    report_data = {
        "timestamp": datetime.now(UTC).isoformat(),
        "total_vulnerabilities": len(vulnerabilities),
        "vulnerabilities": vulnerabilities,
        "sources_used": ["npm-audit", "pip-audit", "osv-scanner"],
        "cross_referenced": True,
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2)

    logger.info(f"Wrote {len(vulnerabilities)} vulnerabilities to {output_path}")


def format_vulnerability_report(vulnerabilities: List[Dict]) -> str:
    """Format vulnerabilities as human-readable report.

    Args:
        vulnerabilities: List of vulnerability findings

    Returns:
        Formatted report string
    """
    if not vulnerabilities:
        return "[OK] No vulnerabilities found"

    # Count by severity
    severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
    for vuln in vulnerabilities:
        severity = vuln.get("severity", "unknown").lower()
        if severity in severity_counts:
            severity_counts[severity] += 1

    # Build report
    report_lines = []
    report_lines.append("=" * 60)
    report_lines.append("VULNERABILITY SCAN RESULTS")
    report_lines.append("=" * 60)
    report_lines.append("")
    report_lines.append(f"Total Vulnerabilities: {len(vulnerabilities)}")
    report_lines.append("")
    report_lines.append("Severity Breakdown:")
    report_lines.append(f"  CRITICAL: {severity_counts['critical']}")
    report_lines.append(f"  HIGH:     {severity_counts['high']}")
    report_lines.append(f"  MEDIUM:   {severity_counts['medium']}")
    report_lines.append(f"  LOW:      {severity_counts['low']}")
    report_lines.append("")
    report_lines.append("=" * 60)
    report_lines.append("FINDINGS:")
    report_lines.append("=" * 60)

    # Group by package
    by_package = {}
    for vuln in vulnerabilities:
        package = vuln.get("package", "unknown")
        if package not in by_package:
            by_package[package] = []
        by_package[package].append(vuln)

    for package, findings in sorted(by_package.items()):
        report_lines.append("")
        report_lines.append(f"Package: {package}")
        report_lines.append("-" * 60)

        for finding in findings:
            vuln_id = finding.get("vulnerability_id", "UNKNOWN")
            severity = finding.get("severity", "unknown").upper()
            title = finding.get("title", "No title")
            confidence = finding.get("confidence", 0)
            sources = finding.get("sources", [])

            report_lines.append(f"  [{severity}] {vuln_id}: {title}")
            report_lines.append(f"    Confidence: {confidence:.1f} (found by: {', '.join(sources)})")

            if finding.get("fixed_version"):
                report_lines.append(f"    Fix: Upgrade to {finding['fixed_version']}")

            if finding.get("references"):
                report_lines.append(f"    References: {finding['references'][0]}")

    report_lines.append("")
    report_lines.append("=" * 60)

    return "\n".join(report_lines)
